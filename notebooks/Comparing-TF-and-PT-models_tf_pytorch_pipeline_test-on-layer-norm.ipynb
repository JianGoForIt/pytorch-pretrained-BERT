{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing TensorFlow (original) and PyTorch models\n",
    "\n",
    "You can use this small notebook to check the conversion of the model's weights from the TensorFlow model to the PyTorch model. In the following, we compare the weights of the last layer on a simple example (in `input.txt`) but both models returns all the hidden layers so you can check every stage of the model.\n",
    "\n",
    "To run this notebook, follow these instructions:\n",
    "- make sure that your Python environment has both TensorFlow and PyTorch installed,\n",
    "- download the original TensorFlow implementation,\n",
    "- download a pre-trained TensorFlow model as indicaded in the TensorFlow implementation readme,\n",
    "- run the script `convert_tf_checkpoint_to_pytorch.py` as indicated in the `README` to convert the pre-trained TensorFlow model to PyTorch.\n",
    "\n",
    "If needed change the relative paths indicated in this notebook (at the beggining of Sections 1 and 2) to point to the relevent models and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:56:48.412622Z",
     "start_time": "2018-11-15T14:56:48.400110Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 15:12:10.009658 140411700147968 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ TensorFlow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:56:49.483829Z",
     "start_time": "2018-11-15T14:56:49.471296Z"
    }
   },
   "outputs": [],
   "source": [
    "original_tf_inplem_dir = \"../bert/\"\n",
    "model_dir = \"/tmp/pretraining_output_test/\"\n",
    "\n",
    "vocab_file = model_dir + \"vocab.txt\"\n",
    "bert_config_file = model_dir + \"bert_config.json\"\n",
    "init_checkpoint = model_dir + \"model.ckpt-20\"\n",
    "\n",
    "input_file = \"./samples/input.txt\"\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:57:51.597932Z",
     "start_time": "2018-11-15T14:57:51.549466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('*', original_tf_inplem_dir + '/extract_features.py')\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "sys.modules['extract_features_tensorflow'] = module\n",
    "sys.path.append('../bert')\n",
    "from extract_features_tensorflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:05.650987Z",
     "start_time": "2018-11-15T14:58:05.541620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 15:12:11.568553 140411700147968 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0702 15:12:11.719705 140411700147968 deprecation_wrapper.py:119] From ../bert//extract_features.py:295: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "layer_indexes = list(range(12))\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=vocab_file, do_lower_case=True)\n",
    "examples = read_examples(input_file)\n",
    "\n",
    "features = convert_examples_to_features(\n",
    "    examples=examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:11.562443Z",
     "start_time": "2018-11-15T14:58:08.036485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 15:12:12.838428 140411700147968 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0702 15:12:12.840993 140411700147968 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fb3e9b9f9d8>) includes params argument, but params are not passed to Estimator.\n",
      "W0702 15:12:12.845199 140411700147968 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpsj8as44d\n",
      "W0702 15:12:12.847528 140411700147968 tpu_context.py:750] Setting TPUConfig.num_shards==1 is an unsupported behavior. Please fix as soon as possible (leaving num_shards as None.)\n",
      "W0702 15:12:12.848565 140411700147968 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    master=None,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        num_shards=1,\n",
    "        per_host_input_for_training=is_per_host))\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=init_checkpoint,\n",
    "    layer_indexes=layer_indexes,\n",
    "    use_tpu=False,\n",
    "    use_one_hot_embeddings=False)\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU.\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    predict_batch_size=1)\n",
    "\n",
    "input_fn = input_fn_builder(\n",
    "    features=features, seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:21.736543Z",
     "start_time": "2018-11-15T14:58:16.723829Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 15:12:13.994565 140411700147968 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0702 15:12:13.997931 140411700147968 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0702 15:12:14.035680 140411700147968 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0702 15:12:14.103726 140411700147968 deprecation.py:323] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0702 15:12:14.550541 140411700147968 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:871: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0702 15:12:16.977650 140411700147968 deprecation_wrapper.py:119] From ../bert//extract_features.py:196: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/word_embeddings:0\n",
      "bert/embeddings/token_type_embeddings:0\n",
      "bert/embeddings/position_embeddings:0\n",
      "bert/embeddings/LayerNorm/beta:0\n",
      "bert/embeddings/LayerNorm/gamma:0\n",
      "bert/encoder/layer_0/attention/self/query/kernel:0\n",
      "bert/encoder/layer_0/attention/self/query/bias:0\n",
      "bert/encoder/layer_0/attention/self/key/kernel:0\n",
      "bert/encoder/layer_0/attention/self/key/bias:0\n",
      "bert/encoder/layer_0/attention/self/value/kernel:0\n",
      "bert/encoder/layer_0/attention/self/value/bias:0\n",
      "bert/encoder/layer_0/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_0/attention/output/dense/bias:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_0/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_0/intermediate/dense/bias:0\n",
      "bert/encoder/layer_0/output/dense/kernel:0\n",
      "bert/encoder/layer_0/output/dense/bias:0\n",
      "bert/encoder/layer_0/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_1/attention/self/query/kernel:0\n",
      "bert/encoder/layer_1/attention/self/query/bias:0\n",
      "bert/encoder/layer_1/attention/self/key/kernel:0\n",
      "bert/encoder/layer_1/attention/self/key/bias:0\n",
      "bert/encoder/layer_1/attention/self/value/kernel:0\n",
      "bert/encoder/layer_1/attention/self/value/bias:0\n",
      "bert/encoder/layer_1/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_1/attention/output/dense/bias:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_1/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_1/intermediate/dense/bias:0\n",
      "bert/encoder/layer_1/output/dense/kernel:0\n",
      "bert/encoder/layer_1/output/dense/bias:0\n",
      "bert/encoder/layer_1/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_2/attention/self/query/kernel:0\n",
      "bert/encoder/layer_2/attention/self/query/bias:0\n",
      "bert/encoder/layer_2/attention/self/key/kernel:0\n",
      "bert/encoder/layer_2/attention/self/key/bias:0\n",
      "bert/encoder/layer_2/attention/self/value/kernel:0\n",
      "bert/encoder/layer_2/attention/self/value/bias:0\n",
      "bert/encoder/layer_2/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_2/attention/output/dense/bias:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_2/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_2/intermediate/dense/bias:0\n",
      "bert/encoder/layer_2/output/dense/kernel:0\n",
      "bert/encoder/layer_2/output/dense/bias:0\n",
      "bert/encoder/layer_2/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_3/attention/self/query/kernel:0\n",
      "bert/encoder/layer_3/attention/self/query/bias:0\n",
      "bert/encoder/layer_3/attention/self/key/kernel:0\n",
      "bert/encoder/layer_3/attention/self/key/bias:0\n",
      "bert/encoder/layer_3/attention/self/value/kernel:0\n",
      "bert/encoder/layer_3/attention/self/value/bias:0\n",
      "bert/encoder/layer_3/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_3/attention/output/dense/bias:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_3/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_3/intermediate/dense/bias:0\n",
      "bert/encoder/layer_3/output/dense/kernel:0\n",
      "bert/encoder/layer_3/output/dense/bias:0\n",
      "bert/encoder/layer_3/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_4/attention/self/query/kernel:0\n",
      "bert/encoder/layer_4/attention/self/query/bias:0\n",
      "bert/encoder/layer_4/attention/self/key/kernel:0\n",
      "bert/encoder/layer_4/attention/self/key/bias:0\n",
      "bert/encoder/layer_4/attention/self/value/kernel:0\n",
      "bert/encoder/layer_4/attention/self/value/bias:0\n",
      "bert/encoder/layer_4/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_4/attention/output/dense/bias:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_4/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_4/intermediate/dense/bias:0\n",
      "bert/encoder/layer_4/output/dense/kernel:0\n",
      "bert/encoder/layer_4/output/dense/bias:0\n",
      "bert/encoder/layer_4/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_5/attention/self/query/kernel:0\n",
      "bert/encoder/layer_5/attention/self/query/bias:0\n",
      "bert/encoder/layer_5/attention/self/key/kernel:0\n",
      "bert/encoder/layer_5/attention/self/key/bias:0\n",
      "bert/encoder/layer_5/attention/self/value/kernel:0\n",
      "bert/encoder/layer_5/attention/self/value/bias:0\n",
      "bert/encoder/layer_5/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_5/attention/output/dense/bias:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_5/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_5/intermediate/dense/bias:0\n",
      "bert/encoder/layer_5/output/dense/kernel:0\n",
      "bert/encoder/layer_5/output/dense/bias:0\n",
      "bert/encoder/layer_5/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_6/attention/self/query/kernel:0\n",
      "bert/encoder/layer_6/attention/self/query/bias:0\n",
      "bert/encoder/layer_6/attention/self/key/kernel:0\n",
      "bert/encoder/layer_6/attention/self/key/bias:0\n",
      "bert/encoder/layer_6/attention/self/value/kernel:0\n",
      "bert/encoder/layer_6/attention/self/value/bias:0\n",
      "bert/encoder/layer_6/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_6/attention/output/dense/bias:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_6/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_6/intermediate/dense/bias:0\n",
      "bert/encoder/layer_6/output/dense/kernel:0\n",
      "bert/encoder/layer_6/output/dense/bias:0\n",
      "bert/encoder/layer_6/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_7/attention/self/query/kernel:0\n",
      "bert/encoder/layer_7/attention/self/query/bias:0\n",
      "bert/encoder/layer_7/attention/self/key/kernel:0\n",
      "bert/encoder/layer_7/attention/self/key/bias:0\n",
      "bert/encoder/layer_7/attention/self/value/kernel:0\n",
      "bert/encoder/layer_7/attention/self/value/bias:0\n",
      "bert/encoder/layer_7/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_7/attention/output/dense/bias:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_7/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_7/intermediate/dense/bias:0\n",
      "bert/encoder/layer_7/output/dense/kernel:0\n",
      "bert/encoder/layer_7/output/dense/bias:0\n",
      "bert/encoder/layer_7/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_8/attention/self/query/kernel:0\n",
      "bert/encoder/layer_8/attention/self/query/bias:0\n",
      "bert/encoder/layer_8/attention/self/key/kernel:0\n",
      "bert/encoder/layer_8/attention/self/key/bias:0\n",
      "bert/encoder/layer_8/attention/self/value/kernel:0\n",
      "bert/encoder/layer_8/attention/self/value/bias:0\n",
      "bert/encoder/layer_8/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_8/attention/output/dense/bias:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_8/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_8/intermediate/dense/bias:0\n",
      "bert/encoder/layer_8/output/dense/kernel:0\n",
      "bert/encoder/layer_8/output/dense/bias:0\n",
      "bert/encoder/layer_8/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_9/attention/self/query/kernel:0\n",
      "bert/encoder/layer_9/attention/self/query/bias:0\n",
      "bert/encoder/layer_9/attention/self/key/kernel:0\n",
      "bert/encoder/layer_9/attention/self/key/bias:0\n",
      "bert/encoder/layer_9/attention/self/value/kernel:0\n",
      "bert/encoder/layer_9/attention/self/value/bias:0\n",
      "bert/encoder/layer_9/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_9/attention/output/dense/bias:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_9/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_9/intermediate/dense/bias:0\n",
      "bert/encoder/layer_9/output/dense/kernel:0\n",
      "bert/encoder/layer_9/output/dense/bias:0\n",
      "bert/encoder/layer_9/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_10/attention/self/query/kernel:0\n",
      "bert/encoder/layer_10/attention/self/query/bias:0\n",
      "bert/encoder/layer_10/attention/self/key/kernel:0\n",
      "bert/encoder/layer_10/attention/self/key/bias:0\n",
      "bert/encoder/layer_10/attention/self/value/kernel:0\n",
      "bert/encoder/layer_10/attention/self/value/bias:0\n",
      "bert/encoder/layer_10/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_10/attention/output/dense/bias:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_10/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_10/intermediate/dense/bias:0\n",
      "bert/encoder/layer_10/output/dense/kernel:0\n",
      "bert/encoder/layer_10/output/dense/bias:0\n",
      "bert/encoder/layer_10/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_11/attention/self/query/kernel:0\n",
      "bert/encoder/layer_11/attention/self/query/bias:0\n",
      "bert/encoder/layer_11/attention/self/key/kernel:0\n",
      "bert/encoder/layer_11/attention/self/key/bias:0\n",
      "bert/encoder/layer_11/attention/self/value/kernel:0\n",
      "bert/encoder/layer_11/attention/self/value/bias:0\n",
      "bert/encoder/layer_11/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_11/attention/output/dense/bias:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_11/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_11/intermediate/dense/bias:0\n",
      "bert/encoder/layer_11/output/dense/kernel:0\n",
      "bert/encoder/layer_11/output/dense/bias:0\n",
      "bert/encoder/layer_11/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma:0\n",
      "bert/pooler/dense/kernel:0\n",
      "bert/pooler/dense/bias:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 15:12:18.118626 140411700147968 deprecation.py:323] From /lfs/1/zjian/anaconda2/envs/bert-pretraining/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention output  [[590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]\n",
      " [590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]\n",
      " [590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]\n",
      " ...\n",
      " [590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]\n",
      " [590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]\n",
      " [590593.062 590593.062 590593.062 ... 590593.062 590593.062 590593.062]] [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]] [128 768]\n",
      "attention output 2  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]\n",
      " [0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]\n",
      " [0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]\n",
      " ...\n",
      " [0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]\n",
      " [0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]\n",
      " [0.841192 0.841192 0.841192 ... 0.841192 0.841192 0.841192]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "attention output  [[1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " ...\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]\n",
      " [1180417.12 1180417.12 1180417.12 ... 1180417.12 1180417.12 1180417.12]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [128 768]\n",
      "attention output 2  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]] [1 1 1 ... 1 1 1] [1 1 1 ... 1 1 1]\n",
      "intermediate output  [[1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " ...\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]\n",
      " [1537 1537 1537 ... 1537 1537 1537]]\n",
      "layer output  [[2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "extracting layer 0\n",
      "extracting layer 1\n",
      "extracting layer 2\n",
      "extracting layer 3\n",
      "extracting layer 4\n",
      "extracting layer 5\n",
      "extracting layer 6\n",
      "extracting layer 7\n",
      "extracting layer 8\n",
      "extracting layer 9\n",
      "extracting layer 10\n",
      "extracting layer 11\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "tensorflow_all_out = []\n",
    "for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "    unique_id = int(result[\"unique_id\"])\n",
    "    feature = unique_id_to_feature[unique_id]\n",
    "    output_json = collections.OrderedDict()\n",
    "    output_json[\"linex_index\"] = unique_id\n",
    "    tensorflow_all_out_features = []\n",
    "    # for (i, token) in enumerate(feature.tokens):\n",
    "    all_layers = []\n",
    "    for (j, layer_index) in enumerate(layer_indexes):\n",
    "        print(\"extracting layer {}\".format(j))\n",
    "        layer_output = result[\"layer_output_%d\" % j]\n",
    "        layers = collections.OrderedDict()\n",
    "        layers[\"index\"] = layer_index\n",
    "        layers[\"values\"] = layer_output\n",
    "        all_layers.append(layers)\n",
    "    tensorflow_out_features = collections.OrderedDict()\n",
    "    tensorflow_out_features[\"layers\"] = all_layers\n",
    "    tensorflow_all_out_features.append(tensorflow_out_features)\n",
    "\n",
    "    output_json[\"features\"] = tensorflow_all_out_features\n",
    "    tensorflow_all_out.append(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:23.970714Z",
     "start_time": "2018-11-15T14:58:23.931930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "odict_keys(['linex_index', 'features'])\n",
      "number of tokens 1\n",
      "number of layers 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tensorflow_all_out))\n",
    "print(len(tensorflow_all_out[0]))\n",
    "print(tensorflow_all_out[0].keys())\n",
    "print(\"number of tokens\", len(tensorflow_all_out[0]['features']))\n",
    "print(\"number of layers\", len(tensorflow_all_out[0]['features'][0]['layers']))\n",
    "tensorflow_all_out[0]['features'][0]['layers'][0]['values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:25.547012Z",
     "start_time": "2018-11-15T14:58:25.516076Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorflow_outputs = list(tensorflow_all_out[0]['features'][0]['layers'][t]['values'] for t in layer_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'double_test/beta:0' shape=(276,) dtype=float32_ref>, <tf.Variable 'double_test/gamma:0' shape=(276,) dtype=float32_ref>]\n",
      "(276,)\n",
      "(128, 276) [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# def layer_norm(input_tensor, name=None):\n",
    "#   \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "#   return tf.contrib.layers.layer_norm(\n",
    "#       inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_tensor = tf.constant(value=0.5, shape=(128, 276))\n",
    "output_tensor = tf.contrib.layers.layer_norm(\n",
    "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=\"double_test\")\n",
    "assign_ops = []\n",
    "tvars = []\n",
    "for tvar in tf.trainable_variables():\n",
    "    assign_ops.append(tf.assign(tvar, tf.ones_like(tvar)))\n",
    "    tvars.append(tvar)\n",
    "    \n",
    "print(tf.trainable_variables())\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    res = sess.run(tvars)\n",
    "    print(res[0].shape)\n",
    "    res = sess.run(output_tensor)\n",
    "    print(res.shape, res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ PyTorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:03:49.528679Z",
     "start_time": "2018-11-15T15:03:49.497697Z"
    }
   },
   "outputs": [],
   "source": [
    "import extract_features\n",
    "import pytorch_pretrained_bert as ppb\n",
    "from extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:18.001177Z",
     "start_time": "2018-11-15T15:21:17.970369Z"
    }
   },
   "outputs": [],
   "source": [
    "init_checkpoint_pt = \"/tmp/pretraining_output_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:20.893669Z",
     "start_time": "2018-11-15T15:21:18.786623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = ppb.BertModel.from_pretrained(init_checkpoint_pt)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:26.963427Z",
     "start_time": "2018-11-15T15:21:26.922494Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids, all_example_index)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:30.718724Z",
     "start_time": "2018-11-15T15:21:30.329205Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958, 27227,  2001,\n",
      "          1037, 13997, 11510,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0])\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         ...,\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625],\n",
      "         [590593.0625, 590593.0625, 590593.0625,  ..., 590593.0625,\n",
      "          590593.0625, 590593.0625]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<AddBackward0>) tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         ...,\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.],\n",
      "         [769., 769., 769.,  ..., 769., 769., 769.]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer done  tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         ...,\n",
      "         [1180417.1250, 1180417.1250, 1180417.1250,  ..., 1180417.1250,\n",
      "          1180417.1250, 1180417.1250],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000],\n",
      "         [1180417.0000, 1180417.0000, 1180417.0000,  ..., 1180417.0000,\n",
      "          1180417.0000, 1180417.0000]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AddBackward0>) tensor([[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         ...,\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "         [2., 2., 2.,  ..., 2., 2., 2.]]], grad_fn=<AddBackward0>)\n",
      "intermediate output  tensor([[[0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         ...,\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413],\n",
      "         [0.8413, 0.8413, 0.8413,  ..., 0.8413, 0.8413, 0.8413]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         ...,\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 1.0000e+00],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06],\n",
      "         [2.0266e-06, 2.0266e-06, 2.0266e-06,  ..., 2.0266e-06,\n",
      "          2.0266e-06, 2.0266e-06]]], grad_fn=<AddBackward0>)\n",
      "layer 0 0\n",
      "layer 1 1\n",
      "layer 2 2\n",
      "layer 3 3\n",
      "layer 4 4\n",
      "layer 5 5\n",
      "layer 6 6\n",
      "layer 7 7\n",
      "layer 8 8\n",
      "layer 9 9\n",
      "layer 10 10\n",
      "layer 11 11\n"
     ]
    }
   ],
   "source": [
    "layer_indexes = list(range(12))\n",
    "\n",
    "pytorch_all_out = []\n",
    "for input_ids, input_mask, input_type_ids, example_indices in eval_dataloader:\n",
    "    print(input_ids)\n",
    "    print(input_mask)\n",
    "    print(example_indices)\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "\n",
    "    all_encoder_layers, _ = model(input_ids, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
    "\n",
    "    for b, example_index in enumerate(example_indices):\n",
    "        feature = features[example_index.item()]\n",
    "        unique_id = int(feature.unique_id)\n",
    "        # feature = unique_id_to_feature[unique_id]\n",
    "        output_json = collections.OrderedDict()\n",
    "        output_json[\"linex_index\"] = unique_id\n",
    "        all_out_features = []\n",
    "        # for (i, token) in enumerate(feature.tokens):\n",
    "        all_layers = []\n",
    "        for (j, layer_index) in enumerate(layer_indexes):\n",
    "            print(\"layer\", j, layer_index)\n",
    "            layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()\n",
    "            layer_output = layer_output[b]\n",
    "            layers = collections.OrderedDict()\n",
    "            layers[\"index\"] = layer_index\n",
    "            layer_output = layer_output\n",
    "            layers[\"values\"] = layer_output if not isinstance(layer_output, (int, float)) else [layer_output]\n",
    "            all_layers.append(layers)\n",
    "\n",
    "            out_features = collections.OrderedDict()\n",
    "            out_features[\"layers\"] = all_layers\n",
    "            all_out_features.append(out_features)\n",
    "        output_json[\"features\"] = all_out_features\n",
    "        pytorch_all_out.append(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:35.703615Z",
     "start_time": "2018-11-15T15:21:35.666150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "odict_keys(['linex_index', 'features'])\n",
      "number of tokens 1\n",
      "number of layers 12\n",
      "hidden_size 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pytorch_all_out))\n",
    "print(len(pytorch_all_out[0]))\n",
    "print(pytorch_all_out[0].keys())\n",
    "print(\"number of tokens\", len(pytorch_all_out))\n",
    "print(\"number of layers\", len(pytorch_all_out[0]['features'][0]['layers']))\n",
    "print(\"hidden_size\", len(pytorch_all_out[0]['features'][0]['layers'][0]['values']))\n",
    "pytorch_all_out[0]['features'][0]['layers'][0]['values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:36.999073Z",
     "start_time": "2018-11-15T15:21:36.966762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 768)\n",
      "(128, 768)\n"
     ]
    }
   ],
   "source": [
    "pytorch_outputs = list(pytorch_all_out[0]['features'][0]['layers'][t]['values'] for t in layer_indexes)\n",
    "print(pytorch_outputs[0].shape)\n",
    "print(pytorch_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:37.936522Z",
     "start_time": "2018-11-15T15:21:37.905269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 768)\n",
      "(128, 768)\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0].shape)\n",
    "print(tensorflow_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0])\n",
    "# for i in range(tensorflow_outputs[0].shape[0]):\n",
    "#     print(tensorflow_outputs[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "0 [[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-2. -2. -2. ... -2. -2. -2.]\n",
      " [-2. -2. -2. ... -2. -2. -2.]]\n",
      "1 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "1 [[1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]]\n",
      "1 [[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "2 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "2 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "2 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "3 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "3 [[1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]]\n",
      "3 [[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "4 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "4 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "4 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "5 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "5 [[1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]]\n",
      "5 [[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "6 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "6 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "6 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "7 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "7 [[1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]]\n",
      "7 [[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "8 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "8 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "8 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "9 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "9 [[1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " ...\n",
      " [1.000000e+00 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]\n",
      " [2.026558e-06 2.026558e-06 2.026558e-06 ... 2.026558e-06 2.026558e-06\n",
      "  2.026558e-06]]\n",
      "9 [[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "10 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "10 [[2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " ...\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 2. 2.]]\n",
      "10 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    print(i, tensorflow_outputs[i])\n",
    "    print(i, pytorch_outputs[i])\n",
    "    print(i, tensorflow_outputs[i] - pytorch_outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [-2. -2. -2. ... -2. -2. -2.]\n",
      " [-2. -2. -2. ... -2. -2. -2.]]\n",
      "[[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n",
      "[[1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " ...\n",
      " [1.       1.       1.       ... 1.       1.       1.      ]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]\n",
      " [1.999998 1.999998 1.999998 ... 1.999998 1.999998 1.999998]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0] - pytorch_outputs[0])\n",
    "print(tensorflow_outputs[1] - pytorch_outputs[1])\n",
    "print(tensorflow_outputs[11] - pytorch_outputs[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Comparing the standard deviation on the last layer of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:39.437137Z",
     "start_time": "2018-11-15T15:21:39.406150Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:40.181870Z",
     "start_time": "2018-11-15T15:21:40.137023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape tensorflow layer, shape pytorch layer, standard deviation\n",
      "((128, 768), (128, 768), 0.25)\n",
      "((128, 768), (128, 768), 1.0897245)\n",
      "((128, 768), (128, 768), 0.0)\n",
      "((128, 768), (128, 768), 1.0897245)\n",
      "((128, 768), (128, 768), 0.0)\n",
      "((128, 768), (128, 768), 1.0897245)\n",
      "((128, 768), (128, 768), 0.0)\n",
      "((128, 768), (128, 768), 1.0897245)\n",
      "((128, 768), (128, 768), 0.0)\n",
      "((128, 768), (128, 768), 1.0897245)\n",
      "((128, 768), (128, 768), 0.0)\n",
      "((128, 768), (128, 768), 1.0897245)\n"
     ]
    }
   ],
   "source": [
    "print('shape tensorflow layer, shape pytorch layer, standard deviation')\n",
    "print('\\n'.join(list(str((np.array(tensorflow_outputs[i]).shape,\n",
    "                          np.array(pytorch_outputs[i]).shape, \n",
    "                          np.sqrt(np.mean((np.array(tensorflow_outputs[i]) - np.array(pytorch_outputs[i]))**2.0)))) for i in range(12))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "bert-pretraining",
   "language": "python",
   "name": "bert-pretraining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
