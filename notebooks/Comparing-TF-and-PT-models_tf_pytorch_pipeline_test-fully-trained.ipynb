{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing TensorFlow (original) and PyTorch models\n",
    "\n",
    "You can use this small notebook to check the conversion of the model's weights from the TensorFlow model to the PyTorch model. In the following, we compare the weights of the last layer on a simple example (in `input.txt`) but both models returns all the hidden layers so you can check every stage of the model.\n",
    "\n",
    "To run this notebook, follow these instructions:\n",
    "- make sure that your Python environment has both TensorFlow and PyTorch installed,\n",
    "- download the original TensorFlow implementation,\n",
    "- download a pre-trained TensorFlow model as indicaded in the TensorFlow implementation readme,\n",
    "- run the script `convert_tf_checkpoint_to_pytorch.py` as indicated in the `README` to convert the pre-trained TensorFlow model to PyTorch.\n",
    "\n",
    "If needed change the relative paths indicated in this notebook (at the beggining of Sections 1 and 2) to point to the relevent models and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:56:48.412622Z",
     "start_time": "2018-11-15T14:56:48.400110Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 16:02:10.969589 140024053769984 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ TensorFlow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:56:49.483829Z",
     "start_time": "2018-11-15T14:56:49.471296Z"
    }
   },
   "outputs": [],
   "source": [
    "original_tf_inplem_dir = \"../bert/\"\n",
    "model_dir = \"/tmp/pretraining_output_test_final_model/\"\n",
    "\n",
    "vocab_file = model_dir + \"vocab.txt\"\n",
    "bert_config_file = model_dir + \"bert_config.json\"\n",
    "init_checkpoint = model_dir + \"bert_model.ckpt\"\n",
    "\n",
    "input_file = \"./samples/input.txt\"\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:57:51.597932Z",
     "start_time": "2018-11-15T14:57:51.549466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('*', original_tf_inplem_dir + '/extract_features.py')\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "sys.modules['extract_features_tensorflow'] = module\n",
    "sys.path.append('../bert')\n",
    "from extract_features_tensorflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:05.650987Z",
     "start_time": "2018-11-15T14:58:05.541620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 16:02:11.077666 140024053769984 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0702 16:02:11.233513 140024053769984 deprecation_wrapper.py:119] From ../bert//extract_features.py:295: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "layer_indexes = list(range(12))\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=vocab_file, do_lower_case=True)\n",
    "examples = read_examples(input_file)\n",
    "\n",
    "features = convert_examples_to_features(\n",
    "    examples=examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "unique_id_to_feature = {}\n",
    "for feature in features:\n",
    "    unique_id_to_feature[feature.unique_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:11.562443Z",
     "start_time": "2018-11-15T14:58:08.036485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 16:02:11.905027 140024053769984 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0702 16:02:11.907641 140024053769984 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f58f672fbf8>) includes params argument, but params are not passed to Estimator.\n",
      "W0702 16:02:11.911869 140024053769984 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpojjqolic\n",
      "W0702 16:02:11.914280 140024053769984 tpu_context.py:750] Setting TPUConfig.num_shards==1 is an unsupported behavior. Please fix as soon as possible (leaving num_shards as None.)\n",
      "W0702 16:02:11.915335 140024053769984 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    master=None,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        num_shards=1,\n",
    "        per_host_input_for_training=is_per_host))\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=init_checkpoint,\n",
    "    layer_indexes=layer_indexes,\n",
    "    use_tpu=False,\n",
    "    use_one_hot_embeddings=False)\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU.\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    predict_batch_size=1)\n",
    "\n",
    "input_fn = input_fn_builder(\n",
    "    features=features, seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:21.736543Z",
     "start_time": "2018-11-15T14:58:16.723829Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 16:02:12.093593 140024053769984 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0702 16:02:12.096588 140024053769984 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0702 16:02:12.134335 140024053769984 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0702 16:02:12.220436 140024053769984 deprecation.py:323] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0702 16:02:12.731821 140024053769984 deprecation_wrapper.py:119] From /dfs/scratch0/zjian/bert-pretraining/src/bert-pretraining/third_party/bert/modeling.py:871: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0702 16:02:15.188163 140024053769984 deprecation_wrapper.py:119] From ../bert//extract_features.py:196: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert/embeddings/word_embeddings:0\n",
      "bert/embeddings/token_type_embeddings:0\n",
      "bert/embeddings/position_embeddings:0\n",
      "bert/embeddings/LayerNorm/beta:0\n",
      "bert/embeddings/LayerNorm/gamma:0\n",
      "bert/encoder/layer_0/attention/self/query/kernel:0\n",
      "bert/encoder/layer_0/attention/self/query/bias:0\n",
      "bert/encoder/layer_0/attention/self/key/kernel:0\n",
      "bert/encoder/layer_0/attention/self/key/bias:0\n",
      "bert/encoder/layer_0/attention/self/value/kernel:0\n",
      "bert/encoder/layer_0/attention/self/value/bias:0\n",
      "bert/encoder/layer_0/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_0/attention/output/dense/bias:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_0/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_0/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_0/intermediate/dense/bias:0\n",
      "bert/encoder/layer_0/output/dense/kernel:0\n",
      "bert/encoder/layer_0/output/dense/bias:0\n",
      "bert/encoder/layer_0/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_0/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_1/attention/self/query/kernel:0\n",
      "bert/encoder/layer_1/attention/self/query/bias:0\n",
      "bert/encoder/layer_1/attention/self/key/kernel:0\n",
      "bert/encoder/layer_1/attention/self/key/bias:0\n",
      "bert/encoder/layer_1/attention/self/value/kernel:0\n",
      "bert/encoder/layer_1/attention/self/value/bias:0\n",
      "bert/encoder/layer_1/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_1/attention/output/dense/bias:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_1/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_1/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_1/intermediate/dense/bias:0\n",
      "bert/encoder/layer_1/output/dense/kernel:0\n",
      "bert/encoder/layer_1/output/dense/bias:0\n",
      "bert/encoder/layer_1/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_1/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_2/attention/self/query/kernel:0\n",
      "bert/encoder/layer_2/attention/self/query/bias:0\n",
      "bert/encoder/layer_2/attention/self/key/kernel:0\n",
      "bert/encoder/layer_2/attention/self/key/bias:0\n",
      "bert/encoder/layer_2/attention/self/value/kernel:0\n",
      "bert/encoder/layer_2/attention/self/value/bias:0\n",
      "bert/encoder/layer_2/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_2/attention/output/dense/bias:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_2/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_2/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_2/intermediate/dense/bias:0\n",
      "bert/encoder/layer_2/output/dense/kernel:0\n",
      "bert/encoder/layer_2/output/dense/bias:0\n",
      "bert/encoder/layer_2/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_2/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_3/attention/self/query/kernel:0\n",
      "bert/encoder/layer_3/attention/self/query/bias:0\n",
      "bert/encoder/layer_3/attention/self/key/kernel:0\n",
      "bert/encoder/layer_3/attention/self/key/bias:0\n",
      "bert/encoder/layer_3/attention/self/value/kernel:0\n",
      "bert/encoder/layer_3/attention/self/value/bias:0\n",
      "bert/encoder/layer_3/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_3/attention/output/dense/bias:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_3/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_3/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_3/intermediate/dense/bias:0\n",
      "bert/encoder/layer_3/output/dense/kernel:0\n",
      "bert/encoder/layer_3/output/dense/bias:0\n",
      "bert/encoder/layer_3/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_3/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_4/attention/self/query/kernel:0\n",
      "bert/encoder/layer_4/attention/self/query/bias:0\n",
      "bert/encoder/layer_4/attention/self/key/kernel:0\n",
      "bert/encoder/layer_4/attention/self/key/bias:0\n",
      "bert/encoder/layer_4/attention/self/value/kernel:0\n",
      "bert/encoder/layer_4/attention/self/value/bias:0\n",
      "bert/encoder/layer_4/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_4/attention/output/dense/bias:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_4/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_4/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_4/intermediate/dense/bias:0\n",
      "bert/encoder/layer_4/output/dense/kernel:0\n",
      "bert/encoder/layer_4/output/dense/bias:0\n",
      "bert/encoder/layer_4/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_4/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_5/attention/self/query/kernel:0\n",
      "bert/encoder/layer_5/attention/self/query/bias:0\n",
      "bert/encoder/layer_5/attention/self/key/kernel:0\n",
      "bert/encoder/layer_5/attention/self/key/bias:0\n",
      "bert/encoder/layer_5/attention/self/value/kernel:0\n",
      "bert/encoder/layer_5/attention/self/value/bias:0\n",
      "bert/encoder/layer_5/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_5/attention/output/dense/bias:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_5/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_5/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_5/intermediate/dense/bias:0\n",
      "bert/encoder/layer_5/output/dense/kernel:0\n",
      "bert/encoder/layer_5/output/dense/bias:0\n",
      "bert/encoder/layer_5/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_5/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_6/attention/self/query/kernel:0\n",
      "bert/encoder/layer_6/attention/self/query/bias:0\n",
      "bert/encoder/layer_6/attention/self/key/kernel:0\n",
      "bert/encoder/layer_6/attention/self/key/bias:0\n",
      "bert/encoder/layer_6/attention/self/value/kernel:0\n",
      "bert/encoder/layer_6/attention/self/value/bias:0\n",
      "bert/encoder/layer_6/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_6/attention/output/dense/bias:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_6/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_6/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_6/intermediate/dense/bias:0\n",
      "bert/encoder/layer_6/output/dense/kernel:0\n",
      "bert/encoder/layer_6/output/dense/bias:0\n",
      "bert/encoder/layer_6/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_6/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_7/attention/self/query/kernel:0\n",
      "bert/encoder/layer_7/attention/self/query/bias:0\n",
      "bert/encoder/layer_7/attention/self/key/kernel:0\n",
      "bert/encoder/layer_7/attention/self/key/bias:0\n",
      "bert/encoder/layer_7/attention/self/value/kernel:0\n",
      "bert/encoder/layer_7/attention/self/value/bias:0\n",
      "bert/encoder/layer_7/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_7/attention/output/dense/bias:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_7/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_7/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_7/intermediate/dense/bias:0\n",
      "bert/encoder/layer_7/output/dense/kernel:0\n",
      "bert/encoder/layer_7/output/dense/bias:0\n",
      "bert/encoder/layer_7/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_7/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_8/attention/self/query/kernel:0\n",
      "bert/encoder/layer_8/attention/self/query/bias:0\n",
      "bert/encoder/layer_8/attention/self/key/kernel:0\n",
      "bert/encoder/layer_8/attention/self/key/bias:0\n",
      "bert/encoder/layer_8/attention/self/value/kernel:0\n",
      "bert/encoder/layer_8/attention/self/value/bias:0\n",
      "bert/encoder/layer_8/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_8/attention/output/dense/bias:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_8/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_8/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_8/intermediate/dense/bias:0\n",
      "bert/encoder/layer_8/output/dense/kernel:0\n",
      "bert/encoder/layer_8/output/dense/bias:0\n",
      "bert/encoder/layer_8/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_8/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_9/attention/self/query/kernel:0\n",
      "bert/encoder/layer_9/attention/self/query/bias:0\n",
      "bert/encoder/layer_9/attention/self/key/kernel:0\n",
      "bert/encoder/layer_9/attention/self/key/bias:0\n",
      "bert/encoder/layer_9/attention/self/value/kernel:0\n",
      "bert/encoder/layer_9/attention/self/value/bias:0\n",
      "bert/encoder/layer_9/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_9/attention/output/dense/bias:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_9/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_9/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_9/intermediate/dense/bias:0\n",
      "bert/encoder/layer_9/output/dense/kernel:0\n",
      "bert/encoder/layer_9/output/dense/bias:0\n",
      "bert/encoder/layer_9/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_9/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_10/attention/self/query/kernel:0\n",
      "bert/encoder/layer_10/attention/self/query/bias:0\n",
      "bert/encoder/layer_10/attention/self/key/kernel:0\n",
      "bert/encoder/layer_10/attention/self/key/bias:0\n",
      "bert/encoder/layer_10/attention/self/value/kernel:0\n",
      "bert/encoder/layer_10/attention/self/value/bias:0\n",
      "bert/encoder/layer_10/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_10/attention/output/dense/bias:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_10/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_10/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_10/intermediate/dense/bias:0\n",
      "bert/encoder/layer_10/output/dense/kernel:0\n",
      "bert/encoder/layer_10/output/dense/bias:0\n",
      "bert/encoder/layer_10/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_10/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_11/attention/self/query/kernel:0\n",
      "bert/encoder/layer_11/attention/self/query/bias:0\n",
      "bert/encoder/layer_11/attention/self/key/kernel:0\n",
      "bert/encoder/layer_11/attention/self/key/bias:0\n",
      "bert/encoder/layer_11/attention/self/value/kernel:0\n",
      "bert/encoder/layer_11/attention/self/value/bias:0\n",
      "bert/encoder/layer_11/attention/output/dense/kernel:0\n",
      "bert/encoder/layer_11/attention/output/dense/bias:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_11/attention/output/LayerNorm/gamma:0\n",
      "bert/encoder/layer_11/intermediate/dense/kernel:0\n",
      "bert/encoder/layer_11/intermediate/dense/bias:0\n",
      "bert/encoder/layer_11/output/dense/kernel:0\n",
      "bert/encoder/layer_11/output/dense/bias:0\n",
      "bert/encoder/layer_11/output/LayerNorm/beta:0\n",
      "bert/encoder/layer_11/output/LayerNorm/gamma:0\n",
      "bert/pooler/dense/kernel:0\n",
      "bert/pooler/dense/bias:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 16:02:16.202486 140024053769984 deprecation.py:323] From /lfs/1/zjian/anaconda2/envs/bert-pretraining/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention output  [[-0.023517929 0.114575304 0.0559358522 ... -0.138221383 0.265626878 -0.034330003]\n",
      " [0.29820472 -0.256990701 -0.188006207 ... -0.171895146 0.276326776 -0.165664867]\n",
      " [0.221838355 -0.363108665 -0.133449152 ... -0.400731623 0.128412277 -0.185162306]\n",
      " ...\n",
      " [0.176892072 -0.00986768864 -0.00209077634 ... -0.148898095 0.252958655 -0.207604825]\n",
      " [0.180311739 -0.00693849288 0.0158761069 ... -0.158081949 0.261587888 -0.220568925]\n",
      " [0.177134782 -0.0169706102 -0.0248442 ... -0.149772689 0.25775969 -0.228787959]] [[0.168550581 -0.285767317 -0.326125622 ... -0.0275705792 0.0382532738 0.163995281]\n",
      " [-0.153370529 0.337782115 -0.169415116 ... 0.297335207 0.387448281 -0.342025697]\n",
      " [-0.4856323 -0.523777962 0.296871603 ... 0.191184327 -0.067088604 0.320287108]\n",
      " ...\n",
      " [0.0265424848 -0.179814368 0.489417434 ... -0.532132208 -0.246509075 0.465869516]\n",
      " [-0.119451314 -0.232424796 0.24276945 ... -0.473972976 -0.13932699 0.318041414]\n",
      " [0.110764056 -0.0750389695 0.322582066 ... -0.0730810165 -0.513685107 0.228968501]] [128 768]\n",
      "attention output 2  [[0.537707 -0.246248618 -0.647434115 ... -0.0391790867 0.421296358 0.132123142]\n",
      " [0.466789722 0.0767612606 -0.836400449 ... 0.336521238 0.88514322 -0.929269791]\n",
      " [-0.219498426 -1.51986229 -0.0432124138 ... -0.207508504 -0.0255540386 0.066957131]\n",
      " ...\n",
      " [0.740078688 -0.272933334 0.714749336 ... -0.951010227 0.022193674 0.46187821]\n",
      " [0.479899943 -0.356478155 0.300304472 ... -0.853832126 0.232991755 0.166398019]\n",
      " [0.893785536 -0.0877004564 0.365593374 ... -0.121625781 -0.434672534 -0.013774924]] [[0.168550581 -0.285767317 -0.326125622 ... -0.0275705792 0.0382532738 0.163995281]\n",
      " [-0.153370529 0.337782115 -0.169415116 ... 0.297335207 0.387448281 -0.342025697]\n",
      " [-0.4856323 -0.523777962 0.296871603 ... 0.191184327 -0.067088604 0.320287108]\n",
      " ...\n",
      " [0.0265424848 -0.179814368 0.489417434 ... -0.532132208 -0.246509075 0.465869516]\n",
      " [-0.119451314 -0.232424796 0.24276945 ... -0.473972976 -0.13932699 0.318041414]\n",
      " [0.110764056 -0.0750389695 0.322582066 ... -0.0730810165 -0.513685107 0.228968501]] [0.257791549 -0.0307785347 -0.277269691 ... 0.164325908 -0.0902161896 -0.124530174] [0.980340779 0.959969 0.963689864 ... 0.94326 0.94426173 0.984306395]\n",
      "intermediate output  [[-0.00518911 -0.0431444198 -0.0014520383 ... -0.024219675 -0.00149830768 -0.00211256417]\n",
      " [-0.115943253 -0.0026330736 -4.74304215e-05 ... -0.0393870771 -0.0568301603 -0.0679060742]\n",
      " [-0.00342090661 -0.00528098317 -0.00249115564 ... -0.0240728352 -0.0189757384 -0.0528491288]\n",
      " ...\n",
      " [-0.00726787746 -0.0286645573 -0.0163031965 ... -0.0497183241 -0.000898356317 -0.0147930523]\n",
      " [-0.00762771675 -0.026431324 -0.0132509368 ... -0.0550978743 -0.000607106835 -0.00814427249]\n",
      " [-0.0170452986 -0.0341212712 -0.0182950553 ... -0.042145323 -0.00106232648 -0.0136275906]]\n",
      "layer output  [[0.108105436 0.00736203417 -0.141343236 ... 0.080431506 0.0717556328 0.00319920108]\n",
      " [-0.00526232272 0.632794499 -0.298507512 ... 0.105944246 0.0906125307 -0.768247247]\n",
      " [-0.318261206 -0.812070429 0.15033704 ... -0.190059707 0.156868219 0.122468628]\n",
      " ...\n",
      " [0.0941404849 -0.330548942 0.613848567 ... 0.439293742 -0.308622807 0.0601773299]\n",
      " [0.019960016 -0.379841834 0.490459025 ... 0.450618446 -0.215709731 -0.0588730089]\n",
      " [0.152956411 -0.26687181 0.496725738 ... 0.750402093 -0.525361121 -0.109606162]]\n",
      "attention output  [[-0.0340261459 0.00169669837 0.158244669 ... -0.0786447227 -0.0155012095 -0.187625095]\n",
      " [0.098694481 0.0922551081 0.0251167379 ... -0.606961191 -0.0622200146 -0.614077568]\n",
      " [0.0561922267 0.174431592 0.125140548 ... -0.349622309 -0.000356086355 -0.629241049]\n",
      " ...\n",
      " [0.000198205933 0.170456916 0.140192643 ... -0.247586891 -0.0742930621 -0.324163824]\n",
      " [-0.0503108799 0.244785935 0.181031123 ... -0.160151139 -0.131090775 -0.334292352]\n",
      " [0.00152142905 0.0187312178 0.131836772 ... -0.215662822 -0.0842659548 -0.329108536]] [[0.108105436 0.00736203417 -0.141343236 ... 0.080431506 0.0717556328 0.00319920108]\n",
      " [-0.00526232272 0.632794499 -0.298507512 ... 0.105944246 0.0906125307 -0.768247247]\n",
      " [-0.318261206 -0.812070429 0.15033704 ... -0.190059707 0.156868219 0.122468628]\n",
      " ...\n",
      " [0.0941404849 -0.330548942 0.613848567 ... 0.439293742 -0.308622807 0.0601773299]\n",
      " [0.019960016 -0.379841834 0.490459025 ... 0.450618446 -0.215709731 -0.0588730089]\n",
      " [0.152956411 -0.26687181 0.496725738 ... 0.750402093 -0.525361121 -0.109606162]] [128 768]\n",
      "attention output 2  [[0.255610526 0.186551243 -0.0278349612 ... 0.0558428168 0.14078778 -0.313410044]\n",
      " [0.218013719 0.99909842 -0.374217957 ... -0.528101265 0.0631542578 -1.60484731]\n",
      " [-0.241024047 -0.654916704 0.275057435 ... -0.641743958 0.224502087 -0.63736403]\n",
      " ...\n",
      " [0.280230045 -0.0957891345 1.18217552 ... 0.374444664 -0.593871891 -0.414114773]\n",
      " [0.0657399297 -0.0514476746 1.03644109 ... 0.534426689 -0.528369904 -0.629463136]\n",
      " [0.381937861 -0.237994447 0.965590656 ... 0.936250925 -0.95756954 -0.705135882]] [[0.108105436 0.00736203417 -0.141343236 ... 0.080431506 0.0717556328 0.00319920108]\n",
      " [-0.00526232272 0.632794499 -0.298507512 ... 0.105944246 0.0906125307 -0.768247247]\n",
      " [-0.318261206 -0.812070429 0.15033704 ... -0.190059707 0.156868219 0.122468628]\n",
      " ...\n",
      " [0.0941404849 -0.330548942 0.613848567 ... 0.439293742 -0.308622807 0.0601773299]\n",
      " [0.019960016 -0.379841834 0.490459025 ... 0.450618446 -0.215709731 -0.0588730089]\n",
      " [0.152956411 -0.26687181 0.496725738 ... 0.750402093 -0.525361121 -0.109606162]] [0.0858371481 0.141999662 -0.0856637 ... 0.0249954034 0.010392678 0.00804973114] [0.896962 0.871488631 0.853116095 ... 0.864801943 0.861251 0.892963409]\n",
      "intermediate output  [[-0.0922814682 -0.0040939739 -0.014592967 ... -0.0765982121 -0.0177181754 -0.0896503329]\n",
      " [-0.169398546 -0.0938636959 -0.15972136 ... -0.103767201 -0.0177014563 -6.24741e-06]\n",
      " [-0.153946742 -0.00786063541 -0.0487948619 ... -0.0890559331 -0.000852228259 -0.00156996818]\n",
      " ...\n",
      " [-0.0702676326 -0.00660134573 -0.014574172 ... -0.0541407876 -0.0424519107 -0.0793547481]\n",
      " [-0.0469958447 -0.00971146 -0.0244296156 ... -0.0576289967 -0.0422120579 -0.0623549335]\n",
      " [-0.0448089242 -0.00777674327 -0.0203918554 ... -0.04914378 -0.0371510349 -0.0763640106]]\n",
      "layer output  [[-0.118972883 -0.201983735 -0.288469195 ... 0.147986859 0.0903656706 0.0335234664]\n",
      " [-0.429482728 0.547392726 -0.239474744 ... 0.452061623 -0.12681447 -1.3169142]\n",
      " [-0.323326409 -0.238592461 0.110841401 ... -0.259103388 0.244961843 -0.517779]\n",
      " ...\n",
      " [-0.138231605 -0.37295121 0.650605 ... 0.834648609 -0.507532954 -0.0788462311]\n",
      " [-0.27554965 -0.248701811 0.581536651 ... 0.87463969 -0.428448528 -0.167960182]\n",
      " [-0.059597075 -0.363312483 0.517596483 ... 1.05931485 -0.697937667 -0.273919582]]\n",
      "attention output  [[-0.0099306833 -0.0305260867 0.234678909 ... 0.0810171813 0.0697725 -0.177753359]\n",
      " [-0.112235531 0.0860852 0.257534415 ... -0.46553427 0.106592149 -0.0237829182]\n",
      " [-0.104050547 0.264184684 0.224686339 ... 0.13618052 0.177689105 -0.283106744]\n",
      " ...\n",
      " [-0.0428349897 0.0225939378 0.215543285 ... 0.107259288 0.0462259948 -0.225092143]\n",
      " [-0.00659101829 -0.0439085662 0.227730796 ... 0.0890783072 0.0954988375 -0.233941436]\n",
      " [0.000618461519 -0.0566066802 0.210754409 ... 0.089259 0.0840530843 -0.215983793]] [[-0.118972883 -0.201983735 -0.288469195 ... 0.147986859 0.0903656706 0.0335234664]\n",
      " [-0.429482728 0.547392726 -0.239474744 ... 0.452061623 -0.12681447 -1.3169142]\n",
      " [-0.323326409 -0.238592461 0.110841401 ... -0.259103388 0.244961843 -0.517779]\n",
      " ...\n",
      " [-0.138231605 -0.37295121 0.650605 ... 0.834648609 -0.507532954 -0.0788462311]\n",
      " [-0.27554965 -0.248701811 0.581536651 ... 0.87463969 -0.428448528 -0.167960182]\n",
      " [-0.059597075 -0.363312483 0.517596483 ... 1.05931485 -0.697937667 -0.273919582]] [128 768]\n",
      "attention output 2  [[-0.060028255 -0.272804707 -0.261085033 ... 0.419577509 0.190612376 -0.246100441]\n",
      " [-0.421245158 0.832943797 -0.15278694 ... 0.00340127945 -0.108491488 -1.45629132]\n",
      " [-0.377180755 0.170989662 0.229080588 ... -0.135565862 0.397152036 -1.0095681]\n",
      " ...\n",
      " [-0.111517787 -0.39535439 1.12568521 ... 1.38991284 -0.751039863 -0.461394936]\n",
      " [-0.273534983 -0.310603201 1.05553246 ... 1.43906772 -0.571613073 -0.621061]\n",
      " [0.0801331326 -0.505488515 0.92827791 ... 1.70440793 -0.97445035 -0.755702615]] [[-0.118972883 -0.201983735 -0.288469195 ... 0.147986859 0.0903656706 0.0335234664]\n",
      " [-0.429482728 0.547392726 -0.239474744 ... 0.452061623 -0.12681447 -1.3169142]\n",
      " [-0.323326409 -0.238592461 0.110841401 ... -0.259103388 0.244961843 -0.517779]\n",
      " ...\n",
      " [-0.138231605 -0.37295121 0.650605 ... 0.834648609 -0.507532954 -0.0788462311]\n",
      " [-0.27554965 -0.248701811 0.581536651 ... 0.87463969 -0.428448528 -0.167960182]\n",
      " [-0.059597075 -0.363312483 0.517596483 ... 1.05931485 -0.697937667 -0.273919582]] [0.149080902 0.123869553 -0.193820208 ... -0.0042500454 -0.10922385 -0.00815355312] [0.898334324 0.888770759 0.862835944 ... 0.839409649 0.826059878 0.90007478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate output  [[-0.0236262083 0.225160047 -0.0296256319 ... -0.0318246037 -0.0236812346 -0.0225447249]\n",
      " [-0.126183853 -0.15823181 -0.0473857597 ... -0.0720712468 -0.142897129 -0.0187649447]\n",
      " [-0.0771942809 -0.0480017141 -0.0587825067 ... -0.0458443314 -0.075253062 -0.0373694785]\n",
      " ...\n",
      " [-0.0274390765 -0.0692289174 -0.0451274924 ... -0.0139679629 -0.00442458596 -0.000603590044]\n",
      " [-0.0476122051 -0.00895324536 -0.0499093756 ... -0.0192492399 -0.00651343819 -0.000587598188]\n",
      " [-0.0426970534 -0.0222028401 -0.0293225758 ... -0.0238419697 -0.00822856463 -0.000817868859]]\n",
      "layer output  [[-0.100653581 -0.220392287 -0.0911183208 ... 0.150086403 0.0866234899 0.116971701]\n",
      " [-0.787523687 0.498794913 0.475328982 ... 0.310904503 -0.309030294 -1.07281244]\n",
      " [-0.484146148 -0.173144937 0.346873283 ... -0.337559 0.28318426 -0.57993418]\n",
      " ...\n",
      " [-0.368741184 -0.236361772 0.787994862 ... 0.899103582 -0.230881214 -0.262915939]\n",
      " [-0.468671679 -0.18532905 0.690786421 ... 0.904403329 -0.155636147 -0.380672157]\n",
      " [-0.239920035 -0.318679035 0.66877681 ... 1.02020955 -0.360894024 -0.449899822]]\n",
      "attention output  [[-0.130242348 0.0242414437 -0.22140196 ... -0.260228634 -0.0434296057 0.234825626]\n",
      " [0.0969638452 -0.141947538 -0.0567142814 ... -0.276756406 -0.0968288779 -0.0246570408]\n",
      " [-0.0177816376 -0.0546234883 0.00739853457 ... -0.232135668 0.134276822 -0.218694493]\n",
      " ...\n",
      " [0.095332332 -0.12755695 -0.0398277901 ... -0.150049657 -0.115291163 0.0799164474]\n",
      " [0.0813385099 -0.109095544 -0.0777500123 ... -0.125638247 -0.0809984282 0.045314759]\n",
      " [0.0855137929 -0.0854434371 -0.0745003521 ... -0.131934449 -0.0876978934 0.0351861641]] [[-0.100653581 -0.220392287 -0.0911183208 ... 0.150086403 0.0866234899 0.116971701]\n",
      " [-0.787523687 0.498794913 0.475328982 ... 0.310904503 -0.309030294 -1.07281244]\n",
      " [-0.484146148 -0.173144937 0.346873283 ... -0.337559 0.28318426 -0.57993418]\n",
      " ...\n",
      " [-0.368741184 -0.236361772 0.787994862 ... 0.899103582 -0.230881214 -0.262915939]\n",
      " [-0.468671679 -0.18532905 0.690786421 ... 0.904403329 -0.155636147 -0.380672157]\n",
      " [-0.239920035 -0.318679035 0.66877681 ... 1.02020955 -0.360894024 -0.449899822]] [128 768]\n",
      "attention output 2  [[-0.296993822 -0.320544899 -0.508050203 ... -0.122056425 0.0160390213 0.699664593]\n",
      " [-0.722690821 0.442084074 0.472385734 ... 0.0860684142 -0.497056872 -1.26662493]\n",
      " [-0.548949599 -0.273302585 0.433599 ... -0.610806704 0.435465395 -0.992278695]\n",
      " ...\n",
      " [-0.307894021 -0.530042171 1.08552825 ... 1.08139181 -0.538991 -0.220108762]\n",
      " [-0.484908342 -0.421813816 0.891939938 ... 1.12254167 -0.383167922 -0.456216782]\n",
      " [-0.116380498 -0.583334804 0.856140733 ... 1.26030505 -0.674753785 -0.574503124]] [[-0.100653581 -0.220392287 -0.0911183208 ... 0.150086403 0.0866234899 0.116971701]\n",
      " [-0.787523687 0.498794913 0.475328982 ... 0.310904503 -0.309030294 -1.07281244]\n",
      " [-0.484146148 -0.173144937 0.346873283 ... -0.337559 0.28318426 -0.57993418]\n",
      " ...\n",
      " [-0.368741184 -0.236361772 0.787994862 ... 0.899103582 -0.230881214 -0.262915939]\n",
      " [-0.468671679 -0.18532905 0.690786421 ... 0.904403329 -0.155636147 -0.380672157]\n",
      " [-0.239920035 -0.318679035 0.66877681 ... 1.02020955 -0.360894024 -0.449899822]] [0.0981540382 0.00063773786 -0.0125773298 ... 0.0312602445 -0.07508751 0.0416950546] [0.914577246 0.884538233 0.834088683 ... 0.7967242 0.814768136 0.90850848]\n",
      "intermediate output  [[-0.169389501 -0.0759828687 -0.0178416912 ... -0.0805133507 -0.121945605 -0.0670827404]\n",
      " [-0.0378501378 -0.000316644204 -0.079243578 ... 0.0849989653 -0.0171110984 -0.0405739695]\n",
      " [-0.149050251 -0.168234244 0.0739016384 ... -0.160961434 -0.111801952 -0.0485367179]\n",
      " ...\n",
      " [-0.146640956 -0.034874443 -0.000215374312 ... -0.113573886 0.0233505238 0.114977606]\n",
      " [-0.085836567 -0.0784648731 -0.00026682709 ... -0.160185337 0.0803736 0.127902776]\n",
      " [-0.128161356 -0.081315 -0.000116885647 ... -0.119229034 0.0880590677 0.35018757]]\n",
      "layer output  [[-0.169569731 -0.529830217 -0.62387228 ... -0.120107025 0.160947 0.47763437]\n",
      " [-0.793211758 -0.110297292 0.711028159 ... 0.456698328 -0.543633223 -1.38960171]\n",
      " [-0.468822777 -0.15283072 0.34094727 ... -0.465970039 0.19818446 -0.189804748]\n",
      " ...\n",
      " [-0.662910819 -0.849151254 0.753567696 ... 0.529063046 -0.74996841 -0.158631086]\n",
      " [-0.689676404 -0.77355957 0.714267552 ... 0.685326397 -0.635191143 -0.303250968]\n",
      " [-0.414518178 -0.891487181 0.825420797 ... 0.830594897 -0.78763783 -0.437175453]]\n",
      "attention output  [[-0.328539371 0.520342946 -0.454365432 ... -0.231119677 0.214161783 -0.205012634]\n",
      " [0.156322286 0.0693834797 -0.476114243 ... -0.0907431841 -0.160551146 0.00437956862]\n",
      " [-0.0267531779 -0.241154358 -0.173049152 ... 0.010489393 0.227822796 0.0801527947]\n",
      " ...\n",
      " [-0.0870927423 -0.0115457308 0.12048544 ... 0.106430992 -0.372927576 0.122010767]\n",
      " [-0.0252559539 -0.00905707106 0.111340091 ... 0.162108466 -0.361757338 0.0543652326]\n",
      " [-0.0441830382 0.0212276205 0.0506846868 ... 0.187970012 -0.307445318 0.0659665912]] [[-0.169569731 -0.529830217 -0.62387228 ... -0.120107025 0.160947 0.47763437]\n",
      " [-0.793211758 -0.110297292 0.711028159 ... 0.456698328 -0.543633223 -1.38960171]\n",
      " [-0.468822777 -0.15283072 0.34094727 ... -0.465970039 0.19818446 -0.189804748]\n",
      " ...\n",
      " [-0.662910819 -0.849151254 0.753567696 ... 0.529063046 -0.74996841 -0.158631086]\n",
      " [-0.689676404 -0.77355957 0.714267552 ... 0.685326397 -0.635191143 -0.303250968]\n",
      " [-0.414518178 -0.891487181 0.825420797 ... 0.830594897 -0.78763783 -0.437175453]] [128 768]\n",
      "attention output 2  [[-0.568322599 0.0206002109 -1.20339334 ... -0.357542276 0.461533546 0.363262057]\n",
      " [-0.638591468 -0.00823950395 0.228029594 ... 0.34898895 -0.597959101 -1.42289793]\n",
      " [-0.489768773 -0.379544109 0.155479655 ... -0.407148302 0.452906 -0.0829081088]\n",
      " ...\n",
      " [-0.883528829 -1.00596297 0.994607449 ... 0.695834637 -1.15738511 -0.00201599672]\n",
      " [-0.844702482 -0.916087031 0.946199179 ... 0.928442836 -1.02594805 -0.263251662]\n",
      " [-0.530004621 -1.03012824 1.00889611 ... 1.11798549 -1.14125681 -0.41755873]] [[-0.169569731 -0.529830217 -0.62387228 ... -0.120107025 0.160947 0.47763437]\n",
      " [-0.793211758 -0.110297292 0.711028159 ... 0.456698328 -0.543633223 -1.38960171]\n",
      " [-0.468822777 -0.15283072 0.34094727 ... -0.465970039 0.19818446 -0.189804748]\n",
      " ...\n",
      " [-0.662910819 -0.849151254 0.753567696 ... 0.529063046 -0.74996841 -0.158631086]\n",
      " [-0.689676404 -0.77355957 0.714267552 ... 0.685326397 -0.635191143 -0.303250968]\n",
      " [-0.414518178 -0.891487181 0.825420797 ... 0.830594897 -0.78763783 -0.437175453]] [0.00903359056 0.00522850314 -0.0284148846 ... -0.0128962044 0.0361828916 0.00830953848] [0.884967625 0.869278133 0.811459482 ... 0.764229596 0.780300558 0.877373755]\n",
      "intermediate output  [[-0.0561004728 -0.0182638653 -0.127918765 ... -0.081226252 -0.150613323 -0.0213311929]\n",
      " [-0.0721619278 -0.169798076 -0.125731066 ... -0.134894833 -0.0978181809 -0.0922915339]\n",
      " [-0.109935962 -0.143016055 -0.0489976592 ... -0.136566952 -0.0681679696 -0.0159594808]\n",
      " ...\n",
      " [-0.076994814 -0.0234478451 -0.100473292 ... -0.1685379 -0.113997832 -0.0551671945]\n",
      " [-0.0934432372 -0.0291666854 -0.0812763199 ... -0.170006812 -0.125943303 -0.0492757261]\n",
      " [-0.0864503831 -0.032041084 -0.0659126267 ... -0.169744059 -0.148749858 -0.0513549671]]\n",
      "layer output  [[-0.540071845 0.0500272587 -0.89557606 ... -0.26914373 0.430562019 0.31089741]\n",
      " [-0.453103513 -0.578923285 0.0848933682 ... 0.671331525 -0.479961693 -1.5545336]\n",
      " [-0.740642667 -0.710118771 0.138602927 ... -0.660527587 0.799326 -0.0161202643]\n",
      " ...\n",
      " [-0.622550726 -0.865465224 0.616362691 ... -0.075886175 -0.572800457 -0.0834857]\n",
      " [-0.673786938 -0.760952711 0.814807832 ... 0.216668099 -0.59974736 -0.396390229]\n",
      " [-0.369158477 -0.896872103 0.967870295 ... 0.399232864 -0.666993618 -0.484312385]]\n",
      "attention output  [[0.0723430067 0.40309751 -0.384515047 ... 0.592145324 0.0832742527 0.0190091059]\n",
      " [-0.058594279 0.215073109 -0.00112644956 ... 0.272381037 0.00793376565 0.0598185807]\n",
      " [0.178559437 0.212551534 -0.662183642 ... 0.0971595943 0.234895214 -0.331379294]\n",
      " ...\n",
      " [0.00107131805 -0.0322795175 -0.0527756773 ... 0.0393784866 -0.0390162244 -0.0284465142]\n",
      " [-0.0301252883 -0.00869526342 -0.0501251519 ... 0.0110317226 -0.0409987122 -0.0321062803]\n",
      " [-0.0475282744 -0.0322071798 -0.0583921671 ... -0.00515568629 -0.0428393297 -0.0463125594]] [[-0.540071845 0.0500272587 -0.89557606 ... -0.26914373 0.430562019 0.31089741]\n",
      " [-0.453103513 -0.578923285 0.0848933682 ... 0.671331525 -0.479961693 -1.5545336]\n",
      " [-0.740642667 -0.710118771 0.138602927 ... -0.660527587 0.799326 -0.0161202643]\n",
      " ...\n",
      " [-0.622550726 -0.865465224 0.616362691 ... -0.075886175 -0.572800457 -0.0834857]\n",
      " [-0.673786938 -0.760952711 0.814807832 ... 0.216668099 -0.59974736 -0.396390229]\n",
      " [-0.369158477 -0.896872103 0.967870295 ... 0.399232864 -0.666993618 -0.484312385]] [128 768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention output 2  [[-0.542055726 0.478041977 -1.12282073 ... 0.341569662 0.503791213 0.416829735]\n",
      " [-0.584998548 -0.37038058 0.20044589 ... 0.897326827 -0.440936 -1.53594208]\n",
      " [-0.583883166 -0.462254822 -0.340663493 ... -0.401084423 0.913186729 -0.272743911]\n",
      " ...\n",
      " [-0.841303229 -1.13098419 0.799435079 ... 0.0242231227 -0.700819135 -0.0711985528]\n",
      " [-0.953500748 -0.972590446 1.04508686 ... 0.316271901 -0.739231825 -0.491385102]\n",
      " [-0.577084482 -1.17544258 1.21596551 ... 0.498981386 -0.819508195 -0.625325918]] [[-0.540071845 0.0500272587 -0.89557606 ... -0.26914373 0.430562019 0.31089741]\n",
      " [-0.453103513 -0.578923285 0.0848933682 ... 0.671331525 -0.479961693 -1.5545336]\n",
      " [-0.740642667 -0.710118771 0.138602927 ... -0.660527587 0.799326 -0.0161202643]\n",
      " ...\n",
      " [-0.622550726 -0.865465224 0.616362691 ... -0.075886175 -0.572800457 -0.0834857]\n",
      " [-0.673786938 -0.760952711 0.814807832 ... 0.216668099 -0.59974736 -0.396390229]\n",
      " [-0.369158477 -0.896872103 0.967870295 ... 0.399232864 -0.666993618 -0.484312385]] [-0.0684132725 -0.0146848019 0.0979247615 ... 0.0356461369 -0.00900822 0.0419348739] [0.890831113 0.878847241 0.816372931 ... 0.752956927 0.81053412 0.904479325]\n",
      "intermediate output  [[-0.0375013612 -0.130561247 -0.0236740094 ... -0.0506873094 -0.0628588721 -0.0143145779]\n",
      " [-0.158785969 -0.0241474286 -0.166451 ... -0.00108454563 -0.0742946193 -0.131230026]\n",
      " [-0.0224612318 -0.0041698385 -0.148642242 ... -0.0018642447 -0.142208189 -0.000352089119]\n",
      " ...\n",
      " [-0.158456862 -0.0322784297 0.863670349 ... -0.0626413 -0.0285613313 -0.0760730356]\n",
      " [-0.164298445 -0.0382101238 0.828193545 ... -0.0559950881 -0.0194328614 -0.0472845063]\n",
      " [-0.166575566 -0.0108715184 0.850932181 ... -0.0670429 -0.00795055181 -0.0535558946]]\n",
      "layer output  [[-0.51608485 0.546453953 -1.46123087 ... -0.174838573 0.116423167 0.252306789]\n",
      " [-0.511611342 -0.560762 0.33194834 ... 0.699359953 -0.427939594 -1.62982821]\n",
      " [-0.41746521 -0.553072453 -0.204212606 ... -0.653351605 0.617670238 -0.0546123534]\n",
      " ...\n",
      " [-1.04041243 -0.973129153 0.401195258 ... 0.078977786 -0.602868319 0.0604954809]\n",
      " [-1.01834738 -0.887426376 0.838321149 ... 0.42651695 -0.640472949 -0.270070076]\n",
      " [-0.684822559 -1.01212418 0.94070363 ... 0.468138516 -0.717011154 -0.429864138]]\n",
      "attention output  [[0.284343451 0.500753284 -0.30748722 ... 0.12584652 0.523161 0.299182594]\n",
      " [-0.0837710723 -0.190506831 -0.207912043 ... -0.308470607 0.266357034 -0.0998648703]\n",
      " [0.111560188 -0.360401869 -0.301420271 ... -0.179041386 0.351755112 -0.23874554]\n",
      " ...\n",
      " [0.0651326627 0.15049845 -0.100320578 ... -0.127453551 0.503728271 0.0866642296]\n",
      " [-0.0120798536 0.0494506918 -0.00650921836 ... -0.124356851 0.325843096 0.00531567633]\n",
      " [-0.0422676578 -0.0166356787 -0.0313484855 ... -0.113018245 0.195222631 -0.0206707958]] [[-0.51608485 0.546453953 -1.46123087 ... -0.174838573 0.116423167 0.252306789]\n",
      " [-0.511611342 -0.560762 0.33194834 ... 0.699359953 -0.427939594 -1.62982821]\n",
      " [-0.41746521 -0.553072453 -0.204212606 ... -0.653351605 0.617670238 -0.0546123534]\n",
      " ...\n",
      " [-1.04041243 -0.973129153 0.401195258 ... 0.078977786 -0.602868319 0.0604954809]\n",
      " [-1.01834738 -0.887426376 0.838321149 ... 0.42651695 -0.640472949 -0.270070076]\n",
      " [-0.684822559 -1.01212418 0.94070363 ... 0.468138516 -0.717011154 -0.429864138]] [128 768]\n",
      "attention output 2  [[-0.277687192 1.12612283 -1.47419238 ... 0.127880186 0.588142335 0.618135333]\n",
      " [-0.671978712 -0.722633839 0.301057398 ... 0.520089746 -0.162705019 -1.78288269]\n",
      " [-0.3547948 -0.861742437 -0.288439035 ... -0.555783093 0.884324729 -0.254752696]\n",
      " ...\n",
      " [-1.15756547 -0.864193261 0.498493075 ... 0.125082642 -0.107483707 0.218726337]\n",
      " [-1.27100813 -0.921089411 1.08933794 ... 0.480310798 -0.342562884 -0.264849126]\n",
      " [-0.924541235 -1.17038119 1.19573915 ... 0.542839289 -0.57652849 -0.497295022]] [[-0.51608485 0.546453953 -1.46123087 ... -0.174838573 0.116423167 0.252306789]\n",
      " [-0.511611342 -0.560762 0.33194834 ... 0.699359953 -0.427939594 -1.62982821]\n",
      " [-0.41746521 -0.553072453 -0.204212606 ... -0.653351605 0.617670238 -0.0546123534]\n",
      " ...\n",
      " [-1.04041243 -0.973129153 0.401195258 ... 0.078977786 -0.602868319 0.0604954809]\n",
      " [-1.01834738 -0.887426376 0.838321149 ... 0.42651695 -0.640472949 -0.270070076]\n",
      " [-0.684822559 -1.01212418 0.94070363 ... 0.468138516 -0.717011154 -0.429864138]] [-0.0679363906 0.0315778293 0.15647687 ... 0.144964859 -0.0341589674 0.0180374514] [0.888236105 0.870490491 0.802891731 ... 0.751574576 0.796418786 0.884449363]\n",
      "intermediate output  [[-0.0938235372 0.252755314 -0.169785574 ... -0.0716987848 -0.0137447 -0.000287332]\n",
      " [-0.0185469408 -0.0631187484 -0.155528948 ... -0.0820669532 0.270720243 -0.00147276546]\n",
      " [-0.158009574 -0.169606343 -0.165939763 ... -0.0137212314 -0.0890502334 -0.0205251016]\n",
      " ...\n",
      " [-0.163379386 -0.0867814794 -0.10030745 ... -0.109050535 -0.071496442 -0.00347353634]\n",
      " [-0.120645143 -0.0706172585 -0.133358434 ... -0.126507789 -0.1047929 -0.00723321363]\n",
      " [-0.0931096748 -0.0815554783 -0.143187553 ... -0.122004375 -0.0422790572 -0.00118286733]]\n",
      "layer output  [[-0.0830383673 0.721672297 -1.3987335 ... -0.516010702 0.0321163945 0.92033565]\n",
      " [-0.97245276 -0.62165457 -0.0824400336 ... 0.326395661 -0.0808705911 -1.54629779]\n",
      " [-0.155772626 -0.796285689 -0.200591624 ... -0.543482363 0.346468568 -0.344308376]\n",
      " ...\n",
      " [-0.850765586 -0.903651178 -0.340040773 ... -0.0387508869 0.0519323871 0.561647058]\n",
      " [-1.00133502 -0.83296597 0.667963 ... 0.403934419 -0.210712 0.0103221703]\n",
      " [-0.698439479 -0.978192329 0.825241089 ... 0.551246405 -0.426704079 -0.248080015]]\n",
      "attention output  [[0.223061368 0.0951648653 -0.0945340544 ... 0.0175405294 0.0483553968 0.013845589]\n",
      " [0.143637896 -0.155754447 -0.159134969 ... -0.262658864 0.144935742 -0.0440141261]\n",
      " [0.211256951 -0.178935081 -0.117405772 ... -0.134702116 0.55280745 -0.0735832453]\n",
      " ...\n",
      " [0.170015872 0.0482790768 0.17098698 ... 0.018162027 0.0886099637 0.0101735219]\n",
      " [0.125646815 0.0315373391 0.148921177 ... -0.0730153099 0.0676989853 -0.0494807065]\n",
      " [0.106967077 0.0222705752 0.181699693 ... -0.0609251671 0.0514365248 -0.0485681593]] [[-0.0830383673 0.721672297 -1.3987335 ... -0.516010702 0.0321163945 0.92033565]\n",
      " [-0.97245276 -0.62165457 -0.0824400336 ... 0.326395661 -0.0808705911 -1.54629779]\n",
      " [-0.155772626 -0.796285689 -0.200591624 ... -0.543482363 0.346468568 -0.344308376]\n",
      " ...\n",
      " [-0.850765586 -0.903651178 -0.340040773 ... -0.0387508869 0.0519323871 0.561647058]\n",
      " [-1.00133502 -0.83296597 0.667963 ... 0.403934419 -0.210712 0.0103221703]\n",
      " [-0.698439479 -0.978192329 0.825241089 ... 0.551246405 -0.426704079 -0.248080015]] [128 768]\n",
      "attention output 2  [[0.166412324 1.04167449 -1.52977586 ... -0.38874495 0.13730906 1.1305418]\n",
      " [-0.914047241 -0.781449914 -0.159442022 ... 0.191909254 0.113929085 -1.74739182]\n",
      " [0.0532144 -0.947642744 -0.222935259 ... -0.492821455 0.898461878 -0.430673212]\n",
      " ...\n",
      " [-0.802224696 -0.931483 -0.0978253186 ... 0.110332295 0.201191872 0.689884365]\n",
      " [-1.03225696 -0.868268907 0.956801534 ... 0.474264801 -0.10080485 -0.0321091823]\n",
      " [-0.7052688 -1.06440854 1.17619896 ... 0.647040784 -0.353757173 -0.340986878]] [[-0.0830383673 0.721672297 -1.3987335 ... -0.516010702 0.0321163945 0.92033565]\n",
      " [-0.97245276 -0.62165457 -0.0824400336 ... 0.326395661 -0.0808705911 -1.54629779]\n",
      " [-0.155772626 -0.796285689 -0.200591624 ... -0.543482363 0.346468568 -0.344308376]\n",
      " ...\n",
      " [-0.850765586 -0.903651178 -0.340040773 ... -0.0387508869 0.0519323871 0.561647058]\n",
      " [-1.00133502 -0.83296597 0.667963 ... 0.403934419 -0.210712 0.0103221703]\n",
      " [-0.698439479 -0.978192329 0.825241089 ... 0.551246405 -0.426704079 -0.248080015]] [-0.0282684434 0.0442759134 0.0567832552 ... 0.106625095 0.0252181627 -0.0155804185] [0.874214113 0.870608 0.791476846 ... 0.764467955 0.792246282 0.87781626]\n",
      "intermediate output  [[0.439468086 -0.115604632 -0.0880283 ... -0.0122854654 -0.00508466223 -0.146816909]\n",
      " [-0.127006754 -0.157727614 -0.097000137 ... -0.161240891 -0.0383466184 -0.155270949]\n",
      " [-0.169745043 -0.121666774 -0.00592684653 ... -0.168469682 -0.114991263 -0.167317167]\n",
      " ...\n",
      " [-0.158328786 -0.155238241 -0.0421470664 ... -0.0155104892 -0.020696044 -0.0919029862]\n",
      " [-0.127900168 -0.159228429 -0.0816640854 ... -0.0378955714 -0.0107044969 -0.0993807837]\n",
      " [-0.13034986 -0.169979781 -0.0903561264 ... -0.0556169301 -0.00536346808 -0.0656361058]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer output  [[-0.400504857 0.599968612 -1.27780664 ... -0.600738645 0.228174895 0.81543082]\n",
      " [-0.712510586 -1.04387331 -0.232637823 ... 0.313348114 -0.189448506 -0.8807531]\n",
      " [0.0555186719 -1.34353602 -0.183687896 ... -0.0756268799 0.567102432 0.0978050828]\n",
      " ...\n",
      " [-1.00748932 -0.849740386 -0.274659723 ... -0.37996307 -0.30851987 0.374443352]\n",
      " [-1.08780396 -0.678281724 0.680303454 ... 0.0840016752 -0.321386844 -0.18527]\n",
      " [-0.826690137 -0.7939924 0.769347608 ... 0.313351929 -0.518493056 -0.396081537]]\n",
      "attention output  [[-0.129962802 0.0542304479 -0.208217412 ... 0.0465455838 0.215447828 0.0425155573]\n",
      " [0.00484702084 -0.162631944 -0.267138511 ... 0.0203325115 0.0877325 -0.049696736]\n",
      " [-0.0492953248 -0.0936832577 -0.261444569 ... 0.108127385 0.17607455 -0.117583416]\n",
      " ...\n",
      " [0.0138748037 -0.154966667 0.0644498095 ... 0.0145573281 -0.0305626262 0.0455522202]\n",
      " [0.073379904 0.00129055604 0.0453381464 ... -0.0243445337 0.0395112932 0.0331408866]\n",
      " [0.095911853 -0.0487403683 0.0871657804 ... 0.0700197816 0.0609976128 -0.00556480512]] [[-0.400504857 0.599968612 -1.27780664 ... -0.600738645 0.228174895 0.81543082]\n",
      " [-0.712510586 -1.04387331 -0.232637823 ... 0.313348114 -0.189448506 -0.8807531]\n",
      " [0.0555186719 -1.34353602 -0.183687896 ... -0.0756268799 0.567102432 0.0978050828]\n",
      " ...\n",
      " [-1.00748932 -0.849740386 -0.274659723 ... -0.37996307 -0.30851987 0.374443352]\n",
      " [-1.08780396 -0.678281724 0.680303454 ... 0.0840016752 -0.321386844 -0.18527]\n",
      " [-0.826690137 -0.7939924 0.769347608 ... 0.313351929 -0.518493056 -0.396081537]] [128 768]\n",
      "attention output 2  [[-0.741680384 0.96396178 -1.70975018 ... -0.617103 0.488361061 1.12257707]\n",
      " [-0.883250296 -1.38113976 -0.434236884 ... 0.380115271 -0.167724818 -1.13014853]\n",
      " [-0.022334639 -1.61793947 -0.363298655 ... 0.0617472939 0.735172153 -0.0462864116]\n",
      " ...\n",
      " [-1.13241529 -1.03949368 -0.0957439 ... -0.321637422 -0.397028595 0.440583]\n",
      " [-1.06416762 -0.613831937 0.792406082 ... 0.0834098682 -0.316916168 -0.174325824]\n",
      " [-0.785952687 -0.796847343 0.927571535 ... 0.376356423 -0.486002058 -0.431843728]] [[-0.400504857 0.599968612 -1.27780664 ... -0.600738645 0.228174895 0.81543082]\n",
      " [-0.712510586 -1.04387331 -0.232637823 ... 0.313348114 -0.189448506 -0.8807531]\n",
      " [0.0555186719 -1.34353602 -0.183687896 ... -0.0756268799 0.567102432 0.0978050828]\n",
      " ...\n",
      " [-1.00748932 -0.849740386 -0.274659723 ... -0.37996307 -0.30851987 0.374443352]\n",
      " [-1.08780396 -0.678281724 0.680303454 ... 0.0840016752 -0.321386844 -0.18527]\n",
      " [-0.826690137 -0.7939924 0.769347608 ... 0.313351929 -0.518493056 -0.396081537]] [-0.0600426197 0.0457274951 0.0868810862 ... 0.00216377643 -0.0855252072 -0.0532591268] [0.890753388 0.891165733 0.811639 ... 0.772925675 0.80692488 0.878016293]\n",
      "intermediate output  [[-0.157137379 -0.131165043 -0.124034941 ... -0.14963448 -0.017104907 0.0219463091]\n",
      " [-0.0175336767 -0.102886923 -0.088428691 ... -0.148629352 -0.114627779 -0.0290930178]\n",
      " [-0.0117727118 -0.00566340378 -0.15011546 ... -0.13876009 -0.016831398 -0.000905978319]\n",
      " ...\n",
      " [-0.0493988097 -0.0957295746 -0.0817658827 ... 0.36182633 -0.10023 -0.0254477691]\n",
      " [-0.0261745788 0.115170985 -0.0560152456 ... 0.146144286 -0.0969337821 -0.0137287658]\n",
      " [-0.0261453744 -0.123968214 -0.0210674237 ... 0.0286845397 -0.096879 -0.0387956724]]\n",
      "layer output  [[-0.819203854 0.519498408 -1.03291667 ... -0.29164362 0.408054739 0.579220355]\n",
      " [-0.760731399 -1.08927131 -0.154260054 ... 0.0964047238 -0.206818044 -0.735123456]\n",
      " [0.0944127068 -1.24785221 0.0789177418 ... 0.394479394 0.173955858 -0.0913455561]\n",
      " ...\n",
      " [-0.864811122 -0.518995821 -0.22545734 ... -0.49712792 -0.253852934 0.326853603]\n",
      " [-0.860202551 -0.321504116 0.379164785 ... -0.0729266778 -0.162820429 -0.077866748]\n",
      " [-0.693905115 -0.680282235 0.451064318 ... 0.361693591 -0.222002566 -0.493348]]\n",
      "attention output  [[0.0158502162 0.0886501595 0.213158607 ... 0.151266366 0.389202356 -0.111425035]\n",
      " [0.058893837 -0.178244174 -0.0662226677 ... -0.000683181 0.457879782 -0.221713841]\n",
      " [-0.00435155258 -0.0797985 -0.137256831 ... -0.00164923817 0.446035385 -0.0974567831]\n",
      " ...\n",
      " [0.0716451406 -0.260189414 0.055032298 ... 0.0938108489 0.0194473565 -0.0717250481]\n",
      " [0.0463135652 -0.273674965 -0.0210360661 ... 0.12503764 0.0948248208 -0.0712085366]\n",
      " [0.0306386873 -0.226199776 -0.0287893191 ... 0.152021199 0.0542077944 -0.016052179]] [[-0.819203854 0.519498408 -1.03291667 ... -0.29164362 0.408054739 0.579220355]\n",
      " [-0.760731399 -1.08927131 -0.154260054 ... 0.0964047238 -0.206818044 -0.735123456]\n",
      " [0.0944127068 -1.24785221 0.0789177418 ... 0.394479394 0.173955858 -0.0913455561]\n",
      " ...\n",
      " [-0.864811122 -0.518995821 -0.22545734 ... -0.49712792 -0.253852934 0.326853603]\n",
      " [-0.860202551 -0.321504116 0.379164785 ... -0.0729266778 -0.162820429 -0.077866748]\n",
      " [-0.693905115 -0.680282235 0.451064318 ... 0.361693591 -0.222002566 -0.493348]] [128 768]\n",
      "attention output 2  [[-1.17201006 0.782267809 -0.961824059 ... -0.198068649 0.984714389 0.615102589]\n",
      " [-0.872661591 -1.45807576 -0.143217668 ... 0.0747331455 0.225483552 -1.15242279]\n",
      " [0.0389047265 -1.48151159 0.0323737077 ... 0.369964719 0.601873279 -0.252535]\n",
      " ...\n",
      " [-0.870971262 -0.80779016 -0.0644528717 ... -0.392468065 -0.26514259 0.230158031]\n",
      " [-0.824339867 -0.576328397 0.413025558 ... 0.0233583897 -0.100815304 -0.175480992]\n",
      " [-0.683868587 -0.86315912 0.469631374 ... 0.414685935 -0.189231724 -0.519014895]] [[-0.819203854 0.519498408 -1.03291667 ... -0.29164362 0.408054739 0.579220355]\n",
      " [-0.760731399 -1.08927131 -0.154260054 ... 0.0964047238 -0.206818044 -0.735123456]\n",
      " [0.0944127068 -1.24785221 0.0789177418 ... 0.394479394 0.173955858 -0.0913455561]\n",
      " ...\n",
      " [-0.864811122 -0.518995821 -0.22545734 ... -0.49712792 -0.253852934 0.326853603]\n",
      " [-0.860202551 -0.321504116 0.379164785 ... -0.0729266778 -0.162820429 -0.077866748]\n",
      " [-0.693905115 -0.680282235 0.451064318 ... 0.361693591 -0.222002566 -0.493348]] [-0.0896339118 -0.0636223555 0.0676668957 ... -0.0503544882 -0.0717453957 -0.0670724] [0.851008832 0.825697362 0.792793095 ... 0.764177501 0.793072343 0.856875539]\n",
      "intermediate output  [[-0.0801594332 -0.138160855 -0.0636577 ... -0.167247713 0.617574573 -0.103102937]\n",
      " [-0.160099551 -0.0895257369 -0.0588004515 ... -0.11603836 -0.169228539 -0.0276617464]\n",
      " [-0.0927760154 -0.125425607 -0.167041734 ... -0.169587806 -0.0593688637 -0.16277048]\n",
      " ...\n",
      " [-0.109420732 -0.166434348 -0.078942515 ... -0.170032367 -0.169841126 -0.121335201]\n",
      " [-0.0651475266 -0.165819407 -0.0988188 ... -0.166035816 -0.161775455 -0.115620121]\n",
      " [-0.0395622365 -0.158740476 -0.0934085622 ... -0.169517264 -0.168526381 -0.101956718]]\n",
      "layer output  [[-1.08503759 0.310666829 -0.787685096 ... -0.354312062 0.961511433 0.308173418]\n",
      " [-0.904021144 -0.938276172 -0.226376474 ... -0.0783001781 0.204731971 -0.782903135]\n",
      " [0.113878921 -1.20160544 -0.432717085 ... 0.287103 0.466878831 0.180988446]\n",
      " ...\n",
      " [-1.09340322 -0.563867033 0.0999528319 ... -0.0922525376 0.132861629 0.179888025]\n",
      " [-1.01952207 -0.382054985 0.492108732 ... 0.268101692 0.107485995 -0.186361939]\n",
      " [-0.936152816 -0.72997874 0.64133662 ... 0.601996183 -0.0500781164 -0.319172084]]\n",
      "attention output  [[0.0942497551 -0.126347527 0.0839450657 ... -0.367151141 -0.0894336402 0.0918525]\n",
      " [0.0235285703 0.0670340359 0.0690439939 ... 0.189982355 0.00476926565 -0.0845474]\n",
      " [0.125043109 0.00660941191 0.135178015 ... 0.0414370373 0.0933162495 0.0225579739]\n",
      " ...\n",
      " [-0.113234624 0.245220855 -0.0844614208 ... 0.0272579491 -0.03493426 -0.0152125284]\n",
      " [-0.151971295 0.212651446 -0.0372295305 ... 0.0287049189 -0.0310257599 -0.0582117066]\n",
      " [-0.220676497 0.194279209 -0.0225864053 ... 0.0702434257 0.0271332711 -0.0729206651]] [[-1.08503759 0.310666829 -0.787685096 ... -0.354312062 0.961511433 0.308173418]\n",
      " [-0.904021144 -0.938276172 -0.226376474 ... -0.0783001781 0.204731971 -0.782903135]\n",
      " [0.113878921 -1.20160544 -0.432717085 ... 0.287103 0.466878831 0.180988446]\n",
      " ...\n",
      " [-1.09340322 -0.563867033 0.0999528319 ... -0.0922525376 0.132861629 0.179888025]\n",
      " [-1.01952207 -0.382054985 0.492108732 ... 0.268101692 0.107485995 -0.186361939]\n",
      " [-0.936152816 -0.72997874 0.64133662 ... 0.601996183 -0.0500781164 -0.319172084]] [128 768]\n",
      "attention output 2  [[-1.33758736 0.202717438 -0.823835611 ... -0.920542896 1.02585912 0.470015436]\n",
      " [-1.09547889 -0.975174606 -0.123791575 ... 0.0678673834 0.210363895 -1.0146234]\n",
      " [0.1892609 -1.32129014 -0.281527638 ... 0.298653901 0.587011337 0.201622963]\n",
      " ...\n",
      " [-1.23972535 -0.30626294 0.0717573538 ... -0.110350192 0.0730206221 0.133727342]\n",
      " [-1.11586773 -0.155965596 0.453900933 ... 0.198338717 0.0484326631 -0.236887082]\n",
      " [-1.10602331 -0.467236042 0.599496126 ... 0.516085386 -0.0347497426 -0.366763443]] [[-1.08503759 0.310666829 -0.787685096 ... -0.354312062 0.961511433 0.308173418]\n",
      " [-0.904021144 -0.938276172 -0.226376474 ... -0.0783001781 0.204731971 -0.782903135]\n",
      " [0.113878921 -1.20160544 -0.432717085 ... 0.287103 0.466878831 0.180988446]\n",
      " ...\n",
      " [-1.09340322 -0.563867033 0.0999528319 ... -0.0922525376 0.132861629 0.179888025]\n",
      " [-1.01952207 -0.382054985 0.492108732 ... 0.268101692 0.107485995 -0.186361939]\n",
      " [-0.936152816 -0.72997874 0.64133662 ... 0.601996183 -0.0500781164 -0.319172084]] [-0.10986238 -0.0433228426 0.0260389317 ... -0.0810748562 -0.0468315892 -0.0543763153] [0.851582229 0.813929737 0.836747 ... 0.805637717 0.810805 0.842506409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate output  [[-0.0266864952 -0.168003961 -0.000373105315 ... -0.170005754 -0.16354847 -0.000815874548]\n",
      " [-0.00106928044 -0.00643255562 -0.0109781073 ... -0.0969317406 -0.0194965042 -0.00230544875]\n",
      " [-0.000643233361 -0.123876981 -0.0257592872 ... -0.165454894 -0.131807864 -0.0153195765]\n",
      " ...\n",
      " [-0.0318699963 -0.0202224683 -0.0120739201 ... -0.0319447704 -0.114373252 -0.0856173486]\n",
      " [-0.0341788307 -0.0163605921 -0.0242529754 ... -0.0367028862 -0.102355875 -0.0618763603]\n",
      " [-0.0376860611 -0.0186775792 -0.0194126852 ... -0.0545464456 -0.0904721841 -0.0628261492]]\n",
      "layer output  [[-0.909168899 0.307837218 -0.577829182 ... -0.793938398 0.471214 0.695334613]\n",
      " [-0.950974107 -0.854580879 -0.168390319 ... 0.0339296535 0.0199695788 -0.703799307]\n",
      " [0.0859396607 -1.14730322 -0.0370468311 ... -0.149369046 0.267103612 0.13064301]\n",
      " ...\n",
      " [-1.15583885 -0.493509829 0.161296189 ... -0.0386419222 0.170636088 -0.262758464]\n",
      " [-1.17897987 -0.333679497 0.583467841 ... 0.160872757 0.125401601 -0.614055812]\n",
      " [-1.20231616 -0.644138396 0.660185695 ... 0.5305264 0.0607202798 -0.640982926]]\n",
      "attention output  [[-0.403576553 0.743784726 -1.22216308 ... -1.61270607 0.899205327 0.773983896]\n",
      " [-0.368861973 0.282352269 -0.106779888 ... -0.161259398 0.11355862 0.160380065]\n",
      " [-0.245071977 0.394970566 -0.293604523 ... -0.250365 0.195960179 0.124533445]\n",
      " ...\n",
      " [-0.0432956442 0.06077094 -0.152526543 ... 0.25739342 0.163408056 0.196786121]\n",
      " [-0.163164601 0.0265916567 -0.246125877 ... 0.224833548 0.129737765 0.160338283]\n",
      " [-0.195823386 0.0091103 -0.275903463 ... 0.198313296 0.12610288 0.167496368]] [[-0.909168899 0.307837218 -0.577829182 ... -0.793938398 0.471214 0.695334613]\n",
      " [-0.950974107 -0.854580879 -0.168390319 ... 0.0339296535 0.0199695788 -0.703799307]\n",
      " [0.0859396607 -1.14730322 -0.0370468311 ... -0.149369046 0.267103612 0.13064301]\n",
      " ...\n",
      " [-1.15583885 -0.493509829 0.161296189 ... -0.0386419222 0.170636088 -0.262758464]\n",
      " [-1.17897987 -0.333679497 0.583467841 ... 0.160872757 0.125401601 -0.614055812]\n",
      " [-1.20231616 -0.644138396 0.660185695 ... 0.5305264 0.0607202798 -0.640982926]] [128 768]\n",
      "attention output 2  [[-0.903332114 0.695378304 -1.2524209 ... -1.62148511 0.978019297 1.02203918]\n",
      " [-1.53085899 -0.579204082 -0.329419822 ... -0.150648311 0.244654089 -0.562582731]\n",
      " [-0.182784215 -0.783431292 -0.399777234 ... -0.467191935 0.619451761 0.354537696]\n",
      " ...\n",
      " [-1.12788343 -0.339287698 -0.00281742308 ... 0.193306684 0.397745162 -0.00159146637]\n",
      " [-1.13510656 -0.200373903 0.275888801 ... 0.311303675 0.300452113 -0.3208628]\n",
      " [-1.18695855 -0.464169651 0.316890955 ... 0.598432243 0.243969828 -0.338989973]] [[-0.909168899 0.307837218 -0.577829182 ... -0.793938398 0.471214 0.695334613]\n",
      " [-0.950974107 -0.854580879 -0.168390319 ... 0.0339296535 0.0199695788 -0.703799307]\n",
      " [0.0859396607 -1.14730322 -0.0370468311 ... -0.149369046 0.267103612 0.13064301]\n",
      " ...\n",
      " [-1.15583885 -0.493509829 0.161296189 ... -0.0386419222 0.170636088 -0.262758464]\n",
      " [-1.17897987 -0.333679497 0.583467841 ... 0.160872757 0.125401601 -0.614055812]\n",
      " [-1.20231616 -0.644138396 0.660185695 ... 0.5305264 0.0607202798 -0.640982926]] [-0.0313441083 0.0120795695 -0.0463639647 ... -0.0432734713 0.0537811331 0.0252613351] [0.852037668 0.80201453 0.855423689 ... 0.834553659 0.836532056 0.842377722]\n",
      "intermediate output  [[-0.0795689896 -0.138330773 0.147483602 ... -0.166200489 0.144993499 -0.147042975]\n",
      " [-0.0750453 -0.00228037219 -0.0565806814 ... -0.0981699526 -0.169546112 -0.150553495]\n",
      " [-0.0188311059 -0.00622028438 -0.154811308 ... -0.103777654 -0.100725673 -0.124436751]\n",
      " ...\n",
      " [-0.118830159 -0.0865728 -0.0798788145 ... -0.125342771 -0.0776729062 -0.146355286]\n",
      " [-0.108273499 -0.0836899057 -0.0759969354 ... -0.112912826 -0.131999493 -0.140121952]\n",
      " [-0.110138968 -0.0964140743 -0.0902164 ... -0.105430134 -0.103563271 -0.11267]]\n",
      "layer output  [[-0.627406955 0.192312 -0.753418565 ... -1.06505561 0.581331968 0.570706725]\n",
      " [-0.760546148 -0.32701385 -0.362665653 ... 0.0177343562 0.605883658 -0.248967499]\n",
      " [0.216695547 -0.568261385 -0.303552181 ... -0.104136467 0.415151358 0.0219873134]\n",
      " ...\n",
      " [-0.397654831 -0.393368423 -0.0526916757 ... 0.194091856 0.510280967 -0.253532261]\n",
      " [-0.431208372 -0.273280531 0.0899932384 ... 0.212269798 0.444749713 -0.246915981]\n",
      " [-0.475708067 -0.398230851 0.125879347 ... 0.355003804 0.384571016 -0.249222279]]\n",
      "extracting layer 0\n",
      "extracting layer 1\n",
      "extracting layer 2\n",
      "extracting layer 3\n",
      "extracting layer 4\n",
      "extracting layer 5\n",
      "extracting layer 6\n",
      "extracting layer 7\n",
      "extracting layer 8\n",
      "extracting layer 9\n",
      "extracting layer 10\n",
      "extracting layer 11\n"
     ]
    }
   ],
   "source": [
    "# with tf.variable_scope(\"test\", dtype=tf.float64):\n",
    "tensorflow_all_out = []\n",
    "for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "    unique_id = int(result[\"unique_id\"])\n",
    "    feature = unique_id_to_feature[unique_id]\n",
    "    output_json = collections.OrderedDict()\n",
    "    output_json[\"linex_index\"] = unique_id\n",
    "    tensorflow_all_out_features = []\n",
    "    # for (i, token) in enumerate(feature.tokens):\n",
    "    all_layers = []\n",
    "    for (j, layer_index) in enumerate(layer_indexes):\n",
    "        print(\"extracting layer {}\".format(j))\n",
    "        layer_output = result[\"layer_output_%d\" % j]\n",
    "        layers = collections.OrderedDict()\n",
    "        layers[\"index\"] = layer_index\n",
    "        layers[\"values\"] = layer_output\n",
    "        all_layers.append(layers)\n",
    "    tensorflow_out_features = collections.OrderedDict()\n",
    "    tensorflow_out_features[\"layers\"] = all_layers\n",
    "    tensorflow_all_out_features.append(tensorflow_out_features)\n",
    "\n",
    "    output_json[\"features\"] = tensorflow_all_out_features\n",
    "    tensorflow_all_out.append(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:23.970714Z",
     "start_time": "2018-11-15T14:58:23.931930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "odict_keys(['linex_index', 'features'])\n",
      "number of tokens 1\n",
      "number of layers 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tensorflow_all_out))\n",
    "print(len(tensorflow_all_out[0]))\n",
    "print(tensorflow_all_out[0].keys())\n",
    "print(\"number of tokens\", len(tensorflow_all_out[0]['features']))\n",
    "print(\"number of layers\", len(tensorflow_all_out[0]['features'][0]['layers']))\n",
    "tensorflow_all_out[0]['features'][0]['layers'][0]['values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T14:58:25.547012Z",
     "start_time": "2018-11-15T14:58:25.516076Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorflow_outputs = list(tensorflow_all_out[0]['features'][0]['layers'][t]['values'] for t in layer_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10810544  0.00736203 -0.14134324 ...  0.08043151  0.07175563\n",
      "   0.0031992 ]\n",
      " [-0.00526232  0.6327945  -0.2985075  ...  0.10594425  0.09061253\n",
      "  -0.76824725]\n",
      " [-0.3182612  -0.8120704   0.15033704 ... -0.1900597   0.15686822\n",
      "   0.12246863]\n",
      " ...\n",
      " [ 0.09414048 -0.33054894  0.61384857 ...  0.43929374 -0.3086228\n",
      "   0.06017733]\n",
      " [ 0.01996002 -0.37984183  0.49045902 ...  0.45061845 -0.21570973\n",
      "  -0.05887301]\n",
      " [ 0.15295641 -0.2668718   0.49672574 ...  0.7504021  -0.5253611\n",
      "  -0.10960616]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'double_test/beta:0' shape=(276,) dtype=float32_ref>, <tf.Variable 'double_test/gamma:0' shape=(276,) dtype=float32_ref>]\n",
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_tensor = tf.constant(value=0.5, shape=(128, 276))\n",
    "output_tensor = tf.contrib.layers.layer_norm(\n",
    "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=\"double_test\")\n",
    "assign_ops = []\n",
    "tvars = []\n",
    "for tvar in tf.trainable_variables():\n",
    "    assign_ops.append(tf.assign(tvar, tf.ones_like(tvar)))\n",
    "    tvars.append(tvar)\n",
    "    \n",
    "print(tf.trainable_variables())\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    res = sess.run(tvars)\n",
    "    res = sess.run(output_tensor)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ PyTorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:03:49.528679Z",
     "start_time": "2018-11-15T15:03:49.497697Z"
    }
   },
   "outputs": [],
   "source": [
    "import extract_features\n",
    "import pytorch_pretrained_bert as ppb\n",
    "from extract_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:18.001177Z",
     "start_time": "2018-11-15T15:21:17.970369Z"
    }
   },
   "outputs": [],
   "source": [
    "init_checkpoint_pt = \"/tmp/pretraining_output_test_final_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:20.893669Z",
     "start_time": "2018-11-15T15:21:18.786623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = ppb.BertModel.from_pretrained(init_checkpoint_pt)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:26.963427Z",
     "start_time": "2018-11-15T15:21:26.922494Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.0)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.0)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_input_type_ids = torch.tensor([f.input_type_ids for f in features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids, all_example_index)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:30.718724Z",
     "start_time": "2018-11-15T15:21:30.329205Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958, 27227,  2001,\n",
      "          1037, 13997, 11510,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([0])\n",
      "attention output  tensor([[[-0.0235,  0.1146,  0.0559,  ..., -0.1382,  0.2656, -0.0343],\n",
      "         [ 0.2982, -0.2570, -0.1880,  ..., -0.1719,  0.2763, -0.1657],\n",
      "         [ 0.2218, -0.3631, -0.1334,  ..., -0.4007,  0.1284, -0.1852],\n",
      "         ...,\n",
      "         [ 0.1769, -0.0099, -0.0021,  ..., -0.1489,  0.2530, -0.2076],\n",
      "         [ 0.1803, -0.0069,  0.0159,  ..., -0.1581,  0.2616, -0.2206],\n",
      "         [ 0.1771, -0.0170, -0.0248,  ..., -0.1498,  0.2578, -0.2288]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.1534,  0.3378, -0.1694,  ...,  0.2973,  0.3874, -0.3420],\n",
      "         [-0.4856, -0.5238,  0.2969,  ...,  0.1912, -0.0671,  0.3203],\n",
      "         ...,\n",
      "         [ 0.0265, -0.1798,  0.4894,  ..., -0.5321, -0.2465,  0.4659],\n",
      "         [-0.1195, -0.2324,  0.2428,  ..., -0.4740, -0.1393,  0.3180],\n",
      "         [ 0.1108, -0.0750,  0.3226,  ..., -0.0731, -0.5137,  0.2290]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[ 0.5377, -0.2462, -0.6474,  ..., -0.0392,  0.4213,  0.1321],\n",
      "         [ 0.4668,  0.0768, -0.8364,  ...,  0.3365,  0.8851, -0.9293],\n",
      "         [-0.2195, -1.5199, -0.0432,  ..., -0.2075, -0.0256,  0.0670],\n",
      "         ...,\n",
      "         [ 0.7401, -0.2729,  0.7147,  ..., -0.9510,  0.0222,  0.4619],\n",
      "         [ 0.4799, -0.3565,  0.3003,  ..., -0.8538,  0.2330,  0.1664],\n",
      "         [ 0.8938, -0.0877,  0.3656,  ..., -0.1216, -0.4347, -0.0138]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.1534,  0.3378, -0.1694,  ...,  0.2973,  0.3874, -0.3420],\n",
      "         [-0.4856, -0.5238,  0.2969,  ...,  0.1912, -0.0671,  0.3203],\n",
      "         ...,\n",
      "         [ 0.0265, -0.1798,  0.4894,  ..., -0.5321, -0.2465,  0.4659],\n",
      "         [-0.1195, -0.2324,  0.2428,  ..., -0.4740, -0.1393,  0.3180],\n",
      "         [ 0.1108, -0.0750,  0.3226,  ..., -0.0731, -0.5137,  0.2290]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([ 2.5779e-01, -3.0779e-02, -2.7727e-01, -3.8848e-01,  3.6842e-01,\n",
      "        -4.6472e-01,  5.1851e-01,  1.9574e-01,  8.7187e-03,  3.1131e-03,\n",
      "         5.5911e-02,  2.4082e-01, -5.1561e-01,  6.2018e-01,  1.4133e-01,\n",
      "        -1.6917e-01, -4.1995e-01, -4.1823e-01,  9.1310e-02,  6.0748e-01,\n",
      "         3.0194e-01, -5.0551e-01,  3.5331e-01,  2.8206e-01, -3.8062e-01,\n",
      "         2.6253e-01,  3.4851e-01, -2.2527e-01, -2.3179e-01,  4.9910e-02,\n",
      "        -1.3719e-01, -1.3108e-02,  3.7722e-01, -2.3444e-01,  1.1979e-01,\n",
      "        -2.4452e-01, -1.6823e-01,  1.0540e-01,  2.1502e-01, -4.5368e-01,\n",
      "        -1.2537e-01, -3.5397e-01, -9.3293e-02,  1.4271e-01, -8.3512e-02,\n",
      "        -1.6169e-01, -2.2406e-01, -2.1923e-01,  1.8921e-01,  1.3868e-02,\n",
      "        -4.2275e-01,  1.5757e-01,  5.7117e-01, -1.0727e+00,  2.0317e-01,\n",
      "        -2.6968e-01,  3.5703e-02, -4.5939e-01,  1.2789e-02, -2.7969e-01,\n",
      "        -6.7436e-02,  1.2649e-01, -7.5450e-01, -5.7181e-01, -2.1688e-01,\n",
      "        -7.7041e-02,  8.0409e-02,  2.9004e-02,  6.0335e-02, -1.9877e-01,\n",
      "         1.0162e-01,  3.9425e-02, -5.8807e-01, -2.8840e-01,  1.6118e-01,\n",
      "         3.2292e-01,  1.1399e-01,  1.0226e-01,  8.0797e-02, -9.5462e-02,\n",
      "         4.1979e-02,  1.6093e-02,  1.4859e-01, -3.1781e-02,  2.3659e-01,\n",
      "         1.4564e-01, -2.0450e-01,  5.2473e-02,  9.9538e-02,  1.1062e-01,\n",
      "        -1.4813e-02, -8.4609e-02,  8.3361e-01,  3.1535e-02,  9.8567e-02,\n",
      "         5.8572e-02,  1.7659e-01, -1.9438e-01, -2.3140e-02, -1.0675e-01,\n",
      "        -8.3583e-02,  2.3579e-01, -3.0846e-02, -8.2531e-02,  1.0158e-01,\n",
      "         2.1368e-01, -4.1900e-01, -7.3500e-02,  3.2035e-01, -1.0121e-01,\n",
      "        -8.6561e-02,  3.2273e-01, -7.9946e-01, -3.8830e-01,  8.7751e-02,\n",
      "         9.8826e-02,  1.4594e-01,  1.5980e-01, -8.1347e-02,  9.0685e-02,\n",
      "        -1.9815e-01, -2.1831e+00, -1.9101e-01,  1.1689e-01,  7.2769e-02,\n",
      "        -3.5399e-02, -9.5608e-02,  9.0747e-02,  1.6770e-01, -9.6488e-02,\n",
      "        -5.8986e-02,  3.2005e-01, -1.2898e-01,  1.1754e-01, -2.0203e-01,\n",
      "        -3.2818e-01, -3.7609e-01, -2.4728e-01, -3.7931e-01, -7.4452e-01,\n",
      "        -9.8233e-02,  4.0150e-02, -1.6278e-01,  1.8381e-01,  2.0925e-01,\n",
      "        -2.3191e+00, -1.3307e-01, -2.5176e-01,  9.7895e-02,  2.0272e-01,\n",
      "         1.5451e-01,  2.1300e-01, -1.6771e-01,  7.0515e-02,  6.5330e-02,\n",
      "         5.7360e-02,  3.8225e-01,  3.2131e-01,  2.2921e-01,  4.1994e-01,\n",
      "        -1.2706e-01,  1.6592e-01,  3.4706e-02,  5.5698e-02,  9.0083e-02,\n",
      "         7.5522e-02, -2.5302e-03, -2.9655e-01, -1.7065e+00, -1.3616e-01,\n",
      "        -1.6343e-02,  2.4872e-01,  2.9591e-01,  5.0073e-01,  1.7376e-01,\n",
      "         5.1711e-01, -2.7242e-02,  1.7358e-01,  1.9530e-01,  4.6222e-02,\n",
      "        -1.2768e-01, -1.5709e-01,  3.3921e-01,  4.4341e-01,  3.6406e-01,\n",
      "         3.9709e-02,  4.6085e-01,  5.6007e-01,  5.8145e-02, -2.1166e-02,\n",
      "        -3.1091e-01, -4.2733e-01,  5.9941e-01, -2.4489e-01, -1.9952e-01,\n",
      "         5.7874e-02, -8.5775e-02,  1.7438e-01, -2.3140e-01,  1.2576e-01,\n",
      "        -9.4729e-02,  4.7020e-01,  2.4542e-01,  3.7365e-01,  1.8621e-02,\n",
      "         1.7672e-01,  4.8899e-01, -1.2693e-01, -6.6310e-02, -1.2871e-01,\n",
      "         5.8599e-02, -2.2685e-01,  2.8196e-01,  1.9973e-01,  4.2373e-02,\n",
      "        -4.2278e-01, -2.2370e-01, -9.6376e-02,  7.3086e-01,  1.6561e-01,\n",
      "         1.1794e-01,  4.1491e-01,  2.3683e-01, -3.5037e-01,  1.1536e-01,\n",
      "         1.2638e-01, -9.9769e-02, -1.8797e-01, -1.9788e-02,  9.1197e-02,\n",
      "         2.1268e-01, -7.7420e-02,  2.3824e-01, -9.8953e-02,  1.4712e-01,\n",
      "         2.5991e-01,  2.0597e-01,  1.9612e-01,  8.6431e-02, -2.5320e-01,\n",
      "        -2.6983e-01,  5.0922e-02,  3.9305e-02,  3.1508e-01, -1.1796e-02,\n",
      "         2.4797e-01, -4.7900e-01,  3.9374e-02, -1.5620e-01, -9.9864e-02,\n",
      "         3.1478e-01,  4.6956e-01, -1.0247e-01, -4.3489e-02,  8.4172e-03,\n",
      "        -8.0305e-02, -2.9134e-01, -2.2966e-01,  1.5391e-01,  2.0498e-02,\n",
      "         2.5726e-01, -5.2779e-02,  3.2498e-01,  1.0985e-01,  6.1144e-01,\n",
      "         3.0967e-01, -5.7404e-02,  1.4842e-01,  6.7556e-02,  2.2393e-02,\n",
      "         1.2113e-01, -7.1725e-02, -6.6165e-03, -9.5795e-02, -2.4788e-01,\n",
      "         1.6128e-01, -2.3460e-02, -2.4699e-01, -4.4107e-02, -1.5102e-01,\n",
      "         7.3968e-02,  1.1620e-01,  2.2701e-02,  3.4940e-02, -1.6602e-01,\n",
      "        -3.8707e-01,  1.2303e-02,  1.4317e-01,  1.4177e-01,  7.1162e-02,\n",
      "         3.0388e-04,  4.2850e-01, -2.7111e-01,  1.2200e-02,  4.7172e-02,\n",
      "        -1.0342e-01, -3.6852e-01,  7.2947e-01, -1.3952e-01, -1.9715e-01,\n",
      "        -1.2032e-02, -2.5648e-01, -4.9745e-03,  1.7163e-01,  4.5461e-01,\n",
      "         2.8798e-01,  8.4074e-02, -2.7832e-01, -4.2050e+00, -2.5436e-01,\n",
      "         9.3863e-02,  4.2393e-01, -1.4098e-01, -3.2062e-01,  2.9002e-01,\n",
      "        -2.0093e-01,  7.9890e-02, -1.0732e-01,  8.0788e-02,  1.2386e-01,\n",
      "        -3.8590e-02, -1.7305e-02,  1.0622e-01, -3.2384e-02,  9.4720e-02,\n",
      "         3.3051e-01,  1.3069e-01, -4.0660e-02, -8.2832e-02,  1.4092e-01,\n",
      "        -1.5917e-01, -1.3184e-01, -2.3219e-01,  1.9032e-01, -6.3644e-02,\n",
      "         1.1415e-01,  7.6270e-02,  8.0228e-02, -1.4469e-02,  3.8586e-01,\n",
      "        -1.4446e-02,  2.1612e-01, -2.0589e-01,  1.9103e-01, -3.1480e-01,\n",
      "         4.1715e-01, -4.0220e-02, -3.6441e-01, -3.2468e-01, -4.1474e-01,\n",
      "        -9.7549e-02,  3.0935e-01, -1.1352e-01,  2.9126e-01, -1.6827e-01,\n",
      "        -3.7892e-01, -2.7938e-01, -1.7717e-01, -1.8205e-01, -4.8331e-02,\n",
      "         3.1176e-01, -1.6180e-01, -2.8250e-02,  1.6033e-02, -8.6836e-02,\n",
      "         5.9511e-02, -4.0695e-02, -2.5213e-02, -1.0682e-01,  1.9171e-01,\n",
      "        -3.6004e-01, -5.2505e-03,  1.4575e-01,  1.3395e-01, -3.3098e-01,\n",
      "        -1.1654e-01, -5.1699e-02, -8.5458e-02, -8.0447e-02, -2.6068e-02,\n",
      "         2.7269e-02, -7.5727e-01,  2.5966e-01, -4.4027e-01, -7.8031e-02,\n",
      "        -8.4315e-02, -2.4545e-01,  1.8171e-01,  2.8277e-02, -2.9943e-01,\n",
      "        -1.0525e-01, -5.0601e-02,  3.7778e-01,  2.3072e-01,  9.0232e-02,\n",
      "         9.3683e-03,  6.7106e-03, -3.1979e-03,  5.5597e-02, -1.5697e-01,\n",
      "         1.5346e-01, -1.6227e-01,  1.1222e-02, -1.6931e-01,  1.8144e-01,\n",
      "        -2.7496e-01,  2.7094e-01, -1.6177e-01,  7.7760e-02, -1.7393e-02,\n",
      "        -3.5944e-01, -2.4198e-01,  2.9998e-03,  2.2186e-02, -1.7116e-01,\n",
      "        -8.4470e-03,  1.2028e-01,  1.0575e-01,  1.4780e-01, -7.6587e-02,\n",
      "        -2.4664e-01, -1.6266e-01,  2.2838e-01, -2.5268e-01, -1.0427e-01,\n",
      "         1.3195e-01,  5.5370e-02, -5.3158e-02, -8.2787e-02, -2.8575e-01,\n",
      "         2.4239e-01, -8.1294e-02, -1.1476e-01, -6.2154e-02,  1.4437e-01,\n",
      "         1.7578e-01, -5.2239e-02, -9.5052e-02, -1.2310e-01, -2.4028e-01,\n",
      "         1.3541e-01,  7.9743e-02, -2.3021e-02, -1.0104e+00, -9.5809e-02,\n",
      "         1.5992e-01, -4.5510e-01,  2.8368e-03, -9.2517e-02, -4.2022e-02,\n",
      "        -1.2421e-01,  1.3576e-01,  4.9815e-01, -2.2489e-01,  8.6401e-02,\n",
      "        -3.2181e-02,  4.6024e-02,  1.0783e-01, -1.9738e-01, -2.4705e-01,\n",
      "         1.7003e-01, -1.4377e-01,  6.0836e-02,  3.7177e-01,  2.8434e-01,\n",
      "        -4.9227e-01, -3.5588e-01,  2.7315e-01,  2.5845e-01,  3.6018e-02,\n",
      "         1.9522e-01, -1.7843e-01, -1.6051e-01, -8.7167e-02,  1.0288e-01,\n",
      "         2.5660e-01, -3.0730e-01, -7.0951e-02, -1.3980e-01, -1.1663e-01,\n",
      "        -2.4542e-01,  2.1354e-01,  2.2251e-01,  4.8302e-02, -2.6383e-02,\n",
      "         1.3435e-01, -2.7807e-03,  3.2373e-01, -2.5855e-01, -1.2901e-01,\n",
      "        -1.2035e-01, -3.6396e-01,  3.1163e-01, -2.9464e-01, -5.5448e-01,\n",
      "        -2.0348e-01, -1.0431e+00,  1.1943e-01, -2.1489e-01, -6.0388e-02,\n",
      "        -1.4263e-01,  9.8254e-02,  1.1313e-02,  8.6843e-02,  1.2205e-01,\n",
      "        -2.2595e-01,  2.2344e-01, -1.0910e-02, -2.9116e-01,  2.9904e-02,\n",
      "        -1.0634e-01, -2.5080e-01,  5.3800e-01,  2.7363e-03, -2.8436e-01,\n",
      "         2.2562e-01, -1.5355e-01, -9.6141e-03, -3.5160e-01, -7.4145e-02,\n",
      "        -1.8211e-01,  2.3844e-01,  1.6109e-01,  5.2170e-03, -3.1517e-01,\n",
      "         5.5372e-02,  6.3091e-02,  3.9125e-02,  6.3962e-02,  1.9221e-02,\n",
      "        -4.3275e-02, -1.3064e-01,  1.7295e-01, -1.1225e-01, -8.6447e-02,\n",
      "        -2.3046e-01, -2.7106e-01,  2.3793e-01,  1.9401e-01, -1.3288e-01,\n",
      "         3.3715e-01, -2.0408e+00, -1.2686e-01,  2.4828e-01,  3.8096e-02,\n",
      "        -5.7901e-01, -4.0854e-01, -4.7542e-01,  2.1448e-01,  3.9830e-01,\n",
      "         1.6589e-02, -1.2844e-01,  1.1775e-01,  1.8905e-01,  1.3855e-01,\n",
      "        -5.2613e-02,  1.8299e-01,  2.8172e-01,  6.4889e-02, -1.1898e-01,\n",
      "        -9.3644e-02, -4.2899e-01, -3.1068e-01, -2.3766e-01,  1.3886e-01,\n",
      "         1.0111e-01,  5.4710e-02, -6.8540e-02, -7.1702e-02, -5.3793e-02,\n",
      "         2.6247e-01, -5.1968e-02,  2.7869e-02, -3.1397e-01,  1.5931e-01,\n",
      "         5.0571e-02,  2.3991e-03, -4.2980e-01,  9.4605e-02,  1.8283e-02,\n",
      "        -9.3458e-02, -2.7621e-01,  3.9827e-01,  1.0785e-01,  8.0008e-02,\n",
      "         1.7207e-01,  5.6882e-01, -4.8990e-02, -1.9813e-01, -1.7304e-01,\n",
      "         2.9611e-02, -3.1768e-01,  1.7857e-01, -2.7627e-02,  2.5045e-01,\n",
      "        -3.4368e-02,  1.4131e-01,  6.0893e-02,  1.0880e-01,  9.3515e-02,\n",
      "         3.5295e-02,  2.0938e-01, -2.2219e-01,  1.4256e-02,  4.1997e-02,\n",
      "         3.5206e-01,  2.3682e-01,  3.6627e-01,  7.4852e-02,  4.1111e-02,\n",
      "        -1.5028e-02,  1.4411e-01,  4.1050e-02,  7.1006e-02,  2.7073e-01,\n",
      "         1.0409e-02,  2.8577e-01, -1.1552e-01,  1.6381e-01,  3.3565e-01,\n",
      "         9.1178e-02, -1.6104e-01,  6.8311e-02,  3.8821e-02, -3.7638e-03,\n",
      "        -3.6849e-02,  1.6130e-01, -3.0216e-01,  4.1257e-02, -2.3570e-02,\n",
      "         9.5418e-02,  3.6305e-02, -1.3093e-01,  6.8629e-02,  2.2442e-01,\n",
      "        -6.7986e-02,  1.8033e-01,  8.5662e-02,  8.6606e-02,  3.3871e-01,\n",
      "        -2.5409e-01, -3.1649e-02,  1.5144e-01,  2.0081e-04, -1.5954e-01,\n",
      "        -5.0360e-02,  3.1934e-01, -1.0287e-01,  1.6966e-02,  2.8538e-01,\n",
      "        -1.7526e-01,  2.4329e-02, -1.1036e-01,  1.5191e-01, -2.1836e+00,\n",
      "         7.9322e-02, -2.7441e-01, -1.9576e-01,  3.3840e-02,  1.0629e-01,\n",
      "         2.3421e-02, -2.0076e-01,  8.8187e-02, -1.6784e-03,  9.1988e-02,\n",
      "        -2.9274e-01,  2.8228e-02, -5.1979e-01,  7.9986e-02, -7.7685e-02,\n",
      "        -5.1658e-01,  3.0614e-01, -3.9945e-03, -1.7505e-01, -1.7647e-02,\n",
      "         1.2386e-01,  5.8742e-01,  2.6091e-01,  2.7960e-02, -3.4600e-01,\n",
      "         1.1717e-01, -6.1749e-02,  5.3778e-01, -4.1136e-02, -5.9014e-02,\n",
      "        -2.4886e-01,  1.5767e-01, -3.5779e-01, -2.1397e-01,  1.1481e-01,\n",
      "         7.5155e-02,  4.3344e-01,  5.5837e-02,  3.5340e-02, -3.0472e-01,\n",
      "         3.9934e-01, -1.5702e-01, -1.4878e-01, -1.7719e-02, -1.1608e-01,\n",
      "         2.5856e-01, -1.6643e-03, -6.5223e-01,  1.6385e-01, -2.7505e-01,\n",
      "         1.0384e-01,  3.7984e-01,  2.1218e-01,  2.5017e-02, -1.8059e-01,\n",
      "        -2.2721e-02,  2.7475e-01, -8.2843e-02,  7.3440e-02, -1.4869e-01,\n",
      "         2.8977e-01, -9.6194e-02,  1.4089e-01, -5.1639e-02,  2.0427e-01,\n",
      "        -4.0324e-02, -2.2649e-01,  1.3113e-01,  7.0327e-02,  2.1987e-01,\n",
      "         1.7708e-02,  5.3146e-03,  9.0577e-02, -3.3763e-01,  1.4876e-02,\n",
      "        -1.4211e-01,  2.3519e-01, -1.1914e-01,  1.8204e-01,  3.7865e-01,\n",
      "         1.1441e-01,  1.4693e-01, -1.8860e-01, -3.3452e-01,  1.1414e-01,\n",
      "        -1.2545e-01, -4.5164e-02,  3.5454e-01, -2.0422e-01,  2.1010e-01,\n",
      "         9.7233e-02, -6.0321e-02,  4.0888e-02, -6.9791e-02,  9.1207e-02,\n",
      "         3.4400e-02, -2.6270e-01, -1.3780e-01,  4.6468e-01, -7.5957e-02,\n",
      "         2.7630e-01, -7.1750e-01,  2.3536e-01, -1.9085e-01,  1.4375e-01,\n",
      "         1.9734e-01,  8.4099e-02, -7.5527e-02, -2.6766e-01,  6.8218e-02,\n",
      "         1.6433e-01, -9.0216e-02, -1.2453e-01], requires_grad=True) Parameter containing:\n",
      "tensor([0.9803, 0.9600, 0.9637, 0.9604, 0.9801, 0.9852, 0.9647, 0.9732, 0.9504,\n",
      "        0.9455, 0.9436, 0.9255, 0.9688, 1.0137, 0.9696, 0.9470, 0.9541, 0.9688,\n",
      "        0.9607, 1.0129, 0.9547, 0.9994, 0.9589, 0.9626, 0.9611, 0.9477, 0.9662,\n",
      "        0.9719, 0.9322, 0.9335, 0.9369, 0.9528, 0.9770, 0.9756, 0.9897, 0.9819,\n",
      "        0.9536, 0.9590, 0.9787, 0.9738, 0.9538, 0.9587, 0.9712, 0.9642, 0.9438,\n",
      "        0.9498, 1.0307, 0.9679, 1.0027, 0.9406, 0.9837, 0.9559, 0.9979, 1.1031,\n",
      "        0.9686, 0.9263, 0.9452, 0.9659, 0.9413, 0.9576, 0.9359, 0.9370, 0.9980,\n",
      "        0.9974, 0.9423, 0.9729, 0.9564, 0.9696, 0.9510, 0.9692, 0.9297, 0.9385,\n",
      "        0.9804, 0.9449, 0.9775, 0.9626, 0.9228, 1.0288, 0.9424, 0.9509, 0.9548,\n",
      "        0.9650, 0.9689, 0.9409, 0.9266, 0.9493, 0.9271, 0.9496, 0.9633, 0.9474,\n",
      "        0.9221, 0.9368, 1.0719, 0.9362, 0.9120, 0.9327, 0.9805, 0.9780, 0.9435,\n",
      "        0.9650, 0.9611, 0.9778, 0.9410, 0.9220, 0.9338, 0.9258, 0.9603, 0.9560,\n",
      "        0.9592, 0.9082, 0.9853, 0.9520, 0.9934, 0.9540, 0.9563, 0.9451, 0.9476,\n",
      "        0.9514, 0.9340, 0.9333, 0.9702, 0.9847, 0.9719, 0.9346, 0.9551, 0.9391,\n",
      "        0.9346, 0.9403, 0.9532, 0.9582, 0.9530, 0.9402, 0.9986, 0.9437, 0.9454,\n",
      "        0.9636, 0.9635, 0.9413, 0.9591, 1.0226, 0.9780, 1.0014, 0.9647, 0.9678,\n",
      "        0.9492, 1.6253, 0.9576, 0.9485, 0.9356, 0.9474, 0.9285, 0.9583, 0.9150,\n",
      "        0.9747, 0.9430, 0.9546, 0.9734, 0.9274, 0.9657, 0.7153, 0.9681, 0.9739,\n",
      "        0.9174, 0.9704, 0.9418, 0.9686, 0.9454, 0.9472, 1.0759, 0.9786, 0.9575,\n",
      "        0.9711, 0.9533, 0.9908, 0.9516, 1.1062, 0.9354, 0.9426, 0.9342, 0.9759,\n",
      "        0.9367, 0.9638, 0.9678, 0.9765, 0.9713, 0.9399, 0.9780, 0.9913, 0.9411,\n",
      "        0.9455, 0.9663, 0.9835, 1.0152, 0.9460, 0.9781, 0.9700, 0.9741, 0.9605,\n",
      "        0.9640, 0.9187, 0.9634, 0.9850, 1.0016, 0.9605, 0.9603, 0.9360, 0.9766,\n",
      "        0.9278, 0.9646, 0.9340, 0.9414, 0.9418, 0.9673, 0.9214, 0.9628, 0.9654,\n",
      "        0.7206, 0.9750, 1.0360, 0.9498, 0.9643, 0.9835, 0.9611, 0.9453, 0.9653,\n",
      "        0.8490, 0.9403, 0.9093, 0.9467, 0.9648, 0.9366, 0.9508, 0.9617, 0.9487,\n",
      "        0.9525, 0.9502, 0.9520, 0.9478, 0.9458, 0.9259, 0.9569, 0.9503, 0.9342,\n",
      "        0.9487, 0.9456, 0.9198, 0.9545, 0.9189, 0.9100, 0.8780, 0.9456, 1.0188,\n",
      "        0.9469, 0.9496, 0.9664, 0.9517, 0.9474, 0.9560, 0.9190, 0.9289, 0.9610,\n",
      "        0.9351, 0.9734, 0.9570, 0.9909, 0.9696, 0.9253, 0.9586, 0.9377, 0.9574,\n",
      "        0.9543, 0.9445, 0.9448, 0.9551, 0.9861, 0.9357, 0.9279, 0.9654, 0.9544,\n",
      "        0.9458, 0.9271, 0.9550, 0.9478, 0.9429, 0.9704, 0.9679, 0.9450, 0.9801,\n",
      "        0.9430, 0.9503, 0.9784, 0.9588, 0.9584, 0.9361, 0.9489, 0.9109, 0.8780,\n",
      "        1.0235, 0.9803, 0.9438, 0.9749, 0.9752, 0.9540, 0.9603, 1.0577, 0.9784,\n",
      "        0.9325, 0.9625, 2.9968, 0.9347, 0.9097, 0.9354, 0.9362, 0.9823, 0.9857,\n",
      "        0.9476, 0.9624, 0.9356, 0.9428, 0.9555, 0.9447, 0.9380, 0.9207, 0.9380,\n",
      "        0.9432, 0.9540, 0.9536, 0.9403, 0.9371, 0.9302, 0.9234, 0.8713, 0.9428,\n",
      "        0.9738, 0.9571, 0.9547, 0.9450, 0.9506, 0.9701, 0.9686, 0.9088, 1.0202,\n",
      "        0.9549, 0.9366, 0.9495, 1.0086, 0.9502, 0.9595, 0.9448, 0.9581, 0.9719,\n",
      "        0.9667, 0.9312, 0.9766, 0.9612, 0.9701, 0.9557, 0.9310, 0.9448, 0.9436,\n",
      "        0.9655, 0.9420, 0.9507, 0.9503, 0.9620, 0.9638, 0.9427, 0.9455, 0.9614,\n",
      "        0.9319, 0.9631, 0.9889, 0.9676, 0.9360, 0.9679, 0.9588, 0.9402, 0.9587,\n",
      "        0.9321, 0.9602, 0.9712, 1.1794, 0.9515, 0.9765, 0.9445, 0.9376, 0.9764,\n",
      "        0.9741, 0.9851, 0.9733, 0.9154, 0.9476, 0.9585, 0.9437, 0.9728, 0.9696,\n",
      "        0.9303, 0.9575, 0.9517, 0.9531, 0.9348, 0.9586, 0.9289, 0.9690, 0.9374,\n",
      "        0.9464, 0.9586, 0.9375, 0.9575, 0.9683, 0.9699, 0.9256, 0.9498, 0.9869,\n",
      "        0.9342, 0.9396, 0.9518, 0.9519, 0.9437, 0.9504, 0.9421, 0.9314, 0.9705,\n",
      "        0.9519, 0.9477, 0.9443, 0.9345, 0.9736, 0.9602, 0.9522, 0.9386, 0.9297,\n",
      "        0.9420, 0.9370, 0.9438, 0.9607, 0.9393, 0.9488, 0.9747, 0.9740, 0.9539,\n",
      "        0.9719, 0.9376, 1.0158, 0.9513, 0.9557, 0.9795, 0.9502, 0.9329, 0.9272,\n",
      "        0.9337, 0.9690, 1.0126, 0.9718, 0.9560, 0.9508, 0.9295, 0.9750, 0.9184,\n",
      "        0.9630, 0.9436, 0.9183, 0.9682, 0.9579, 0.9582, 0.9719, 0.9593, 0.9649,\n",
      "        0.9551, 0.9201, 0.9851, 0.9559, 0.9730, 0.9563, 0.9155, 0.9522, 0.9568,\n",
      "        0.9531, 0.9582, 0.9442, 0.9689, 0.9702, 0.8536, 0.9278, 0.9635, 0.9421,\n",
      "        0.9146, 0.9667, 0.9451, 0.9482, 0.9539, 0.9730, 0.9438, 0.9845, 0.9964,\n",
      "        0.9473, 1.0750, 0.9624, 0.9765, 0.9419, 0.9433, 0.9702, 0.9150, 0.9575,\n",
      "        0.9413, 1.0038, 0.9449, 0.9344, 0.9532, 0.9840, 0.9875, 0.9515, 0.9778,\n",
      "        0.9368, 0.9451, 0.9565, 0.9460, 0.8238, 0.9651, 0.9378, 0.9508, 0.9481,\n",
      "        0.9709, 0.9565, 0.9691, 0.9621, 0.9449, 0.9364, 0.9428, 0.7577, 0.9599,\n",
      "        0.9845, 0.9525, 0.9472, 0.9559, 0.9536, 0.9680, 0.9552, 0.9387, 1.0333,\n",
      "        0.9422, 0.8801, 0.9566, 0.9472, 0.9537, 0.8967, 0.9581, 0.9847, 0.9455,\n",
      "        0.9709, 0.9517, 0.9380, 0.9527, 0.9550, 0.9548, 0.9555, 0.9482, 0.9709,\n",
      "        0.9482, 0.9600, 0.9339, 0.9534, 0.9738, 0.9765, 0.9532, 0.9407, 0.9695,\n",
      "        0.9408, 0.9763, 0.9680, 0.9314, 0.9643, 0.9531, 0.9352, 0.9703, 0.9490,\n",
      "        0.9552, 0.9774, 0.9496, 0.9592, 0.9359, 0.9286, 0.9809, 0.9391, 0.9554,\n",
      "        0.9604, 0.9919, 0.9892, 0.9532, 0.9621, 0.9564, 0.9727, 0.9559, 0.9488,\n",
      "        0.9257, 0.9428, 0.9470, 0.9605, 0.9531, 0.9485, 0.9348, 0.9533, 0.9760,\n",
      "        0.9552, 0.9800, 0.9490, 0.9928, 0.9467, 0.9634, 0.9542, 0.9656, 0.9502,\n",
      "        0.9484, 0.9457, 0.8506, 0.9479, 0.9306, 0.9506, 0.9820, 0.9404, 0.9443,\n",
      "        0.9660, 0.9463, 0.9364, 0.9670, 0.9334, 0.9435, 0.9640, 0.9324, 0.9535,\n",
      "        0.9343, 0.9409, 0.9365, 0.9357, 0.9530, 0.9503, 0.9757, 0.9440, 0.9457,\n",
      "        0.9562, 0.9769, 0.9572, 0.9432, 0.9108, 0.9517, 0.9432, 0.9639, 0.9336,\n",
      "        0.9541, 0.9579, 0.9651, 0.9497, 0.9246, 0.9530, 1.0747, 0.9240, 0.9244,\n",
      "        0.9591, 0.9688, 0.9424, 0.9484, 0.9640, 0.9501, 0.9493, 0.9506, 0.9345,\n",
      "        0.9669, 0.9735, 0.9465, 0.9350, 0.9889, 0.9660, 0.9585, 0.9397, 0.9347,\n",
      "        0.9380, 0.9731, 0.9760, 0.9567, 0.9851, 0.9694, 0.9465, 0.9863, 0.9611,\n",
      "        0.9632, 0.9644, 0.9449, 0.9680, 0.9952, 0.9633, 0.9373, 0.9777, 0.9459,\n",
      "        0.9547, 0.9405, 0.9747, 0.9294, 0.9280, 0.9388, 0.7935, 1.0033, 0.9642,\n",
      "        0.9527, 0.9665, 0.9753, 0.9216, 0.9908, 0.9443, 0.9560, 0.9666, 0.9097,\n",
      "        0.9624, 0.9813, 0.9554, 0.9560, 0.9626, 0.9598, 0.9389, 0.9468, 0.9368,\n",
      "        0.9615, 0.9698, 0.9458, 0.9310, 0.9451, 0.9507, 0.9571, 0.9754, 0.9547,\n",
      "        0.9290, 0.9744, 0.9839, 0.9426, 1.0479, 0.9855, 0.9264, 0.9772, 0.9563,\n",
      "        0.9852, 0.9318, 0.9340, 0.9573, 0.9755, 0.9436, 0.9368, 0.9418, 0.9439,\n",
      "        0.9370, 0.9602, 0.9647, 0.9584, 0.9886, 0.9391, 0.9924, 0.9456, 0.9325,\n",
      "        1.0528, 0.9822, 0.9245, 0.9366, 0.9576, 0.9250, 0.9418, 0.9593, 0.9578,\n",
      "        0.9433, 0.9443, 0.9843], requires_grad=True)\n",
      "intermediate output  tensor([[[-5.6372e-03, -4.3266e-02, -1.7487e-03,  ..., -2.4559e-02,\n",
      "          -1.7991e-03, -2.4576e-03],\n",
      "         [-1.1571e-01, -3.0061e-03, -9.1577e-05,  ..., -3.9548e-02,\n",
      "          -5.6824e-02, -6.7819e-02],\n",
      "         [-3.8261e-03, -5.7304e-03, -2.8573e-03,  ..., -2.4414e-02,\n",
      "          -1.9376e-02, -5.2877e-02],\n",
      "         ...,\n",
      "         [-7.7370e-03, -2.8951e-02, -1.6732e-02,  ..., -4.9774e-02,\n",
      "          -1.1369e-03, -1.5236e-02],\n",
      "         [-8.0987e-03, -2.6744e-02, -1.3706e-02,  ..., -5.5106e-02,\n",
      "          -8.0352e-04, -8.6167e-03],\n",
      "         [-1.7466e-02, -3.4342e-02, -1.8703e-02,  ..., -4.2277e-02,\n",
      "          -1.3207e-03, -1.4080e-02]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[ 0.1083,  0.0067, -0.1416,  ...,  0.0805,  0.0719,  0.0030],\n",
      "         [-0.0054,  0.6318, -0.2985,  ...,  0.1057,  0.0907, -0.7689],\n",
      "         [-0.3178, -0.8130,  0.1504,  ..., -0.1899,  0.1570,  0.1224],\n",
      "         ...,\n",
      "         [ 0.0943, -0.3308,  0.6135,  ...,  0.4392, -0.3086,  0.0598],\n",
      "         [ 0.0201, -0.3801,  0.4901,  ...,  0.4505, -0.2157, -0.0593],\n",
      "         [ 0.1532, -0.2672,  0.4964,  ...,  0.7504, -0.5254, -0.1100]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[-3.3886e-02,  1.4558e-03,  1.5792e-01,  ..., -7.9203e-02,\n",
      "          -1.5839e-02, -1.8781e-01],\n",
      "         [ 9.8877e-02,  9.2122e-02,  2.4910e-02,  ..., -6.0705e-01,\n",
      "          -6.2665e-02, -6.1422e-01],\n",
      "         [ 5.6587e-02,  1.7412e-01,  1.2485e-01,  ..., -3.5014e-01,\n",
      "          -5.4153e-04, -6.2947e-01],\n",
      "         ...,\n",
      "         [ 4.3537e-04,  1.7024e-01,  1.3995e-01,  ..., -2.4814e-01,\n",
      "          -7.4664e-02, -3.2432e-01],\n",
      "         [-4.9978e-02,  2.4454e-01,  1.8084e-01,  ..., -1.6076e-01,\n",
      "          -1.3148e-01, -3.3442e-01],\n",
      "         [ 1.8009e-03,  1.8527e-02,  1.3167e-01,  ..., -2.1625e-01,\n",
      "          -8.4636e-02, -3.2922e-01]]], grad_fn=<AddBackward0>) tensor([[[ 0.1083,  0.0067, -0.1416,  ...,  0.0805,  0.0719,  0.0030],\n",
      "         [-0.0054,  0.6318, -0.2985,  ...,  0.1057,  0.0907, -0.7689],\n",
      "         [-0.3178, -0.8130,  0.1504,  ..., -0.1899,  0.1570,  0.1224],\n",
      "         ...,\n",
      "         [ 0.0943, -0.3308,  0.6135,  ...,  0.4392, -0.3086,  0.0598],\n",
      "         [ 0.0201, -0.3801,  0.4901,  ...,  0.4505, -0.2157, -0.0593],\n",
      "         [ 0.1532, -0.2672,  0.4964,  ...,  0.7504, -0.5254, -0.1100]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[ 0.2562,  0.1849, -0.0289,  ...,  0.0549,  0.1404, -0.3141],\n",
      "         [ 0.2180,  0.9978, -0.3744,  ..., -0.5284,  0.0628, -1.6057],\n",
      "         [-0.2399, -0.6564,  0.2747,  ..., -0.6422,  0.2244, -0.6377],\n",
      "         ...,\n",
      "         [ 0.2809, -0.0967,  1.1813,  ...,  0.3734, -0.5945, -0.4151],\n",
      "         [ 0.0665, -0.0523,  1.0356,  ...,  0.5332, -0.5290, -0.6305],\n",
      "         [ 0.3828, -0.2389,  0.9648,  ...,  0.9352, -0.9582, -0.7060]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[ 0.1083,  0.0067, -0.1416,  ...,  0.0805,  0.0719,  0.0030],\n",
      "         [-0.0054,  0.6318, -0.2985,  ...,  0.1057,  0.0907, -0.7689],\n",
      "         [-0.3178, -0.8130,  0.1504,  ..., -0.1899,  0.1570,  0.1224],\n",
      "         ...,\n",
      "         [ 0.0943, -0.3308,  0.6135,  ...,  0.4392, -0.3086,  0.0598],\n",
      "         [ 0.0201, -0.3801,  0.4901,  ...,  0.4505, -0.2157, -0.0593],\n",
      "         [ 0.1532, -0.2672,  0.4964,  ...,  0.7504, -0.5254, -0.1100]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([ 8.5837e-02,  1.4200e-01, -8.5664e-02, -1.8797e-01,  2.1057e-01,\n",
      "        -2.4416e-01,  2.7290e-01,  1.6495e-01,  3.2365e-02,  1.0971e-01,\n",
      "        -2.0775e-02,  2.4005e-01, -1.8267e-01,  3.0327e-01, -5.4403e-03,\n",
      "        -1.4303e-02, -1.1269e-01, -2.9029e-01, -5.4553e-02,  5.1764e-01,\n",
      "         2.0928e-01, -3.6854e-01,  2.0008e-01,  1.0004e-01, -1.7577e-01,\n",
      "         2.2374e-01,  1.1766e-01, -1.7958e-01, -1.9987e-01,  9.8001e-02,\n",
      "        -3.8966e-02,  3.9075e-02,  2.0839e-01, -1.9816e-01,  1.2072e-01,\n",
      "        -7.4523e-02, -1.4145e-01,  6.9318e-02,  1.2679e-01, -3.1750e-01,\n",
      "         2.4117e-02, -1.8195e-01, -1.6364e-02,  1.1730e-01, -1.0515e-01,\n",
      "        -1.9714e-01, -6.0606e-02, -7.0980e-02,  9.4621e-02, -1.6172e-02,\n",
      "        -4.1445e-01,  8.4047e-02,  4.2278e-01, -4.3997e-01,  1.0947e-01,\n",
      "        -3.1991e-01, -7.4993e-02, -3.2303e-01,  4.3473e-02, -2.5488e-01,\n",
      "        -9.3522e-02, -1.4271e-02, -4.9369e-01, -4.2574e-01, -7.1936e-02,\n",
      "         6.1253e-02, -5.8429e-02, -4.6612e-02,  2.2238e-02, -9.4320e-02,\n",
      "        -6.3199e-02,  3.6283e-02, -3.6720e-01, -3.0245e-01,  2.2521e-01,\n",
      "         1.2977e-01,  3.2817e-02,  2.3162e-01,  4.3562e-02,  2.6053e-02,\n",
      "         1.0522e-02,  4.9938e-02,  5.2837e-02,  8.1291e-02,  6.1032e-02,\n",
      "         1.4802e-02, -1.8582e-01,  1.3861e-01,  3.3469e-02,  8.8703e-02,\n",
      "        -3.1411e-02, -8.8899e-02,  5.4395e-01, -1.4193e-01,  8.8956e-02,\n",
      "        -4.8855e-02,  2.6914e-01, -1.9660e-01,  2.1199e-02, -3.1289e-02,\n",
      "        -3.5979e-02,  1.5278e-01, -4.0404e-02, -7.4327e-02,  1.2858e-01,\n",
      "         4.0334e-02, -2.9333e-01,  3.4560e-02,  1.1060e-01,  1.5179e-03,\n",
      "         1.2504e-02,  1.8766e-02, -4.7775e-01, -2.5696e-01, -5.2442e-02,\n",
      "        -2.0316e-02,  9.2689e-02,  1.2319e-01,  2.3511e-02,  1.0320e-01,\n",
      "        -2.2639e-01, -1.2759e+00, -1.5359e-01,  1.0497e-01,  7.1435e-02,\n",
      "        -8.2181e-02, -1.1539e-01,  7.8514e-02,  1.2399e-01,  3.0569e-03,\n",
      "        -1.9764e-02,  1.8016e-01, -7.9376e-02,  4.6436e-02, -5.3625e-02,\n",
      "        -1.6864e-01, -2.2689e-01, -1.7785e-01, -2.5519e-01, -4.1561e-01,\n",
      "        -5.1113e-02, -4.5124e-03,  1.4976e-02,  1.8270e-02,  6.0275e-03,\n",
      "        -9.6517e-01, -7.0321e-02, -1.1285e-01,  7.5535e-02,  8.7690e-02,\n",
      "         1.1125e-01,  8.0380e-02, -1.2796e-01,  8.2928e-02,  4.7012e-02,\n",
      "         7.2278e-02,  2.7569e-01,  2.6086e-01,  1.8015e-01,  4.6072e-02,\n",
      "        -1.0445e-02,  3.1879e-03, -5.1754e-02,  6.5119e-02,  1.4766e-02,\n",
      "         6.7017e-02,  1.3209e-03, -3.0013e-01, -9.7466e-01,  8.9708e-03,\n",
      "        -7.5214e-02,  9.7904e-02,  4.2524e-02,  3.5628e-01,  1.6467e-01,\n",
      "         4.6966e-01, -3.6066e-02,  5.2178e-02,  3.6662e-02,  1.4243e-02,\n",
      "        -5.2950e-02, -5.3561e-02,  7.4746e-02,  2.3693e-01,  1.2329e-01,\n",
      "         9.0140e-02,  2.9616e-01,  2.3798e-01,  1.0830e-01, -8.2987e-02,\n",
      "        -1.9990e-01, -2.6593e-01,  3.6729e-01, -7.5275e-02, -8.3015e-02,\n",
      "         5.6904e-02, -7.4600e-02,  7.8693e-02, -2.8709e-02, -2.1914e-02,\n",
      "        -1.2160e-01,  2.1840e-01,  7.4711e-02,  2.7897e-01,  2.3845e-02,\n",
      "         7.2931e-02,  1.8909e-01, -5.3926e-02,  5.9777e-03, -2.7527e-02,\n",
      "         1.0153e-01, -1.1936e-01,  1.9166e-01,  7.1722e-02, -5.1855e-02,\n",
      "        -1.2120e-01, -4.0607e-01, -5.1932e-02,  1.6313e-01,  2.5084e-02,\n",
      "        -6.7911e-02,  2.6443e-01,  1.9162e-01, -1.9408e-01,  1.0773e-01,\n",
      "         1.5768e-02, -2.0234e-02, -9.5764e-03, -3.8908e-02, -1.1116e-02,\n",
      "         1.7622e-01, -4.0152e-02,  2.2360e-01, -4.6216e-02,  5.9286e-02,\n",
      "         1.3976e-01,  4.2180e-02,  2.6758e-02, -4.7266e-02, -5.4953e-02,\n",
      "        -1.7332e-01,  4.7147e-02,  4.2273e-02,  7.3690e-02,  8.7856e-02,\n",
      "        -1.6135e-01, -2.0652e-01, -2.0493e-02, -2.9228e-02, -3.5709e-01,\n",
      "         1.2682e-01,  2.7217e-01, -6.2599e-02, -8.6119e-02, -6.3243e-02,\n",
      "        -1.5008e-01, -1.6937e-01, -2.0133e-01, -1.2466e-03,  5.3692e-03,\n",
      "         1.6784e-01, -8.3860e-02,  2.1513e-01,  8.9194e-02,  2.9874e-01,\n",
      "         1.2635e-01, -3.6667e-02,  6.4175e-02,  1.0769e-01,  1.3463e-02,\n",
      "         1.0019e-01,  9.3421e-03, -3.8965e-02, -1.0163e-01, -2.0176e-01,\n",
      "        -3.5802e-02, -3.6974e-02, -2.0047e-01, -1.2517e-01, -5.9417e-02,\n",
      "         6.5727e-02,  1.0573e-01, -1.0305e-02, -1.2041e-01, -1.3971e-01,\n",
      "        -8.1471e-02,  1.0920e-01,  2.2179e-01,  2.4759e-01,  3.9482e-02,\n",
      "         5.2953e-02,  1.4425e-02, -1.4783e-01, -1.7511e-02,  1.3223e-01,\n",
      "        -1.1093e-02, -3.2532e-01,  4.4576e-01, -6.7037e-02, -1.2040e-01,\n",
      "         7.4323e-02, -1.9547e-01, -5.8757e-02,  1.2348e-01,  2.6547e-01,\n",
      "         2.1292e-01, -1.3931e-01, -1.5638e-01, -2.4629e+00, -1.2759e-01,\n",
      "         1.2892e-02,  2.4209e-01, -7.5945e-02, -1.1335e-01,  1.8653e-02,\n",
      "        -4.0490e-02,  7.8240e-02,  5.0317e-02, -6.1830e-02,  2.7452e-01,\n",
      "        -5.3724e-02, -6.3063e-02,  3.9417e-03, -7.3492e-02,  2.0477e-02,\n",
      "         1.3990e-01,  2.9270e-02, -1.8136e-02, -6.5834e-02,  3.6823e-02,\n",
      "        -2.2458e-01, -3.9937e-01, -8.8788e-02,  1.8374e-01, -5.4395e-02,\n",
      "        -4.3962e-02, -6.8120e-03, -3.0401e-02,  1.0252e-01,  2.8407e-01,\n",
      "        -1.1808e-01,  1.8507e-01, -7.1884e-02,  6.2409e-02, -2.6125e-01,\n",
      "         3.2840e-01,  4.9374e-02, -2.1945e-01, -6.6897e-02, -3.8845e-01,\n",
      "        -8.4390e-02,  2.7234e-01, -6.6010e-02, -7.7096e-02, -5.8041e-02,\n",
      "        -2.0223e-01, -1.2828e-01, -7.9008e-02, -1.5027e-01, -1.3035e-02,\n",
      "         1.4925e-01, -9.5348e-02, -9.9232e-02,  5.0893e-02, -7.3293e-02,\n",
      "        -8.9028e-02, -7.7623e-02, -1.7390e-02,  1.8698e-03,  5.4975e-02,\n",
      "        -2.3773e-01,  4.7328e-02,  1.0715e-01,  8.9251e-02, -2.6932e-01,\n",
      "        -2.0014e-02, -1.0062e-02, -8.6682e-02, -4.0788e-02, -1.3710e-02,\n",
      "        -5.7753e-02, -3.9650e-01,  1.5819e-01, -3.1740e-01,  2.7811e-02,\n",
      "        -3.9947e-02, -2.6995e-01,  1.3130e-01,  1.8187e-02, -1.4568e-01,\n",
      "        -1.3657e-01, -6.2021e-02,  2.0700e-01, -2.8883e-02,  8.0085e-02,\n",
      "         5.6116e-02, -1.1029e-01, -1.3365e-01,  1.6706e-02,  2.6342e-02,\n",
      "         1.5214e-02,  1.5118e-02, -7.6433e-02, -1.7251e-01,  9.2939e-02,\n",
      "        -2.0556e-01,  1.2255e-01, -2.6475e-02,  2.0642e-02, -5.4825e-02,\n",
      "        -2.3549e-01, -9.6248e-02,  8.8796e-02,  1.3571e-01, -5.1882e-02,\n",
      "        -1.1737e-01,  5.9750e-02,  1.2839e-01,  5.5852e-02,  2.9574e-02,\n",
      "        -9.7698e-02, -2.4356e-01,  1.6191e-01, -6.4947e-02, -1.0726e-01,\n",
      "         2.6032e-02,  8.9355e-03, -2.0000e-02, -8.4955e-02, -1.4515e-04,\n",
      "         1.3922e-01,  8.5789e-03, -7.3263e-02, -6.0833e-02,  6.2368e-02,\n",
      "         2.0408e-01, -2.1904e-01, -7.8874e-02, -1.2200e-01, -2.4105e-01,\n",
      "         1.2296e-01, -2.0439e-02, -9.1131e-02, -4.3783e-01,  5.1286e-02,\n",
      "         7.4276e-02, -2.4120e-01,  1.6334e-02,  4.7109e-02, -1.6143e-01,\n",
      "        -1.3797e-01,  8.6905e-02,  2.4781e-01, -2.2786e-01,  1.2199e-01,\n",
      "         1.8875e-02, -1.9502e-03,  6.8838e-02, -9.5184e-02, -1.3612e-01,\n",
      "         1.3390e-01, -6.2416e-02,  1.3856e-02,  2.5924e-01,  1.5965e-01,\n",
      "        -2.6366e-01, -3.0607e-01,  1.0746e-01,  2.4046e-02,  8.5776e-02,\n",
      "         5.1739e-02, -6.6908e-02, -6.9206e-02, -2.0922e-02,  3.8374e-03,\n",
      "         5.9248e-02, -1.9416e-01, -3.8006e-02, -1.5806e-01, -1.1823e-01,\n",
      "        -9.7198e-02,  1.1794e-01,  1.8018e-03, -5.3643e-02,  1.3244e-02,\n",
      "         1.0492e-01,  1.9306e-02,  2.8919e-01, -2.6281e-01, -4.7742e-02,\n",
      "         2.0474e-02, -1.3288e-01,  1.4810e-01, -8.4401e-02, -2.7600e-01,\n",
      "        -2.3292e-01, -7.0531e-01, -1.2917e-02, -7.5083e-02,  2.8812e-02,\n",
      "        -6.4849e-02,  6.9450e-02,  3.1924e-02,  5.8179e-02,  1.5103e-01,\n",
      "        -1.4970e-01,  9.6338e-02, -7.9897e-02, -2.4238e-01,  8.4571e-02,\n",
      "         4.6926e-02, -1.1364e-01,  2.2739e-01,  7.4617e-02, -2.4072e-01,\n",
      "         1.9932e-01, -2.0840e-01, -1.0476e-01, -2.7133e-01, -3.1582e-02,\n",
      "        -2.7995e-03,  1.0618e-01,  6.9327e-02, -9.4563e-02, -2.5350e-01,\n",
      "        -1.1224e-01,  3.9563e-02,  1.4343e-02,  1.0703e-02, -7.9517e-02,\n",
      "        -7.7046e-02,  1.8960e-04,  1.1763e-01, -1.5931e-01, -2.5760e-02,\n",
      "        -1.8895e-01, -1.6665e-01,  1.1495e-01,  9.3534e-02, -3.2218e-01,\n",
      "         1.5994e-01, -8.0288e-01, -1.7235e-01,  1.9614e-01,  2.6550e-02,\n",
      "        -6.7360e-01, -2.2312e-01, -2.4769e-01,  2.3946e-01,  1.5088e-01,\n",
      "        -4.2992e-02, -1.4882e-01,  4.0992e-02,  1.2863e-01,  1.0113e-01,\n",
      "        -3.9589e-02, -4.1158e-02,  2.4886e-01,  1.3087e-01, -1.3462e-01,\n",
      "        -1.0839e-01, -3.0102e-01, -1.8166e-01, -7.0880e-02, -3.1970e-02,\n",
      "        -3.4201e-02,  7.5160e-04, -7.3670e-02, -4.2427e-02, -5.8652e-02,\n",
      "         2.3111e-01,  7.8396e-02, -2.7461e-02, -9.0169e-02, -3.7658e-02,\n",
      "        -2.5310e-02, -1.7875e-02, -2.6136e-01,  2.4332e-02, -7.5451e-03,\n",
      "        -1.2623e-01, -1.7213e-01,  2.2759e-01, -6.5000e-02,  2.7191e-02,\n",
      "         1.8052e-02,  4.1366e-01,  5.0379e-02, -4.5965e-02, -6.5131e-02,\n",
      "         7.9861e-02, -3.3580e-02,  2.6935e-02, -6.4742e-02,  8.9402e-02,\n",
      "        -4.2739e-02,  8.9612e-02, -4.6635e-02,  1.2795e-01, -2.4737e-02,\n",
      "         1.3799e-01,  1.4913e-01, -1.5174e-01,  4.9751e-02, -1.5048e-03,\n",
      "         2.9210e-01,  3.2564e-02,  3.1519e-01,  2.4737e-02, -5.9259e-02,\n",
      "        -3.3368e-02,  5.9252e-02, -4.9013e-02, -4.9689e-02, -1.2974e-01,\n",
      "         1.6995e-02,  6.2272e-02, -7.2129e-02,  1.1067e-01,  1.7617e-01,\n",
      "         1.3362e-01, -1.1290e-01, -2.6263e-02,  5.8156e-03, -7.3941e-02,\n",
      "         8.7596e-02,  2.8825e-01, -2.1539e-01, -3.9005e-02,  7.2810e-02,\n",
      "         4.4566e-02,  6.8019e-02, -2.0573e-01,  1.6230e-01,  8.7474e-02,\n",
      "        -7.4973e-02,  4.3945e-02,  1.0579e-01, -6.0179e-02,  3.4313e-01,\n",
      "        -2.1877e-01,  4.3219e-02,  1.3212e-01, -1.1729e-01, -5.7323e-02,\n",
      "         5.7660e-02,  2.0610e-01, -1.0786e-01, -3.9089e-02,  1.5406e-01,\n",
      "         2.6961e-02,  4.3425e-02, -5.1698e-02,  9.1930e-02, -8.1683e-01,\n",
      "        -7.3213e-02, -2.1459e-01, -1.0263e-01, -1.3105e-02,  8.6215e-02,\n",
      "         1.1604e-01, -1.4322e-01,  9.3040e-02, -6.3341e-02, -2.1220e-03,\n",
      "        -1.0719e-01, -4.3711e-02, -1.7138e-01,  1.4525e-02,  1.8926e-02,\n",
      "        -2.4502e-01,  1.3946e-01, -1.1428e-02, -1.3872e-01,  1.7052e-02,\n",
      "        -1.0297e-01,  4.6580e-01,  1.9646e-01,  1.4460e-01, -1.0840e-01,\n",
      "        -3.1653e-03, -3.9201e-02,  2.8856e-01, -8.0229e-02, -1.1469e-02,\n",
      "        -1.2297e-01,  1.0523e-01, -1.8942e-01, -1.3458e-01,  6.0276e-02,\n",
      "         1.3952e-01,  1.2275e-01,  5.8178e-02,  1.1007e-01, -1.0605e-01,\n",
      "         3.0518e-01, -1.8426e-01, -9.7014e-02, -5.8405e-02, -1.8248e-01,\n",
      "         1.1566e-01, -1.0691e-01, -3.0140e-01,  1.6488e-01, -8.2017e-02,\n",
      "         7.4420e-03,  1.9995e-01,  2.2853e-02,  1.2944e-01,  2.2457e-02,\n",
      "        -2.5948e-02,  2.4372e-01, -1.1012e-01,  7.1336e-02, -2.0806e-01,\n",
      "         1.8529e-01, -1.0258e-01,  5.1174e-02,  2.1166e-02, -7.3214e-02,\n",
      "        -1.6878e-01, -1.2947e-01,  2.8269e-02,  1.8373e-02,  1.7840e-01,\n",
      "         1.1727e-02, -5.0973e-02, -5.2247e-02, -1.7691e-01,  9.7303e-02,\n",
      "        -1.0208e-01,  1.9775e-01,  4.2992e-02,  1.1434e-01,  2.2850e-01,\n",
      "         1.2486e-02,  1.0060e-01, -7.9221e-03, -2.1274e-01,  1.2765e-02,\n",
      "         3.5673e-02, -2.1904e-02,  1.7778e-01, -3.4326e-02,  9.2271e-02,\n",
      "         1.4573e-01, -9.1294e-02, -7.1086e-02,  5.3185e-02, -3.3967e-02,\n",
      "         1.0409e-01, -2.6777e-01, -1.9378e-01,  3.0135e-01, -1.1028e-01,\n",
      "         6.1252e-02, -4.5051e-01,  1.1792e-01,  5.3902e-02,  1.8740e-02,\n",
      "         1.1801e-02,  1.5212e-01, -3.0846e-02, -1.9900e-01, -6.8238e-02,\n",
      "         2.4995e-02,  1.0393e-02,  8.0497e-03], requires_grad=True) Parameter containing:\n",
      "tensor([0.8970, 0.8715, 0.8531, 0.8691, 0.9489, 0.9267, 0.8984, 0.8933, 0.8883,\n",
      "        0.8677, 0.8263, 0.8614, 0.8637, 0.9061, 0.8787, 0.8674, 0.8052, 0.8844,\n",
      "        0.9047, 0.9702, 0.8813, 0.8903, 0.9029, 0.8783, 0.8594, 0.9128, 0.8560,\n",
      "        0.8883, 0.8354, 0.8712, 0.8992, 0.8560, 0.9149, 0.8976, 0.8836, 0.8841,\n",
      "        0.8836, 0.8792, 0.8864, 0.9250, 0.8919, 0.9026, 0.8662, 0.8721, 0.8684,\n",
      "        0.8780, 0.9980, 0.8710, 0.9154, 0.8691, 0.9277, 0.8868, 0.9704, 0.8326,\n",
      "        0.8852, 0.8300, 0.8682, 0.8911, 0.8602, 0.8663, 0.8834, 0.8344, 0.8183,\n",
      "        0.9349, 0.8850, 0.8832, 0.8781, 0.8855, 0.8800, 0.8915, 0.8772, 0.9046,\n",
      "        0.9115, 0.8452, 0.9072, 0.8338, 0.8760, 0.8025, 0.8471, 0.8479, 0.8702,\n",
      "        0.8887, 0.9015, 0.8770, 0.8152, 0.8274, 0.8686, 0.8856, 0.8734, 0.8827,\n",
      "        0.8624, 0.8667, 1.0798, 0.8574, 0.8574, 0.8536, 0.8143, 0.8664, 0.8699,\n",
      "        0.8848, 0.9044, 0.8958, 0.8867, 0.8403, 0.8388, 0.7280, 0.8776, 0.8650,\n",
      "        0.8917, 0.8439, 0.8790, 0.8855, 0.9293, 0.8918, 0.8825, 0.8737, 0.8733,\n",
      "        0.8809, 0.8871, 0.8516, 0.9031, 0.9010, 0.8558, 0.8578, 0.8008, 0.8265,\n",
      "        0.8581, 0.7883, 0.8722, 0.8663, 0.8950, 0.8674, 0.7346, 0.8841, 0.8589,\n",
      "        0.8844, 0.8954, 0.8326, 0.8781, 0.8415, 0.8895, 0.8870, 0.8940, 0.9083,\n",
      "        0.8438, 1.0917, 0.8724, 0.8556, 0.8646, 0.8731, 0.8633, 0.8533, 0.8224,\n",
      "        0.8903, 0.8817, 0.8705, 0.9086, 0.8869, 0.9040, 0.7555, 0.8650, 0.8815,\n",
      "        0.8697, 0.8946, 0.8637, 0.8925, 0.8932, 0.8681, 0.8810, 0.8722, 0.8853,\n",
      "        0.8746, 0.8744, 0.9230, 0.9150, 1.0604, 0.8506, 0.8567, 0.8501, 0.8644,\n",
      "        0.8699, 0.8800, 0.8713, 0.9208, 0.9096, 0.8693, 0.9408, 0.9105, 0.8870,\n",
      "        0.8740, 0.9103, 0.9760, 0.9725, 0.8807, 0.8689, 0.8843, 0.9001, 0.8747,\n",
      "        0.8677, 0.8548, 0.8812, 0.9189, 0.7870, 0.8951, 0.8897, 0.8922, 0.8975,\n",
      "        0.8512, 0.8854, 0.8676, 0.8472, 0.8743, 0.8752, 0.8547, 0.8618, 0.7412,\n",
      "        0.7049, 0.9082, 0.8370, 0.8679, 0.8771, 0.9037, 0.8964, 0.8493, 0.8726,\n",
      "        0.7549, 0.8460, 0.7868, 0.8851, 0.8790, 0.8757, 0.9026, 0.8796, 0.8899,\n",
      "        0.8756, 0.8594, 0.8724, 0.8373, 0.8850, 0.8376, 0.8972, 0.8916, 0.8598,\n",
      "        0.8466, 0.8399, 0.7069, 0.8732, 0.8460, 0.8319, 0.8357, 0.8553, 0.9937,\n",
      "        0.8600, 0.8696, 0.8802, 0.8661, 0.8677, 0.8712, 0.8471, 0.8556, 0.9003,\n",
      "        0.8798, 0.9051, 0.8862, 0.8980, 0.8788, 0.8521, 0.8742, 0.8743, 0.8927,\n",
      "        0.8652, 0.8636, 0.8504, 0.8751, 0.9021, 0.8731, 0.8291, 0.8673, 0.8596,\n",
      "        0.8745, 0.8466, 0.9025, 0.8853, 0.7419, 0.8903, 0.8470, 0.8711, 0.9098,\n",
      "        0.8989, 0.8839, 0.8487, 0.7519, 0.8908, 0.8716, 0.8976, 0.8090, 0.7788,\n",
      "        0.9569, 0.8780, 0.9095, 0.9258, 0.8892, 0.8728, 0.8738, 0.9931, 0.9117,\n",
      "        0.8326, 0.8883, 3.0383, 0.8955, 0.8431, 0.6579, 0.8460, 0.8749, 0.8827,\n",
      "        0.8743, 0.8851, 0.8903, 0.8727, 0.8160, 0.8880, 0.8582, 0.8496, 0.8668,\n",
      "        0.8800, 0.8741, 0.8716, 0.8636, 0.8862, 0.8570, 0.8453, 0.7637, 0.8765,\n",
      "        0.9092, 0.9154, 0.8479, 0.8727, 0.8910, 0.8802, 0.8944, 0.8330, 0.9207,\n",
      "        0.8388, 0.8743, 0.8705, 0.7527, 0.8793, 0.8850, 0.8554, 0.8971, 0.8970,\n",
      "        0.9150, 0.8914, 0.7071, 0.8978, 0.8769, 0.8769, 0.8398, 0.8615, 0.8593,\n",
      "        0.9049, 0.8634, 0.8588, 0.8543, 0.8863, 0.8961, 0.8441, 0.8862, 0.8822,\n",
      "        0.8743, 0.8889, 0.8737, 0.9018, 0.8499, 0.8936, 0.8817, 0.8753, 0.8688,\n",
      "        0.8786, 0.9033, 0.8709, 1.0359, 0.8741, 0.9296, 0.8707, 0.8652, 0.9132,\n",
      "        0.9030, 0.8820, 0.9276, 0.8201, 0.8538, 0.8951, 0.8453, 0.9019, 0.8856,\n",
      "        0.8835, 0.8494, 0.8535, 0.8944, 0.8570, 0.8299, 0.8463, 0.8819, 0.8302,\n",
      "        0.8678, 0.8964, 0.8650, 0.8805, 0.8848, 0.8606, 0.8522, 0.8752, 0.8679,\n",
      "        0.8756, 0.8565, 0.8585, 0.8828, 0.8826, 0.7203, 0.8612, 0.8389, 0.8992,\n",
      "        0.8400, 0.8621, 0.8670, 0.8661, 0.8870, 0.8783, 0.8287, 0.8670, 0.8497,\n",
      "        0.8781, 0.8675, 0.8957, 0.9021, 0.8669, 0.8564, 0.8880, 0.9112, 0.8925,\n",
      "        0.8916, 0.8483, 0.7495, 0.8967, 0.8582, 0.9454, 0.8668, 0.8769, 0.8362,\n",
      "        0.8806, 0.8889, 0.9496, 0.8819, 0.9039, 0.8805, 0.8657, 0.8811, 0.8257,\n",
      "        0.8968, 0.8804, 0.7095, 0.8947, 0.9067, 0.8785, 0.9086, 0.8621, 0.8231,\n",
      "        0.8428, 0.8365, 0.9436, 0.8376, 0.8986, 0.8789, 0.8325, 0.8593, 0.8737,\n",
      "        0.8727, 0.8755, 0.8641, 0.8786, 0.9110, 0.7589, 0.8682, 0.8548, 0.8932,\n",
      "        0.8434, 0.8995, 0.8655, 0.8770, 0.8926, 0.8747, 0.9220, 0.8834, 0.9099,\n",
      "        0.8808, 0.8229, 0.8844, 0.8720, 0.8257, 0.8417, 0.8930, 0.8400, 0.8874,\n",
      "        0.8408, 0.9101, 0.8831, 0.8718, 0.8825, 0.8943, 0.8169, 0.8273, 0.9048,\n",
      "        0.8722, 0.8852, 0.8846, 0.8854, 0.7490, 0.9094, 0.8573, 0.8881, 0.8623,\n",
      "        0.9092, 0.8659, 0.9031, 0.8662, 0.8588, 0.8770, 0.8766, 0.6566, 0.8561,\n",
      "        0.8803, 0.8798, 0.8539, 0.9044, 0.8693, 0.9020, 0.8640, 0.8431, 0.9360,\n",
      "        0.8497, 0.7791, 0.8855, 0.8697, 0.8750, 0.8769, 0.8846, 0.8830, 0.8790,\n",
      "        0.8685, 0.8878, 0.8708, 0.8790, 0.9061, 0.8992, 0.8510, 0.8605, 0.8761,\n",
      "        0.8895, 0.8947, 0.8645, 0.8753, 0.8856, 0.8985, 0.8651, 0.8690, 0.8614,\n",
      "        0.8931, 0.8423, 0.8759, 0.8807, 0.8827, 0.8694, 0.8215, 0.9007, 0.8775,\n",
      "        0.8741, 0.7361, 0.8882, 0.8782, 0.8545, 0.8704, 0.8952, 0.8668, 0.8672,\n",
      "        0.8667, 0.9665, 0.9072, 0.8762, 0.8644, 0.8921, 0.8450, 0.8907, 0.8804,\n",
      "        0.8739, 0.8608, 0.8899, 0.8759, 0.8757, 0.8805, 0.8779, 0.8972, 0.8945,\n",
      "        0.8619, 0.8765, 0.8890, 0.9112, 0.8811, 0.8865, 0.8809, 0.8834, 0.8573,\n",
      "        0.8790, 0.8587, 0.6600, 0.8658, 0.8462, 0.8842, 0.8908, 0.8820, 0.8576,\n",
      "        0.8618, 0.8738, 0.8774, 0.8892, 0.8675, 0.9095, 0.8516, 0.8734, 0.8959,\n",
      "        0.8197, 0.8367, 0.8976, 0.8817, 0.9009, 0.8700, 0.8846, 0.8726, 0.8486,\n",
      "        0.9044, 0.8818, 0.8559, 0.8726, 0.8339, 0.8766, 0.8422, 0.8851, 0.8682,\n",
      "        0.8279, 0.8971, 0.8927, 0.8435, 0.8766, 0.8792, 0.8303, 0.8394, 0.8484,\n",
      "        0.8802, 0.8847, 0.8288, 0.8859, 0.8986, 0.8915, 0.9146, 0.8777, 0.8582,\n",
      "        0.8972, 0.8678, 0.8685, 0.8881, 0.7163, 0.8882, 0.8548, 0.8665, 0.8853,\n",
      "        0.8779, 0.9138, 0.9125, 0.8994, 0.8849, 0.8916, 0.9030, 0.8858, 0.8693,\n",
      "        0.8935, 0.9037, 0.8489, 0.8992, 0.9089, 0.8992, 0.8993, 0.8799, 0.8711,\n",
      "        0.8864, 0.8963, 0.9132, 0.8829, 0.8416, 0.8520, 0.7368, 0.9049, 0.9152,\n",
      "        0.9001, 0.8779, 0.9102, 0.8462, 0.8698, 0.8841, 0.8854, 0.8867, 0.8229,\n",
      "        0.8988, 0.8276, 0.8651, 0.8802, 0.9173, 0.8561, 0.8637, 0.8870, 0.8718,\n",
      "        0.8797, 0.8875, 0.8833, 0.8404, 0.8861, 0.8885, 0.8804, 0.8488, 0.8835,\n",
      "        0.8829, 0.9060, 0.9203, 0.8629, 0.9053, 0.9156, 0.8822, 0.8900, 0.8860,\n",
      "        0.9076, 0.8396, 0.8681, 0.8855, 0.9169, 0.8775, 0.8476, 0.8902, 0.8488,\n",
      "        0.8590, 0.8831, 0.8787, 0.8912, 0.9026, 0.8696, 0.9109, 0.8676, 0.8377,\n",
      "        0.9469, 0.8678, 0.8483, 0.8307, 0.8614, 0.8810, 0.8502, 0.8771, 0.8735,\n",
      "        0.8648, 0.8613, 0.8930], requires_grad=True)\n",
      "intermediate output  tensor([[[-9.2289e-02, -4.5042e-03, -1.5018e-02,  ..., -7.6451e-02,\n",
      "          -1.8238e-02, -8.9176e-02],\n",
      "         [-1.6929e-01, -9.3569e-02, -1.5952e-01,  ..., -1.0361e-01,\n",
      "          -1.8135e-02, -1.8341e-05],\n",
      "         [-1.5382e-01, -8.3270e-03, -4.8777e-02,  ..., -8.8838e-02,\n",
      "          -1.0873e-03, -1.8812e-03],\n",
      "         ...,\n",
      "         [-7.0264e-02, -7.0462e-03, -1.4986e-02,  ..., -5.4153e-02,\n",
      "          -4.2724e-02, -7.9344e-02],\n",
      "         [-4.7167e-02, -1.0158e-02, -2.4708e-02,  ..., -5.7617e-02,\n",
      "          -4.2484e-02, -6.2431e-02],\n",
      "         [-4.4984e-02, -8.2258e-03, -2.0728e-02,  ..., -4.9192e-02,\n",
      "          -3.7470e-02, -7.6382e-02]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.1186, -0.2028, -0.2890,  ...,  0.1480,  0.0899,  0.0336],\n",
      "         [-0.4303,  0.5465, -0.2393,  ...,  0.4525, -0.1273, -1.3178],\n",
      "         [-0.3229, -0.2393,  0.1110,  ..., -0.2591,  0.2445, -0.5182],\n",
      "         ...,\n",
      "         [-0.1378, -0.3733,  0.6494,  ...,  0.8348, -0.5075, -0.0791],\n",
      "         [-0.2751, -0.2491,  0.5805,  ...,  0.8746, -0.4287, -0.1683],\n",
      "         [-0.0591, -0.3636,  0.5165,  ...,  1.0593, -0.6979, -0.2739]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[-1.0142e-02, -3.0156e-02,  2.3483e-01,  ...,  8.0889e-02,\n",
      "           6.9455e-02, -1.7797e-01],\n",
      "         [-1.1267e-01,  8.6194e-02,  2.5739e-01,  ..., -4.6620e-01,\n",
      "           1.0663e-01, -2.4106e-02],\n",
      "         [-1.0439e-01,  2.6400e-01,  2.2452e-01,  ...,  1.3620e-01,\n",
      "           1.7776e-01, -2.8354e-01],\n",
      "         ...,\n",
      "         [-4.3044e-02,  2.2619e-02,  2.1553e-01,  ...,  1.0701e-01,\n",
      "           4.5925e-02, -2.2534e-01],\n",
      "         [-6.7369e-03, -4.3911e-02,  2.2778e-01,  ...,  8.8915e-02,\n",
      "           9.5189e-02, -2.3421e-01],\n",
      "         [ 4.6084e-04, -5.6597e-02,  2.1077e-01,  ...,  8.9027e-02,\n",
      "           8.3726e-02, -2.1622e-01]]], grad_fn=<AddBackward0>) tensor([[[-0.1186, -0.2028, -0.2890,  ...,  0.1480,  0.0899,  0.0336],\n",
      "         [-0.4303,  0.5465, -0.2393,  ...,  0.4525, -0.1273, -1.3178],\n",
      "         [-0.3229, -0.2393,  0.1110,  ..., -0.2591,  0.2445, -0.5182],\n",
      "         ...,\n",
      "         [-0.1378, -0.3733,  0.6494,  ...,  0.8348, -0.5075, -0.0791],\n",
      "         [-0.2751, -0.2491,  0.5805,  ...,  0.8746, -0.4287, -0.1683],\n",
      "         [-0.0591, -0.3636,  0.5165,  ...,  1.0593, -0.6979, -0.2739]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.0596, -0.2734, -0.2617,  ...,  0.4193,  0.1893, -0.2464],\n",
      "         [-0.4224,  0.8319, -0.1528,  ...,  0.0032, -0.1089, -1.4571],\n",
      "         [-0.3769,  0.1699,  0.2289,  ..., -0.1355,  0.3966, -1.0103],\n",
      "         ...,\n",
      "         [-0.1111, -0.3958,  1.1238,  ...,  1.3896, -0.7513, -0.4621],\n",
      "         [-0.2731, -0.3112,  1.0541,  ...,  1.4387, -0.5723, -0.6220],\n",
      "         [ 0.0807, -0.5060,  0.9266,  ...,  1.7040, -0.9748, -0.7561]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.1186, -0.2028, -0.2890,  ...,  0.1480,  0.0899,  0.0336],\n",
      "         [-0.4303,  0.5465, -0.2393,  ...,  0.4525, -0.1273, -1.3178],\n",
      "         [-0.3229, -0.2393,  0.1110,  ..., -0.2591,  0.2445, -0.5182],\n",
      "         ...,\n",
      "         [-0.1378, -0.3733,  0.6494,  ...,  0.8348, -0.5075, -0.0791],\n",
      "         [-0.2751, -0.2491,  0.5805,  ...,  0.8746, -0.4287, -0.1683],\n",
      "         [-0.0591, -0.3636,  0.5165,  ...,  1.0593, -0.6979, -0.2739]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([ 1.4908e-01,  1.2387e-01, -1.9382e-01, -2.6516e-01,  3.2723e-01,\n",
      "        -2.6683e-01,  2.4730e-01,  6.1491e-02, -7.3449e-02,  2.1492e-02,\n",
      "         1.2675e-02,  3.0995e-01, -2.6778e-01,  4.0405e-01, -3.4641e-02,\n",
      "         3.9571e-02, -1.6680e-01, -1.2976e-01, -1.3566e-01,  4.2645e-01,\n",
      "         3.9993e-01, -3.7809e-01,  1.7936e-01,  2.2313e-01, -1.1832e-01,\n",
      "         2.2211e-01,  2.1237e-01, -1.8153e-01, -2.3392e-01,  6.9849e-02,\n",
      "        -9.2087e-02,  1.3829e-01,  1.6265e-01, -1.9473e-01,  4.6062e-04,\n",
      "        -1.0237e-01, -2.2683e-01,  2.9074e-01,  2.8178e-01, -3.6748e-01,\n",
      "        -5.2879e-02, -3.0600e-01, -5.9431e-02,  2.3933e-01, -4.3620e-02,\n",
      "        -6.2528e-02, -1.5611e-01, -1.1906e-01,  1.2894e-01, -7.0883e-02,\n",
      "        -2.9995e-01,  9.9230e-02,  2.3311e-01, -2.8561e-01,  8.3432e-03,\n",
      "        -1.4152e-01,  4.5698e-02, -3.5179e-01,  1.0765e-01, -1.9651e-01,\n",
      "        -1.8129e-01,  8.5286e-03, -4.7774e-01, -2.7281e-01, -2.4864e-02,\n",
      "         1.3102e-01,  8.6735e-02, -3.2567e-02,  8.9957e-02, -1.2802e-01,\n",
      "        -1.4076e-01,  8.6996e-03, -1.9639e-01, -1.4970e-01,  2.0501e-01,\n",
      "         9.9063e-03,  1.4105e-02,  2.4861e-01,  3.0966e-02,  1.8979e-01,\n",
      "        -7.7188e-02, -5.5435e-02,  5.6634e-02,  2.7114e-01,  4.4619e-02,\n",
      "         6.1451e-02, -2.2658e-01,  2.3620e-01,  9.0390e-02,  4.1902e-02,\n",
      "         1.7128e-02, -1.3279e-01,  6.7356e-01, -2.4921e-01,  9.9231e-02,\n",
      "        -1.5657e-02,  3.2357e-01, -2.2846e-01, -5.1368e-02, -7.1767e-02,\n",
      "         4.2802e-02,  1.4500e-01, -4.5787e-02, -1.8874e-01,  1.6548e-01,\n",
      "         3.4196e-01, -1.0741e-01, -2.9142e-02,  1.1800e-01,  2.1754e-01,\n",
      "        -5.7510e-02,  4.1775e-02, -2.9146e-01, -3.7388e-01, -1.4622e-01,\n",
      "        -4.2330e-02,  7.2059e-02,  2.5794e-02, -1.6783e-01,  2.5270e-01,\n",
      "        -1.0750e-01, -5.8917e-01, -1.6664e-01,  5.5713e-02, -1.0991e-01,\n",
      "        -1.2644e-01, -6.4919e-02,  1.2673e-01, -7.4835e-03, -2.4770e-01,\n",
      "         1.3205e-01,  1.1671e-01, -4.2680e-02, -5.3891e-02, -1.7239e-01,\n",
      "        -3.0079e-01, -6.9750e-02, -3.7419e-02, -2.0132e-01, -2.5914e-01,\n",
      "        -4.9117e-03,  8.9486e-02,  1.8712e-02,  3.0112e-02,  1.4473e-02,\n",
      "        -9.0344e-01, -6.0110e-02, -1.0303e-01, -5.3968e-02,  2.8272e-01,\n",
      "        -1.9929e-02,  1.0962e-01, -1.2144e-01,  7.9465e-02,  9.0445e-02,\n",
      "        -1.7963e-01,  1.8165e-01,  3.6240e-01,  1.7090e-01, -1.4041e-02,\n",
      "         1.9184e-02,  7.5799e-03, -1.4422e-01,  5.4243e-02,  5.0206e-02,\n",
      "         2.3259e-01,  1.2299e-01, -1.9728e-01, -7.7248e-01, -4.3642e-02,\n",
      "        -1.4283e-01,  1.0638e-01,  7.3260e-02,  3.7032e-01,  9.3441e-02,\n",
      "         4.6840e-01,  4.9734e-02,  4.9681e-03, -1.5053e-01,  8.1051e-02,\n",
      "        -7.2369e-02, -4.3397e-03, -4.9834e-02,  3.5236e-01,  2.1686e-01,\n",
      "        -1.1804e-01,  3.1861e-01,  1.4513e-01,  2.2797e-01, -3.6351e-03,\n",
      "        -8.2182e-02, -4.6217e-01,  2.0373e-01, -5.3652e-02, -2.0500e-02,\n",
      "         1.0104e-01,  3.1081e-02,  2.0516e-01, -8.0955e-02, -9.0073e-02,\n",
      "        -1.7379e-01,  1.3866e-01,  1.8480e-01,  8.3222e-02, -8.1217e-02,\n",
      "         1.6176e-01,  1.2298e-01, -1.3571e-02,  8.0166e-02, -3.0001e-02,\n",
      "         7.3083e-02, -1.8209e-01,  1.6285e-01,  2.3463e-01, -1.4227e-03,\n",
      "        -3.2271e-01, -1.6259e-01, -6.5370e-02,  3.0021e-01, -2.0402e-02,\n",
      "        -1.5613e-02,  2.0378e-01,  1.1530e-01, -4.2502e-02,  1.1402e-03,\n",
      "         1.7406e-01,  1.5372e-01, -9.3741e-02,  7.5142e-02, -4.6734e-02,\n",
      "         1.2181e-01, -5.2953e-02,  1.1573e-01,  7.5447e-03, -5.4073e-03,\n",
      "         1.5250e-01,  5.5590e-02,  1.3328e-01, -1.3299e-02, -8.2515e-02,\n",
      "         3.5268e-02, -1.0056e-01, -7.0932e-02, -5.1251e-02, -1.0646e-01,\n",
      "         9.9978e-03, -2.7321e-01,  8.3973e-02, -1.3280e-01, -4.4828e-01,\n",
      "         4.6611e-02,  2.6252e-01, -8.0808e-02, -6.8779e-02, -7.1045e-02,\n",
      "        -8.1604e-02, -2.7690e-01, -1.1386e-01,  7.9901e-02,  1.8157e-02,\n",
      "         1.7551e-01, -2.1507e-01,  1.8739e-01, -1.9230e-02,  2.0729e-01,\n",
      "         1.0184e-01, -6.9293e-02,  3.8655e-02,  1.2810e-01,  1.1738e-02,\n",
      "         2.5324e-02,  2.8772e-02,  2.8810e-02, -9.8997e-02, -1.6962e-01,\n",
      "         1.5131e-01, -1.1425e-01, -2.1760e-01, -4.5138e-02, -2.7204e-01,\n",
      "         2.8566e-01,  1.5510e-01,  7.0770e-02, -2.2723e-01, -1.3153e-01,\n",
      "        -5.9633e-03,  1.0762e-01,  2.9013e-01,  1.5143e-01, -3.5916e-02,\n",
      "         6.3297e-02,  6.6008e-02, -2.1061e-01,  1.9628e-02,  2.2970e-01,\n",
      "         7.0171e-02,  4.1883e-05,  3.6176e-01, -5.2161e-02, -1.1936e-01,\n",
      "        -8.6783e-02, -5.4471e-02, -6.6908e-02,  3.1105e-02,  2.9388e-01,\n",
      "         2.6489e-01, -2.3535e-01, -1.5412e-01, -2.0185e+00, -1.4089e-01,\n",
      "        -3.3577e-02,  1.6253e-01,  6.7454e-02, -1.8524e-01,  2.9893e-02,\n",
      "        -1.8569e-01, -3.9034e-02, -7.3650e-02, -3.0490e-02, -3.5751e-02,\n",
      "         1.4023e-02, -7.1914e-02,  8.3594e-02, -1.1021e-01, -2.0924e-02,\n",
      "         1.7564e-01, -3.7428e-02, -5.4801e-02, -8.9877e-02,  5.2717e-02,\n",
      "        -4.3204e-02, -1.0025e-01, -9.8288e-02,  1.9749e-01, -1.2205e-01,\n",
      "         2.7310e-02,  9.7316e-03, -8.4249e-02, -3.9363e-04,  6.2876e-02,\n",
      "        -6.0999e-02,  8.6416e-02,  6.4864e-02, -8.4829e-02, -6.2527e-02,\n",
      "         5.1020e-01, -6.5360e-02, -1.9927e-01, -1.2727e-01, -1.9250e-01,\n",
      "        -2.5504e-02,  1.8653e-01, -8.9002e-02, -1.2349e-01, -4.2752e-02,\n",
      "        -7.1351e-02, -7.4150e-02,  7.4017e-02, -2.4354e-01,  5.0513e-02,\n",
      "         8.1920e-02, -1.0628e-01, -1.2754e-01,  1.9295e-02,  1.0775e-01,\n",
      "        -1.0230e-01, -5.1375e-03,  5.5683e-02, -1.2699e-01, -7.7957e-02,\n",
      "        -1.0605e-01, -7.9575e-02,  9.7901e-02, -1.2462e-02, -2.1976e-01,\n",
      "        -1.7569e-02, -2.7258e-02, -4.7887e-02, -2.3733e-01, -2.2269e-02,\n",
      "        -2.1753e-02, -2.1152e-01,  1.9606e-01, -2.3574e-01, -6.4681e-02,\n",
      "        -6.2689e-03, -2.0781e-01,  1.9941e-01,  1.0169e-01, -2.3552e-01,\n",
      "         1.0052e-02,  3.0391e-02,  1.6760e-01,  1.9079e-02,  1.8226e-01,\n",
      "        -5.9280e-02, -7.4923e-02, -1.0552e-01, -3.6297e-02,  4.3477e-02,\n",
      "        -4.5285e-02, -1.4976e-01,  1.2570e-02, -1.4570e-01, -5.5753e-02,\n",
      "        -3.8616e-01,  1.8542e-01, -1.0588e-02,  7.3040e-02, -9.1399e-02,\n",
      "        -2.8767e-01, -2.0149e-01, -1.5190e-02, -1.0988e-02, -9.7871e-02,\n",
      "         1.3675e-01,  5.0311e-03, -6.6983e-02,  1.3031e-01, -1.0676e-01,\n",
      "        -1.0308e-01, -3.4424e-01,  1.0380e-01, -2.0771e-01, -2.4391e-01,\n",
      "         1.1163e-01, -1.6806e-03, -1.5776e-01, -1.6872e-01,  1.1109e-02,\n",
      "        -1.0130e-02, -1.1655e-01, -1.0477e-01,  3.7815e-02,  1.1231e-01,\n",
      "         2.0840e-01, -3.0999e-01, -1.1691e-01, -1.1297e-01, -1.6702e-01,\n",
      "         5.3092e-02,  5.4440e-02,  8.3041e-02, -3.0208e-01,  1.4027e-02,\n",
      "        -4.0243e-02, -3.0690e-01,  9.0996e-02, -1.0909e-01, -2.4868e-01,\n",
      "        -2.1890e-02,  1.8476e-02,  2.0135e-01, -3.5365e-01,  1.5860e-01,\n",
      "        -5.0192e-02, -9.2075e-02,  7.9052e-02, -2.2766e-02, -1.5634e-01,\n",
      "        -2.7157e-02, -3.1175e-01,  4.6405e-02,  2.9467e-01,  1.2874e-01,\n",
      "        -2.3297e-01, -1.7375e-01,  2.6247e-02,  1.5769e-01,  4.0897e-02,\n",
      "         8.5116e-02, -2.3572e-01, -4.3676e-02, -6.1113e-02, -1.2978e-02,\n",
      "         4.3371e-03, -8.2757e-02, -1.3661e-02, -1.7326e-01, -9.5861e-02,\n",
      "        -2.5870e-02,  9.9552e-02, -8.8619e-02,  1.6603e-02, -1.7565e-01,\n",
      "         6.1347e-03,  1.9212e-01,  1.6928e-01, -3.0247e-01, -7.3087e-02,\n",
      "        -5.5806e-02, -1.0246e-01,  2.4455e-01, -1.0663e-01, -1.4906e-01,\n",
      "         1.7439e-02, -5.9003e-01,  7.5388e-02, -1.3651e-01,  8.1256e-03,\n",
      "        -2.1577e-02,  2.8644e-02,  3.1208e-02,  2.1346e-02, -1.5259e-01,\n",
      "        -1.4292e-01, -1.0508e-01, -3.0576e-02, -1.0963e-01,  1.3459e-02,\n",
      "        -1.0713e-02, -1.2893e-01,  6.7321e-02, -1.4364e-02, -2.4257e-01,\n",
      "         2.4263e-01, -1.9213e-01,  1.8522e-01, -2.6677e-01,  1.4725e-01,\n",
      "        -1.5645e-01,  1.0884e-01,  1.9280e-01, -1.8107e-01, -9.5030e-02,\n",
      "        -5.1229e-02,  1.1342e-01,  1.7577e-02,  1.0676e-01,  3.2094e-02,\n",
      "        -2.0288e-01, -1.3674e-01,  6.2849e-02, -2.1011e-02,  2.9177e-02,\n",
      "        -1.7086e-02, -9.8396e-02,  1.0205e-01, -1.9731e-02, -1.6746e-01,\n",
      "         1.0425e-01, -1.5722e-01, -1.0945e-01,  5.6515e-02, -7.3300e-02,\n",
      "        -6.1232e-01, -1.8454e-01, -2.2980e-01,  8.5435e-02,  1.7077e-01,\n",
      "        -8.8553e-02, -2.9191e-01,  7.5500e-02,  9.5263e-02,  2.1859e-01,\n",
      "        -1.2138e-01,  1.2861e-01,  1.5085e-01,  1.4101e-01, -5.5720e-02,\n",
      "        -1.2266e-01, -1.0041e-01, -1.2753e-01, -8.4831e-02, -5.6529e-02,\n",
      "         2.1101e-02,  5.8478e-02, -1.9348e-01, -1.6858e-02, -3.5771e-02,\n",
      "         5.0574e-02, -1.8273e-02,  3.3636e-02,  1.2767e-01,  3.3926e-02,\n",
      "         4.0594e-02, -1.0906e-01,  1.0634e-01,  1.2692e-01,  3.4079e-02,\n",
      "        -1.3776e-01, -8.2339e-02,  1.0153e-01,  1.0941e-02,  3.7519e-02,\n",
      "         1.0497e-01,  3.9406e-01, -6.2549e-02, -9.2865e-02, -9.7316e-02,\n",
      "        -1.7678e-02, -2.1788e-01, -1.3640e-02, -8.4834e-02,  1.9414e-01,\n",
      "        -1.4053e-02,  1.1295e-01,  1.6454e-02,  1.3462e-01,  1.0233e-01,\n",
      "         1.2413e-01,  4.5280e-02, -1.6307e-01,  7.0181e-02,  4.4070e-02,\n",
      "         8.3682e-02,  4.0648e-02,  1.5755e-01, -2.1351e-02, -8.9363e-02,\n",
      "        -1.3260e-01, -6.5820e-02,  3.9252e-02,  2.9248e-02, -1.1381e-01,\n",
      "         4.7972e-02, -1.2351e-02, -4.6980e-02,  3.6954e-02,  8.5143e-02,\n",
      "         2.3897e-02, -1.5198e-01, -4.8888e-02, -1.1015e-01, -7.5341e-02,\n",
      "         7.8378e-02,  2.8217e-01, -1.1963e-01, -1.3616e-02, -8.9681e-02,\n",
      "         3.7982e-02,  2.3843e-01, -1.5551e-01,  1.8208e-01,  2.8728e-01,\n",
      "         1.8034e-02,  8.6419e-02,  1.1385e-02,  7.2312e-02,  2.3695e-01,\n",
      "        -2.1530e-01,  1.2828e-01,  5.2344e-02,  5.7554e-02, -2.2924e-02,\n",
      "        -1.0375e-01,  2.3662e-03, -1.0316e-01, -3.5931e-02,  2.5473e-01,\n",
      "        -5.2978e-02,  3.2919e-02,  1.8880e-02,  1.9944e-01, -1.6244e-01,\n",
      "        -3.0332e-02, -1.1299e-01, -1.1068e-01,  7.2287e-03, -1.3550e-01,\n",
      "         1.5577e-02, -7.9689e-02,  7.5800e-02, -1.1706e-01,  3.1917e-02,\n",
      "        -1.0668e-01,  7.7809e-03, -4.6894e-02,  6.3662e-02, -9.9579e-02,\n",
      "        -1.9506e-02,  1.8548e-01, -7.4091e-02, -1.7413e-01,  6.3420e-02,\n",
      "        -9.2029e-02,  2.5220e-01,  5.6505e-02,  1.0384e-01, -1.1955e-01,\n",
      "         1.1596e-01, -8.4455e-02,  2.3554e-01, -1.5994e-01, -1.1280e-02,\n",
      "        -2.1256e-01,  2.0121e-01, -1.1597e-01, -1.4829e-01, -4.2132e-03,\n",
      "        -1.9714e-02,  1.3075e-01,  6.9513e-02,  8.5244e-02, -1.4833e-01,\n",
      "         2.3104e-01, -2.1775e-01, -9.4482e-02,  6.1282e-02, -2.3996e-01,\n",
      "         1.1155e-01, -1.7763e-02, -1.8718e-01, -3.8825e-02,  6.0780e-03,\n",
      "         6.6984e-02,  3.1552e-01,  1.9603e-01,  5.2209e-02, -9.1339e-03,\n",
      "        -1.4537e-01,  1.0152e-01,  5.4608e-02,  9.9402e-02, -2.3387e-01,\n",
      "         4.0388e-02,  8.8444e-03,  1.9418e-01,  1.1585e-01,  1.3871e-01,\n",
      "        -1.3353e-01, -2.0897e-01,  1.4376e-03, -1.0051e-01,  1.1219e-01,\n",
      "        -2.1526e-02, -3.2484e-02,  3.0378e-02, -2.2317e-01,  9.5877e-02,\n",
      "        -2.7256e-01,  1.4125e-01,  9.3174e-02,  1.1130e-01,  2.1801e-01,\n",
      "         4.2201e-02,  2.3711e-01, -4.9170e-02, -1.5893e-01, -1.0375e-01,\n",
      "        -1.6157e-01,  1.2011e-02,  8.9855e-02, -1.6744e-01,  5.9672e-02,\n",
      "         1.2638e-01, -7.2508e-02, -1.6503e-01,  1.0823e-01, -9.6212e-03,\n",
      "        -3.0928e-02, -1.9600e-01, -6.5314e-02,  8.6465e-02, -1.0106e-01,\n",
      "         4.4698e-02, -4.6217e-01,  1.7706e-01, -1.1506e-01, -2.0777e-02,\n",
      "         2.3295e-02,  1.5582e-01,  3.5600e-02, -2.5429e-01, -4.4140e-02,\n",
      "        -4.2500e-03, -1.0922e-01, -8.1536e-03], requires_grad=True) Parameter containing:\n",
      "tensor([0.8983, 0.8888, 0.8628, 0.8585, 0.9588, 0.9161, 0.9020, 0.8955, 0.8884,\n",
      "        0.8570, 0.8173, 0.8588, 0.8595, 0.8851, 0.8867, 0.8674, 0.7935, 0.8415,\n",
      "        0.8920, 0.9671, 0.9309, 0.9026, 0.8909, 0.8934, 0.8294, 0.9125, 0.8437,\n",
      "        0.9004, 0.8118, 0.8614, 0.8971, 0.8729, 0.9006, 0.8878, 0.9005, 0.8693,\n",
      "        0.8846, 0.9048, 0.9069, 0.9432, 0.8936, 0.9320, 0.9036, 0.8875, 0.8758,\n",
      "        0.8482, 0.9921, 0.8804, 0.9362, 0.8905, 0.9243, 0.8791, 0.9413, 0.7759,\n",
      "        0.8913, 0.7935, 0.8564, 0.8593, 0.8868, 0.8197, 0.9000, 0.8243, 0.7300,\n",
      "        0.9000, 0.8865, 0.8852, 0.8568, 0.8766, 0.8672, 0.9074, 0.8725, 0.9073,\n",
      "        0.9125, 0.7903, 0.9282, 0.8071, 0.8602, 0.7319, 0.8261, 0.8238, 0.8748,\n",
      "        0.9112, 0.9040, 0.8853, 0.7552, 0.8059, 0.8770, 0.8792, 0.8868, 0.8776,\n",
      "        0.8361, 0.8825, 1.0548, 0.8760, 0.8508, 0.8029, 0.7167, 0.8583, 0.8869,\n",
      "        0.9044, 0.9100, 0.9173, 0.8892, 0.8600, 0.8600, 0.7863, 0.8751, 0.8989,\n",
      "        0.8751, 0.8401, 0.8931, 0.8767, 0.8600, 0.9186, 0.8946, 0.8598, 0.8870,\n",
      "        0.8827, 0.8879, 0.8546, 0.8843, 0.8020, 0.8574, 0.8729, 0.7566, 0.8172,\n",
      "        0.8726, 0.7621, 0.8801, 0.8841, 0.8930, 0.8680, 0.6278, 0.8710, 0.8545,\n",
      "        0.9151, 0.8610, 0.7808, 0.8845, 0.7724, 0.8831, 0.9060, 0.9047, 0.9007,\n",
      "        0.8473, 0.8475, 0.8851, 0.8534, 0.8733, 0.8661, 0.8391, 0.8641, 0.7948,\n",
      "        0.8993, 0.8945, 0.8716, 0.8963, 0.8867, 0.9223, 0.6787, 0.8781, 0.8854,\n",
      "        0.8858, 0.9008, 0.8612, 0.9066, 0.8751, 0.8424, 0.9038, 0.8871, 0.8870,\n",
      "        0.8826, 0.8357, 0.9335, 0.9111, 0.9884, 0.8217, 0.8398, 0.8526, 0.8634,\n",
      "        0.8715, 0.8926, 0.8473, 0.9429, 0.8961, 0.8626, 0.9453, 0.8887, 0.9143,\n",
      "        0.8644, 0.9086, 0.9902, 0.9262, 0.8598, 0.8646, 0.9123, 0.9048, 0.8885,\n",
      "        0.8541, 0.8233, 0.8913, 0.9076, 0.6894, 0.8692, 0.8623, 0.8805, 0.8789,\n",
      "        0.8457, 0.8912, 0.8734, 0.8497, 0.8840, 0.8841, 0.8869, 0.8384, 0.6767,\n",
      "        0.6513, 0.9196, 0.7662, 0.8327, 0.8968, 0.8912, 0.8874, 0.8190, 0.8801,\n",
      "        0.6765, 0.8483, 0.8033, 0.8911, 0.8909, 0.8496, 0.8910, 0.8514, 0.9172,\n",
      "        0.8760, 0.8542, 0.8816, 0.8188, 0.8823, 0.8146, 0.8605, 0.8800, 0.8752,\n",
      "        0.8247, 0.8543, 0.6568, 0.8593, 0.8452, 0.8324, 0.7579, 0.8520, 0.9711,\n",
      "        0.8709, 0.8655, 0.9024, 0.8707, 0.8530, 0.8501, 0.8232, 0.8566, 0.9064,\n",
      "        0.8714, 0.9194, 0.8606, 0.8506, 0.8413, 0.8753, 0.8624, 0.8744, 0.8889,\n",
      "        0.8521, 0.8512, 0.8539, 0.8683, 0.8964, 0.8919, 0.7944, 0.8800, 0.8577,\n",
      "        0.8961, 0.8170, 0.9166, 0.8988, 0.7287, 0.8893, 0.8258, 0.8732, 0.9324,\n",
      "        0.8600, 0.8806, 0.8511, 0.7299, 0.8904, 0.8942, 0.9047, 0.7783, 0.7214,\n",
      "        0.9709, 0.8567, 0.8858, 0.9102, 0.8750, 0.8955, 0.8850, 1.0208, 0.9252,\n",
      "        0.8499, 0.8923, 2.2833, 0.9117, 0.8343, 0.6115, 0.8259, 0.8659, 0.9000,\n",
      "        0.8723, 0.9101, 0.8955, 0.8790, 0.6758, 0.8675, 0.8517, 0.8358, 0.8543,\n",
      "        0.8674, 0.9014, 0.8405, 0.8776, 0.8956, 0.8529, 0.8151, 0.7296, 0.8776,\n",
      "        0.9172, 0.9052, 0.8541, 0.8618, 0.8865, 0.8910, 0.8665, 0.8275, 0.9573,\n",
      "        0.8270, 0.8958, 0.8260, 0.6474, 0.8957, 0.8824, 0.8664, 0.9029, 0.8652,\n",
      "        0.9230, 0.8894, 0.6369, 0.8744, 0.8699, 0.8728, 0.8125, 0.8615, 0.8812,\n",
      "        0.9093, 0.8236, 0.8432, 0.8311, 0.8781, 0.9035, 0.8603, 0.8566, 0.8966,\n",
      "        0.8371, 0.8898, 0.8734, 0.9149, 0.8639, 0.9125, 0.8959, 0.8835, 0.8657,\n",
      "        0.8645, 0.9158, 0.8687, 0.5520, 0.8949, 0.9233, 0.8711, 0.8824, 0.8949,\n",
      "        0.9077, 0.8977, 0.9096, 0.8055, 0.8623, 0.8854, 0.8195, 0.8991, 0.8857,\n",
      "        0.8836, 0.8448, 0.8477, 0.9076, 0.8549, 0.7801, 0.8251, 0.8634, 0.8022,\n",
      "        0.8854, 0.9150, 0.8439, 0.8573, 0.8835, 0.8757, 0.8614, 0.9047, 0.8803,\n",
      "        0.8446, 0.8287, 0.8542, 0.8793, 0.8973, 0.7083, 0.8495, 0.8154, 0.8994,\n",
      "        0.8305, 0.8809, 0.8696, 0.8747, 0.8647, 0.8856, 0.7751, 0.8690, 0.8175,\n",
      "        0.8794, 0.8627, 0.8714, 0.9334, 0.8910, 0.8441, 0.8985, 0.8965, 0.8620,\n",
      "        0.9072, 0.8216, 0.6473, 0.8752, 0.8545, 0.9541, 0.8628, 0.8892, 0.8276,\n",
      "        0.8716, 0.9044, 0.9461, 0.9122, 0.9006, 0.8905, 0.8515, 0.8975, 0.8038,\n",
      "        0.8966, 0.8654, 0.7750, 0.9140, 0.9158, 0.8972, 0.8495, 0.8236, 0.8218,\n",
      "        0.8496, 0.8385, 0.9488, 0.8280, 0.8931, 0.8694, 0.8227, 0.8695, 0.8311,\n",
      "        0.8905, 0.8709, 0.8450, 0.8684, 0.9067, 0.7319, 0.8667, 0.8466, 0.8734,\n",
      "        0.8543, 0.8924, 0.8797, 0.8763, 0.8934, 0.8827, 0.8924, 0.8829, 0.9186,\n",
      "        0.8727, 0.7359, 0.8533, 0.8567, 0.8114, 0.8139, 0.8814, 0.8400, 0.8569,\n",
      "        0.8507, 0.9003, 0.8734, 0.8627, 0.8791, 0.9049, 0.7970, 0.8152, 0.8709,\n",
      "        0.8780, 0.8703, 0.8860, 0.8968, 0.6612, 0.9258, 0.8621, 0.9034, 0.7897,\n",
      "        0.9233, 0.9146, 0.8912, 0.8832, 0.8557, 0.8778, 0.8907, 0.6835, 0.8716,\n",
      "        0.8501, 0.8928, 0.8804, 0.9031, 0.8390, 0.8899, 0.8810, 0.8069, 0.9404,\n",
      "        0.8383, 0.7030, 0.8785, 0.8522, 0.8844, 0.9508, 0.8925, 0.8791, 0.8525,\n",
      "        0.8968, 0.8959, 0.8599, 0.8890, 0.8880, 0.9177, 0.8501, 0.8898, 0.8267,\n",
      "        0.9120, 0.8830, 0.8834, 0.8595, 0.8654, 0.9115, 0.8281, 0.8591, 0.8326,\n",
      "        0.8874, 0.8633, 0.8689, 0.8748, 0.8764, 0.8710, 0.8201, 0.8483, 0.8756,\n",
      "        0.8819, 0.5952, 0.9002, 0.8886, 0.8379, 0.8742, 0.9027, 0.8605, 0.8738,\n",
      "        0.8822, 0.9769, 0.9049, 0.8423, 0.8794, 0.9018, 0.8763, 0.8986, 0.8644,\n",
      "        0.8414, 0.8583, 0.8876, 0.8772, 0.8746, 0.8756, 0.8748, 0.9027, 0.9035,\n",
      "        0.8829, 0.9004, 0.8477, 0.9122, 0.8711, 0.8888, 0.8768, 0.8834, 0.8061,\n",
      "        0.8909, 0.8247, 0.6466, 0.8569, 0.8429, 0.9049, 0.9087, 0.8519, 0.8414,\n",
      "        0.8937, 0.9034, 0.8398, 0.8841, 0.8888, 0.9095, 0.8742, 0.8549, 0.9058,\n",
      "        0.8301, 0.8460, 0.8814, 0.8976, 0.9225, 0.8680, 0.8988, 0.8982, 0.8207,\n",
      "        0.9192, 0.8822, 0.8298, 0.8670, 0.7762, 0.8883, 0.8447, 0.8818, 0.8814,\n",
      "        0.8456, 0.8827, 0.8975, 0.8729, 0.8495, 0.8610, 0.6367, 0.8330, 0.7869,\n",
      "        0.8888, 0.8859, 0.8263, 0.8910, 0.8948, 0.8842, 0.9102, 0.8784, 0.8272,\n",
      "        0.8977, 0.8720, 0.8663, 0.8530, 0.5796, 0.8890, 0.8775, 0.8445, 0.8784,\n",
      "        0.8829, 0.8899, 0.8831, 0.9092, 0.8909, 0.8955, 0.9011, 0.8852, 0.8782,\n",
      "        0.8745, 0.8855, 0.8472, 0.8979, 0.8977, 0.8935, 0.8772, 0.9015, 0.8410,\n",
      "        0.8908, 0.8983, 0.9101, 0.8797, 0.8079, 0.8476, 0.7241, 0.9120, 0.8964,\n",
      "        0.8545, 0.8562, 0.9000, 0.8367, 0.8573, 0.8675, 0.8748, 0.8773, 0.8278,\n",
      "        0.8899, 0.8057, 0.8664, 0.8770, 0.9068, 0.8565, 0.8718, 0.9031, 0.8494,\n",
      "        0.8349, 0.9026, 0.9046, 0.8588, 0.8819, 0.9001, 0.8599, 0.8708, 0.9087,\n",
      "        0.8999, 0.9033, 0.9158, 0.8867, 0.9773, 0.8856, 0.8493, 0.9158, 0.8425,\n",
      "        0.9138, 0.8086, 0.8742, 0.8949, 0.9093, 0.8573, 0.8349, 0.8957, 0.8223,\n",
      "        0.8582, 0.8723, 0.8832, 0.9020, 0.9107, 0.8684, 0.8567, 0.8811, 0.8346,\n",
      "        0.8949, 0.8830, 0.8492, 0.7941, 0.8749, 0.8583, 0.8376, 0.8922, 0.8760,\n",
      "        0.8394, 0.8261, 0.9001], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0240,  0.2246, -0.0299,  ..., -0.0320, -0.0240, -0.0229],\n",
      "         [-0.1260, -0.1582, -0.0475,  ..., -0.0721, -0.1430, -0.0191],\n",
      "         [-0.0767, -0.0480, -0.0587,  ..., -0.0462, -0.0751, -0.0375],\n",
      "         ...,\n",
      "         [-0.0277, -0.0699, -0.0452,  ..., -0.0144, -0.0048, -0.0008],\n",
      "         [-0.0476, -0.0098, -0.0499,  ..., -0.0196, -0.0069, -0.0008],\n",
      "         [-0.0427, -0.0230, -0.0295,  ..., -0.0241, -0.0086, -0.0010]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.1006, -0.2209, -0.0915,  ...,  0.1505,  0.0859,  0.1166],\n",
      "         [-0.7888,  0.4993,  0.4750,  ...,  0.3117, -0.3097, -1.0737],\n",
      "         [-0.4839, -0.1733,  0.3466,  ..., -0.3386,  0.2832, -0.5806],\n",
      "         ...,\n",
      "         [-0.3685, -0.2360,  0.7873,  ...,  0.8993, -0.2317, -0.2631],\n",
      "         [-0.4681, -0.1852,  0.6906,  ...,  0.9045, -0.1561, -0.3807],\n",
      "         [-0.2397, -0.3184,  0.6685,  ...,  1.0203, -0.3613, -0.4498]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[-0.1301,  0.0236, -0.2214,  ..., -0.2607, -0.0433,  0.2347],\n",
      "         [ 0.0974, -0.1418, -0.0565,  ..., -0.2768, -0.0969, -0.0247],\n",
      "         [-0.0173, -0.0548,  0.0074,  ..., -0.2318,  0.1342, -0.2191],\n",
      "         ...,\n",
      "         [ 0.0959, -0.1279, -0.0396,  ..., -0.1505, -0.1152,  0.0798],\n",
      "         [ 0.0820, -0.1094, -0.0775,  ..., -0.1259, -0.0809,  0.0453],\n",
      "         [ 0.0862, -0.0858, -0.0743,  ..., -0.1323, -0.0876,  0.0351]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.1006, -0.2209, -0.0915,  ...,  0.1505,  0.0859,  0.1166],\n",
      "         [-0.7888,  0.4993,  0.4750,  ...,  0.3117, -0.3097, -1.0737],\n",
      "         [-0.4839, -0.1733,  0.3466,  ..., -0.3386,  0.2832, -0.5806],\n",
      "         ...,\n",
      "         [-0.3685, -0.2360,  0.7873,  ...,  0.8993, -0.2317, -0.2631],\n",
      "         [-0.4681, -0.1852,  0.6906,  ...,  0.9045, -0.1561, -0.3807],\n",
      "         [-0.2397, -0.3184,  0.6685,  ...,  1.0203, -0.3613, -0.4498]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.2965, -0.3224, -0.5085,  ..., -0.1222,  0.0151,  0.6985],\n",
      "         [-0.7235,  0.4427,  0.4723,  ...,  0.0869, -0.4977, -1.2675],\n",
      "         [-0.5478, -0.2737,  0.4332,  ..., -0.6115,  0.4353, -0.9935],\n",
      "         ...,\n",
      "         [-0.3066, -0.5300,  1.0847,  ...,  1.0808, -0.5399, -0.2204],\n",
      "         [-0.4830, -0.4220,  0.8920,  ...,  1.1222, -0.3836, -0.4563],\n",
      "         [-0.1149, -0.5833,  0.8559,  ...,  1.2599, -0.6752, -0.5745]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.1006, -0.2209, -0.0915,  ...,  0.1505,  0.0859,  0.1166],\n",
      "         [-0.7888,  0.4993,  0.4750,  ...,  0.3117, -0.3097, -1.0737],\n",
      "         [-0.4839, -0.1733,  0.3466,  ..., -0.3386,  0.2832, -0.5806],\n",
      "         ...,\n",
      "         [-0.3685, -0.2360,  0.7873,  ...,  0.8993, -0.2317, -0.2631],\n",
      "         [-0.4681, -0.1852,  0.6906,  ...,  0.9045, -0.1561, -0.3807],\n",
      "         [-0.2397, -0.3184,  0.6685,  ...,  1.0203, -0.3613, -0.4498]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([ 9.8154e-02,  6.3774e-04, -1.2577e-02, -2.6485e-01,  2.2569e-01,\n",
      "        -2.9641e-01,  2.4747e-01,  1.1774e-01, -2.7783e-02, -7.0422e-02,\n",
      "        -3.0908e-02,  3.1093e-01, -2.6628e-01,  2.0706e-01, -7.2847e-02,\n",
      "        -9.4162e-03, -1.3567e-01, -1.2767e-01, -6.3203e-02,  4.0762e-01,\n",
      "         3.8712e-01, -2.8452e-01,  1.4909e-01,  2.2131e-01, -5.6543e-02,\n",
      "         2.8126e-01,  1.6519e-01, -1.1127e-01, -2.4605e-01,  5.3563e-02,\n",
      "        -6.3316e-02,  1.7497e-01,  1.5856e-01, -1.2965e-01,  2.0694e-02,\n",
      "        -1.3242e-01, -2.4417e-01,  2.5358e-01,  1.9470e-01, -2.3680e-01,\n",
      "        -4.1490e-02, -2.1299e-01, -2.6798e-02,  2.2347e-01, -2.7827e-02,\n",
      "        -1.0282e-01, -1.7520e-01, -1.4033e-01,  1.0435e-01, -5.8156e-02,\n",
      "        -3.0013e-01, -6.6589e-03,  2.4403e-01, -2.9211e-01,  8.5737e-02,\n",
      "        -4.3960e-02,  6.0762e-02, -3.1801e-01,  1.6760e-01, -8.7259e-02,\n",
      "        -1.1741e-01, -2.8634e-02, -3.0703e-01, -2.7590e-01,  6.9773e-04,\n",
      "         1.4979e-01,  1.6905e-02, -5.6480e-02,  2.2336e-04, -1.6992e-01,\n",
      "        -2.5477e-01,  7.7741e-02, -2.2443e-01, -1.3008e-01,  2.0084e-01,\n",
      "         9.1861e-02, -1.3307e-02,  1.6561e-01,  9.6587e-02,  1.3529e-01,\n",
      "        -3.2601e-03, -3.1500e-02,  1.6565e-01,  1.7743e-01,  4.4448e-02,\n",
      "        -2.9374e-02, -2.5185e-01,  1.2804e-01,  3.4918e-02,  8.6807e-02,\n",
      "        -8.6376e-02, -2.5894e-02,  6.4711e-01, -1.3120e-01,  1.2133e-01,\n",
      "        -6.7510e-02,  3.0981e-01, -1.7404e-01, -5.8967e-02, -5.0193e-02,\n",
      "         1.5408e-01,  1.7392e-01,  4.7479e-02, -1.0590e-01,  1.2508e-01,\n",
      "        -9.5866e-03, -1.4982e-01, -1.4860e-01,  1.0015e-01,  4.7867e-02,\n",
      "         2.8447e-02,  5.1294e-02, -2.5824e-01, -2.4127e-01, -7.8891e-02,\n",
      "        -1.0295e-02, -4.0256e-02,  2.8219e-02, -9.4428e-02,  1.6991e-01,\n",
      "        -9.1097e-02, -4.7545e-01, -1.5529e-01, -2.0690e-02,  1.7653e-02,\n",
      "        -1.6568e-01, -8.0467e-02,  1.2093e-01,  1.3715e-02, -1.3690e-02,\n",
      "         3.6439e-02,  1.2319e-01, -1.8539e-01, -5.1824e-02, -1.3782e-01,\n",
      "        -2.2387e-01, -3.1047e-02,  9.9025e-02, -1.8112e-01, -2.1626e-01,\n",
      "        -2.8259e-02,  3.7540e-02, -8.5244e-02,  9.1933e-02, -1.7665e-02,\n",
      "        -7.1929e-01, -9.9009e-03, -4.0145e-03, -7.9340e-02,  3.5093e-01,\n",
      "        -8.2438e-03,  1.4342e-01, -1.6214e-01,  1.0111e-01,  1.1694e-01,\n",
      "        -6.0595e-02,  1.8739e-01,  3.2668e-01,  1.0441e-01, -5.3589e-02,\n",
      "        -1.5180e-02, -6.3438e-02, -1.2403e-01,  4.9650e-02, -3.3878e-02,\n",
      "         1.7127e-01,  1.0588e-01, -2.2972e-01, -9.0119e-01, -5.4623e-02,\n",
      "        -9.0948e-02,  9.0804e-02, -1.0387e-02,  4.2993e-01,  6.5765e-02,\n",
      "         2.7530e-01, -4.1744e-02, -1.4427e-02, -2.7464e-02,  1.6821e-01,\n",
      "        -1.1286e-01,  4.7933e-02, -8.4216e-02,  2.4184e-01,  1.8445e-01,\n",
      "        -6.7450e-02,  2.2213e-01,  1.5371e-01,  1.3307e-01,  2.9645e-02,\n",
      "        -3.6786e-02, -3.5829e-01,  2.0249e-01, -4.5241e-02,  5.9980e-02,\n",
      "         9.2134e-02, -6.8065e-02,  1.8779e-01, -6.9821e-02, -1.4162e-01,\n",
      "        -1.6714e-01,  1.6691e-01,  8.9460e-02,  6.3519e-02, -9.7317e-02,\n",
      "         1.2154e-01,  8.7193e-02,  1.8104e-02,  1.1518e-01, -7.4577e-02,\n",
      "         8.6523e-04, -1.4853e-01,  1.4992e-01,  2.1982e-01, -6.2652e-02,\n",
      "        -1.0333e-01, -2.1111e-01, -6.8198e-02,  3.4473e-01, -1.7158e-01,\n",
      "        -7.3759e-03,  1.4744e-01, -4.2163e-04,  1.6848e-02,  6.0283e-04,\n",
      "         3.3154e-01,  1.8997e-01, -1.2764e-01,  1.2796e-01, -3.5534e-02,\n",
      "         1.7838e-01,  2.7443e-02,  1.6520e-01, -1.6107e-02, -7.2027e-03,\n",
      "         1.6736e-01, -5.5805e-03,  1.2286e-02, -7.7282e-02, -1.9792e-01,\n",
      "        -2.8086e-02,  7.1250e-02, -1.3975e-01, -1.1017e-02,  2.4749e-02,\n",
      "        -1.6406e-01, -1.9438e-01,  9.7925e-02, -1.8627e-01, -5.0382e-01,\n",
      "         3.7032e-02,  2.6317e-01, -8.9585e-02, -6.4747e-02, -2.3155e-02,\n",
      "        -1.5937e-01, -2.7097e-01, -1.5473e-01,  5.1408e-02,  6.8162e-02,\n",
      "         1.2628e-01, -1.2923e-01,  2.0348e-01,  1.1622e-01,  1.9504e-01,\n",
      "        -2.3158e-02, -5.3770e-02,  7.0174e-02,  1.1703e-01, -3.1210e-03,\n",
      "         5.9134e-03,  2.3967e-02,  2.1468e-03,  3.1165e-02, -2.1215e-01,\n",
      "         9.8383e-02, -1.7574e-01, -2.2207e-01, -9.5243e-02, -1.7898e-01,\n",
      "         1.4797e-01,  2.9399e-02,  5.5734e-03, -1.8656e-01, -2.9962e-01,\n",
      "        -2.6311e-02,  1.2533e-01,  2.8263e-01,  9.8626e-02,  8.6000e-03,\n",
      "         2.9776e-02, -7.5487e-02, -1.1722e-01, -6.0773e-02,  1.7980e-01,\n",
      "         3.8020e-02, -4.4057e-02,  3.3132e-01,  3.0507e-02, -9.1289e-03,\n",
      "        -1.7084e-01, -2.1840e-02, -9.6562e-02,  1.3608e-01,  3.0600e-01,\n",
      "         1.8332e-01, -3.2898e-01, -1.2047e-01, -1.5130e+00, -3.0059e-02,\n",
      "        -5.8323e-02,  2.5708e-01,  4.7769e-02, -5.4290e-02,  2.9803e-02,\n",
      "         2.4728e-02,  3.2745e-02,  4.0513e-02, -8.7205e-02,  2.0399e-01,\n",
      "        -1.1941e-01,  8.6326e-04,  1.3665e-01, -2.1812e-01, -8.5742e-03,\n",
      "         1.9394e-01,  3.7148e-02, -6.8790e-02, -5.1105e-02,  5.2636e-02,\n",
      "        -8.1317e-02, -7.1392e-02,  7.8889e-02,  2.5265e-01, -1.5909e-01,\n",
      "         5.7163e-02, -6.3708e-02, -1.4868e-01,  3.9489e-03,  3.1052e-02,\n",
      "        -7.9882e-02,  7.9420e-02,  1.4721e-01, -4.2761e-02, -1.4754e-02,\n",
      "         2.7791e-01, -8.5729e-03, -1.9331e-01, -1.6957e-01, -2.6689e-01,\n",
      "        -1.0953e-01,  2.4183e-01, -5.6735e-02,  2.0545e-01, -4.2195e-02,\n",
      "        -1.1173e-01, -6.6929e-02,  8.8261e-02, -1.2616e-01,  1.2090e-01,\n",
      "         9.9619e-02, -1.0028e-01,  4.5375e-02,  8.3765e-02,  5.2962e-02,\n",
      "        -1.5338e-01, -4.7866e-02,  1.0047e-01, -1.3920e-01, -1.8127e-01,\n",
      "        -9.8146e-02,  2.9643e-03,  7.8968e-02,  8.1373e-02, -2.3570e-01,\n",
      "        -5.4736e-02, -5.9348e-02, -1.0931e-01, -2.2233e-01, -5.0199e-02,\n",
      "        -7.1237e-02, -8.6988e-01,  1.4608e-01, -3.3478e-01,  1.3826e-02,\n",
      "        -4.6592e-02, -1.7072e-01,  1.7223e-01, -1.0518e-01, -1.0429e-01,\n",
      "         7.4642e-02,  2.8558e-02,  1.4168e-01, -5.3438e-02,  1.0337e-01,\n",
      "        -4.4964e-02, -1.2888e-01, -2.7146e-01,  3.9508e-02, -8.0570e-02,\n",
      "        -6.0924e-02, -6.7347e-02,  2.3558e-02, -2.5386e-01, -6.9126e-03,\n",
      "        -3.5166e-01,  1.1461e-01,  1.7058e-02,  6.7550e-02, -1.4763e-01,\n",
      "        -1.4587e-01, -2.0417e-01,  5.9560e-03,  9.1514e-02, -3.9475e-03,\n",
      "         8.5915e-02, -3.0838e-02,  2.4874e-02,  1.8842e-02,  2.5598e-02,\n",
      "        -7.0702e-02, -2.3413e-01,  2.0478e-01, -4.4954e-02, -2.9390e-01,\n",
      "         9.9178e-02, -6.0986e-02, -1.0965e-01, -2.2159e-01,  1.8004e-01,\n",
      "         2.3073e-02, -1.6354e-01, -1.7198e-01,  1.0194e-01,  2.1425e-02,\n",
      "         2.1026e-01, -2.6333e-01, -2.1243e-01, -9.5284e-02, -1.4958e-01,\n",
      "         7.5161e-03, -1.4692e-02,  2.9291e-02, -8.7205e-02,  2.2434e-02,\n",
      "        -9.4384e-02, -2.3367e-01,  1.1959e-01, -1.2279e-02, -2.8997e-01,\n",
      "        -5.6700e-02,  2.1384e-03,  2.0294e-01, -2.5877e-01,  3.2930e-02,\n",
      "        -6.7770e-02, -2.5604e-02,  1.1318e-01, -8.5972e-02, -1.6720e-01,\n",
      "         5.4816e-02, -3.1525e-01,  7.7372e-02,  2.3332e-01,  1.0377e-01,\n",
      "        -7.1962e-02, -2.4447e-01, -3.8184e-02,  1.1163e-01,  3.6261e-02,\n",
      "        -3.2350e-02, -2.0449e-01, -5.0251e-02, -1.1258e-01, -6.7933e-02,\n",
      "        -4.6616e-02, -1.8579e-02,  3.6472e-02, -1.1585e-01, -7.3166e-02,\n",
      "         1.4847e-01,  2.5834e-01, -1.0685e-01, -1.7624e-02, -1.4362e-02,\n",
      "         4.2677e-02,  6.9527e-02,  9.1463e-02, -2.8164e-01, -3.4372e-03,\n",
      "         5.2693e-03,  2.7203e-02,  2.0807e-01,  2.2284e-02, -7.7826e-02,\n",
      "        -1.7371e-02, -7.0181e-02,  6.2733e-02, -2.9081e-02,  7.4984e-02,\n",
      "         9.3401e-03,  4.3651e-02,  1.7906e-02, -4.5930e-02, -1.0684e-01,\n",
      "        -2.1029e-02, -1.3290e-01,  5.3577e-02, -1.0844e-01, -3.9781e-02,\n",
      "        -5.1238e-02, -8.6415e-02,  1.7359e-02, -2.1246e-02, -2.4755e-01,\n",
      "         1.8788e-01, -9.4327e-02, -2.1667e-02, -2.5514e-01,  6.4396e-02,\n",
      "        -1.3820e-01,  1.8427e-01,  1.0977e-01, -1.1843e-01, -1.9014e-01,\n",
      "        -1.1078e-01,  1.0334e-01,  3.5446e-03,  7.3319e-02,  8.6781e-02,\n",
      "        -2.0812e-01, -1.0225e-01, -1.9198e-02, -4.5170e-02, -1.2390e-02,\n",
      "        -4.7242e-02, -3.9963e-02,  1.1649e-01, -1.4705e-02, -3.0744e-01,\n",
      "         2.8368e-02, -1.6747e-01, -9.8943e-02,  1.0276e-02, -8.0036e-02,\n",
      "        -6.0797e-01, -1.0748e-01, -9.2733e-02,  1.7866e-01,  1.9602e-01,\n",
      "         4.1104e-03, -1.9534e-01,  1.4699e-01,  9.9614e-02,  1.3764e-01,\n",
      "        -1.8631e-01,  1.0556e-01,  3.7236e-02,  1.1503e-01, -3.8844e-02,\n",
      "        -1.7457e-01, -9.3577e-02, -2.0534e-01, -6.8368e-02, -9.2624e-02,\n",
      "         4.7715e-02,  6.3211e-02, -1.4937e-01,  3.2467e-02, -9.5304e-02,\n",
      "         2.5179e-02, -7.6505e-03,  9.0308e-02,  4.3262e-02, -9.6863e-02,\n",
      "         1.4888e-02, -1.2210e-01, -1.5659e-01,  1.7844e-01, -8.3422e-02,\n",
      "        -1.6563e-01, -9.1568e-03,  1.4812e-01, -3.7658e-02, -2.8561e-02,\n",
      "         4.5819e-02,  3.8932e-01, -6.2121e-02, -1.1218e-01, -4.6545e-02,\n",
      "         1.2372e-01, -5.3927e-02,  5.4165e-02,  5.8166e-03,  1.2191e-01,\n",
      "        -3.7591e-03,  9.5859e-02,  1.1170e-01,  1.5269e-01,  3.4002e-02,\n",
      "         7.0052e-02,  8.2124e-02, -1.9926e-01,  1.3998e-02,  1.0645e-01,\n",
      "         7.5225e-02,  2.9195e-02,  1.3633e-01, -5.0516e-04, -1.0737e-01,\n",
      "        -1.5147e-03, -7.6138e-02, -4.5278e-02, -4.8135e-02, -9.1245e-02,\n",
      "         5.3529e-02, -1.1678e-01, -3.6211e-03, -7.5725e-02,  8.9740e-02,\n",
      "         1.4915e-01, -7.7901e-02,  2.2684e-02,  5.8973e-02, -6.1211e-02,\n",
      "         1.9602e-01,  3.1565e-01, -2.4154e-01,  3.5784e-02,  4.9467e-02,\n",
      "         1.4409e-01,  2.4490e-01, -9.8887e-02,  2.4835e-01,  9.2711e-02,\n",
      "         3.2594e-02,  7.4081e-02,  5.6852e-02,  9.8065e-02,  1.7443e-01,\n",
      "        -1.7547e-01,  1.0821e-01,  4.6956e-02, -3.4640e-02, -9.7270e-02,\n",
      "        -1.2609e-01,  1.2035e-01, -7.6743e-02, -1.0639e-01,  1.6784e-01,\n",
      "        -4.9866e-02,  1.0333e-01,  1.0522e-01,  1.5179e-01, -3.0479e-01,\n",
      "        -7.0301e-02, -2.9005e-02, -3.4377e-02, -1.2775e-02, -9.8235e-02,\n",
      "         7.0030e-02, -4.1555e-02,  2.9312e-02, -1.7456e-01,  2.6781e-02,\n",
      "        -7.3569e-02,  1.1341e-02,  1.9553e-02,  1.2608e-01,  2.6021e-02,\n",
      "        -3.6152e-03,  4.9143e-02, -1.2326e-01, -1.1573e-01,  3.9702e-02,\n",
      "        -1.4943e-01,  2.1344e-01,  9.9080e-02,  1.9108e-01, -1.0432e-01,\n",
      "         1.3486e-02, -9.2064e-02,  2.4596e-01, -9.7183e-02, -1.0261e-01,\n",
      "        -1.3897e-01,  1.7840e-01, -8.0058e-02, -1.1487e-01,  6.1808e-02,\n",
      "        -7.8657e-03,  1.6728e-01,  1.0095e-01,  1.2244e-01, -1.5050e-01,\n",
      "         2.0223e-01, -2.0201e-01, -1.0440e-01, -4.1215e-02, -1.3105e-01,\n",
      "         8.3872e-02, -3.2062e-02, -8.0579e-02,  8.3907e-02,  2.8233e-02,\n",
      "         1.0655e-01,  2.2481e-01,  1.4318e-01,  8.2618e-02,  1.2441e-01,\n",
      "        -2.4331e-01,  8.0832e-02,  1.0313e-01,  8.7676e-02, -2.1546e-01,\n",
      "         9.5545e-02,  3.8837e-02,  1.1641e-01,  4.2114e-02,  6.0657e-02,\n",
      "        -1.0108e-01, -9.0974e-02, -6.2579e-02, -8.1594e-02,  9.3718e-02,\n",
      "        -3.0397e-02, -1.4253e-02, -1.7241e-02, -1.1229e-01,  3.3603e-02,\n",
      "        -8.4441e-02,  1.9749e-01,  8.5270e-02,  9.0223e-02,  1.9584e-01,\n",
      "         6.8251e-02,  2.2177e-01,  8.4442e-02, -5.9482e-02, -1.0383e-02,\n",
      "        -7.3091e-02,  3.5653e-02,  9.8981e-02, -1.4852e-01, -2.6779e-02,\n",
      "         1.7400e-01, -1.0461e-01, -5.7997e-02,  1.1912e-01, -5.9575e-02,\n",
      "         4.6618e-02, -2.0759e-01, -3.5469e-02,  5.1045e-02, -1.3807e-01,\n",
      "         8.5282e-02, -3.9886e-01,  1.7920e-01, -1.5387e-03,  7.5340e-02,\n",
      "         1.2698e-01,  1.3463e-01,  4.1148e-02, -1.7055e-01, -1.2576e-01,\n",
      "         3.1260e-02, -7.5088e-02,  4.1695e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.9146, 0.8845, 0.8341, 0.8420, 0.9525, 0.9039, 0.9156, 0.8954, 0.8849,\n",
      "        0.8568, 0.7819, 0.8451, 0.8553, 0.8321, 0.8874, 0.8648, 0.7876, 0.8147,\n",
      "        0.9045, 0.9582, 0.9450, 0.8608, 0.9016, 0.8986, 0.8418, 0.9480, 0.8289,\n",
      "        0.8888, 0.8167, 0.8594, 0.8937, 0.8344, 0.9180, 0.8974, 0.9039, 0.8900,\n",
      "        0.9003, 0.8842, 0.9071, 0.9468, 0.8743, 0.9304, 0.8953, 0.8814, 0.8629,\n",
      "        0.8396, 0.9850, 0.8779, 0.9244, 0.8908, 0.9269, 0.8835, 0.9547, 0.7657,\n",
      "        0.8931, 0.7828, 0.8524, 0.8392, 0.8798, 0.8102, 0.8977, 0.8065, 0.6234,\n",
      "        0.8794, 0.8757, 0.8807, 0.8717, 0.8821, 0.8574, 0.9203, 0.8979, 0.9088,\n",
      "        0.8812, 0.7338, 0.9214, 0.7736, 0.8682, 0.6294, 0.8138, 0.8159, 0.8832,\n",
      "        0.9122, 0.9130, 0.8568, 0.7379, 0.8020, 0.8592, 0.8817, 0.8630, 0.9070,\n",
      "        0.8374, 0.8827, 1.0261, 0.8612, 0.8452, 0.7977, 0.6657, 0.8258, 0.8883,\n",
      "        0.8980, 0.9023, 0.9122, 0.9091, 0.8401, 0.8105, 0.5685, 0.8603, 0.8841,\n",
      "        0.8935, 0.7966, 0.8781, 0.8654, 0.8267, 0.8945, 0.8937, 0.8658, 0.8607,\n",
      "        0.8976, 0.8940, 0.8525, 0.8868, 0.7819, 0.8422, 0.8352, 0.7278, 0.8056,\n",
      "        0.8382, 0.7817, 0.8759, 0.8472, 0.8882, 0.8420, 0.5945, 0.8747, 0.8761,\n",
      "        0.9233, 0.8694, 0.7830, 0.8835, 0.7136, 0.9112, 0.9154, 0.9143, 0.9198,\n",
      "        0.8128, 0.7930, 0.8805, 0.8640, 0.8702, 0.8833, 0.8056, 0.8724, 0.7660,\n",
      "        0.8982, 0.8898, 0.8440, 0.9027, 0.8693, 0.9099, 0.6348, 0.8922, 0.8848,\n",
      "        0.8313, 0.9146, 0.8501, 0.9049, 0.8727, 0.8299, 0.8853, 0.8814, 0.8877,\n",
      "        0.8722, 0.8250, 0.9284, 0.9135, 0.8632, 0.7864, 0.8574, 0.8284, 0.8523,\n",
      "        0.8674, 0.8888, 0.8654, 0.9281, 0.9295, 0.8691, 0.9372, 0.8916, 0.9052,\n",
      "        0.8582, 0.8937, 0.9662, 0.9201, 0.8324, 0.8890, 0.9005, 0.8913, 0.8618,\n",
      "        0.8350, 0.8184, 0.8823, 0.8847, 0.6450, 0.8378, 0.8477, 0.8713, 0.8696,\n",
      "        0.8020, 0.8740, 0.8542, 0.8375, 0.8853, 0.8905, 0.8595, 0.8514, 0.5558,\n",
      "        0.6437, 0.9081, 0.7277, 0.8521, 0.8787, 0.8916, 0.8721, 0.8306, 0.8597,\n",
      "        0.6676, 0.8440, 0.7987, 0.8994, 0.8930, 0.8470, 0.8877, 0.8528, 0.9034,\n",
      "        0.8923, 0.8602, 0.8578, 0.7804, 0.8948, 0.8100, 0.8368, 0.8965, 0.8680,\n",
      "        0.7889, 0.8129, 0.6568, 0.8379, 0.8279, 0.8436, 0.7138, 0.8352, 0.9660,\n",
      "        0.8584, 0.8630, 0.8846, 0.8474, 0.8287, 0.8406, 0.7895, 0.8653, 0.9054,\n",
      "        0.8731, 0.9361, 0.8732, 0.8293, 0.8047, 0.8694, 0.8936, 0.8720, 0.9074,\n",
      "        0.8527, 0.8723, 0.8585, 0.8356, 0.9085, 0.8882, 0.7686, 0.8764, 0.8543,\n",
      "        0.8828, 0.7912, 0.9161, 0.9025, 0.7272, 0.8940, 0.7872, 0.8752, 0.9383,\n",
      "        0.8621, 0.8812, 0.8268, 0.6833, 0.9170, 0.8665, 0.9051, 0.7609, 0.6894,\n",
      "        0.9717, 0.8819, 0.9159, 0.9170, 0.8696, 0.8898, 0.8814, 0.9637, 0.9226,\n",
      "        0.8731, 0.8971, 1.7872, 0.8729, 0.8144, 0.6454, 0.7903, 0.8655, 0.8813,\n",
      "        0.8543, 0.8942, 0.8934, 0.8905, 0.5627, 0.8703, 0.8402, 0.8395, 0.8644,\n",
      "        0.8861, 0.8888, 0.8225, 0.8832, 0.8866, 0.8486, 0.8034, 0.6131, 0.8662,\n",
      "        0.9359, 0.9198, 0.8384, 0.8663, 0.9179, 0.9020, 0.8587, 0.8009, 0.9212,\n",
      "        0.8138, 0.8621, 0.8136, 0.6413, 0.8926, 0.8804, 0.8411, 0.9284, 0.8523,\n",
      "        0.9143, 0.8835, 0.6472, 0.8919, 0.8692, 0.8735, 0.8419, 0.8573, 0.8613,\n",
      "        0.8960, 0.8063, 0.8433, 0.8135, 0.8871, 0.9273, 0.8162, 0.8806, 0.8997,\n",
      "        0.8295, 0.8818, 0.8775, 0.9112, 0.8475, 0.9252, 0.8961, 0.8836, 0.8600,\n",
      "        0.8622, 0.9105, 0.8784, 3.4967, 0.8838, 0.9454, 0.8403, 0.8665, 0.8904,\n",
      "        0.9235, 0.8660, 0.9162, 0.8005, 0.8533, 0.8895, 0.8001, 0.9020, 0.8819,\n",
      "        0.8836, 0.8568, 0.8400, 0.8900, 0.8453, 0.7510, 0.7981, 0.8902, 0.7944,\n",
      "        0.8797, 0.9012, 0.8318, 0.8730, 0.8872, 0.8314, 0.8622, 0.9115, 0.8903,\n",
      "        0.8580, 0.8386, 0.8335, 0.8723, 0.8853, 0.6911, 0.8418, 0.8377, 0.9142,\n",
      "        0.8254, 0.8775, 0.8689, 0.8623, 0.8628, 0.8966, 0.8008, 0.8896, 0.8172,\n",
      "        0.9041, 0.8210, 0.8917, 0.9274, 0.8912, 0.8359, 0.8916, 0.8993, 0.8749,\n",
      "        0.8989, 0.8349, 0.6049, 0.8845, 0.8655, 0.9546, 0.8451, 0.8779, 0.8376,\n",
      "        0.8686, 0.8989, 0.9409, 0.8884, 0.9208, 0.8961, 0.8513, 0.9022, 0.8193,\n",
      "        0.9058, 0.8724, 0.7129, 0.8983, 0.9136, 0.8928, 0.8202, 0.7814, 0.8168,\n",
      "        0.8290, 0.8222, 0.9475, 0.8215, 0.8759, 0.8812, 0.8205, 0.8475, 0.8284,\n",
      "        0.8792, 0.8742, 0.8500, 0.8754, 0.9115, 0.5943, 0.8713, 0.8291, 0.8726,\n",
      "        0.8299, 0.8821, 0.8594, 0.8830, 0.8940, 0.8826, 0.8772, 0.9131, 0.8790,\n",
      "        0.8617, 0.6409, 0.8432, 0.8693, 0.8263, 0.8133, 0.8701, 0.8337, 0.8407,\n",
      "        0.8335, 0.8880, 0.8409, 0.8557, 0.8873, 0.8867, 0.7633, 0.7668, 0.8537,\n",
      "        0.8640, 0.8893, 0.8863, 0.9168, 0.6847, 0.9391, 0.8534, 0.9074, 0.7788,\n",
      "        0.9157, 0.8699, 0.8575, 0.8620, 0.8345, 0.8747, 0.8670, 0.6805, 0.8850,\n",
      "        0.8511, 0.8847, 0.8653, 0.8961, 0.8498, 0.8937, 0.8582, 0.7925, 0.9189,\n",
      "        0.7995, 0.6563, 0.9101, 0.8548, 0.8906, 0.9790, 0.8665, 0.8514, 0.8584,\n",
      "        0.8800, 0.8907, 0.8131, 0.8789, 0.9092, 0.8975, 0.8519, 0.8823, 0.8143,\n",
      "        0.9048, 0.8761, 0.8947, 0.8586, 0.8678, 0.9057, 0.8185, 0.8640, 0.8415,\n",
      "        0.8827, 0.8503, 0.8828, 0.8576, 0.8690, 0.8598, 0.7129, 0.8849, 0.8650,\n",
      "        0.8845, 0.5945, 0.9105, 0.8679, 0.8340, 0.8722, 0.8992, 0.8446, 0.8691,\n",
      "        0.8805, 0.9856, 0.9040, 0.8272, 0.8915, 0.8971, 0.8678, 0.9066, 0.8558,\n",
      "        0.7888, 0.8466, 0.8894, 0.8709, 0.8829, 0.8679, 0.8555, 0.8940, 0.9056,\n",
      "        0.8607, 0.9041, 0.8344, 0.9105, 0.8449, 0.8772, 0.8883, 0.8650, 0.7883,\n",
      "        0.8834, 0.7940, 0.6145, 0.8600, 0.8599, 0.9204, 0.8977, 0.8176, 0.8204,\n",
      "        0.8837, 0.8953, 0.8635, 0.8822, 0.8823, 0.9357, 0.8714, 0.8476, 0.9164,\n",
      "        0.8008, 0.8253, 0.8706, 0.8844, 0.8798, 0.8610, 0.8883, 0.8853, 0.7990,\n",
      "        0.9084, 0.8783, 0.8213, 0.8863, 0.7843, 0.8878, 0.8224, 0.8719, 0.8659,\n",
      "        0.8199, 0.8750, 0.8996, 0.8796, 0.8391, 0.8458, 0.6327, 0.8098, 0.7960,\n",
      "        0.8730, 0.8767, 0.8220, 0.8861, 0.9027, 0.8973, 0.9092, 0.8775, 0.8067,\n",
      "        0.9051, 0.8391, 0.8685, 0.8539, 0.5617, 0.8811, 0.8404, 0.8469, 0.8851,\n",
      "        0.8798, 0.8716, 0.9099, 0.8997, 0.8932, 0.9067, 0.8957, 0.8614, 0.8670,\n",
      "        0.8781, 0.8837, 0.8342, 0.8730, 0.8997, 0.8820, 0.8711, 0.8816, 0.8413,\n",
      "        0.9035, 0.8946, 0.9210, 0.8673, 0.7616, 0.8548, 0.6293, 0.9055, 0.9030,\n",
      "        0.8368, 0.8469, 0.8873, 0.8379, 0.8295, 0.8808, 0.8912, 0.8931, 0.7891,\n",
      "        0.9011, 0.8113, 0.8540, 0.8540, 0.8822, 0.8533, 0.8768, 0.8803, 0.8394,\n",
      "        0.8134, 0.8780, 0.8908, 0.8636, 0.8833, 0.8941, 0.8840, 0.8285, 0.8885,\n",
      "        0.8993, 0.8883, 0.9120, 0.8966, 0.9183, 0.8774, 0.8525, 0.9135, 0.8566,\n",
      "        0.9161, 0.7581, 0.8766, 0.8746, 0.9182, 0.8723, 0.8438, 0.8988, 0.8345,\n",
      "        0.8519, 0.8903, 0.8806, 0.9162, 0.9005, 0.8428, 0.8641, 0.8816, 0.8012,\n",
      "        0.9155, 0.9107, 0.8634, 0.7486, 0.8965, 0.8714, 0.8470, 0.8888, 0.8728,\n",
      "        0.7967, 0.8148, 0.9085], requires_grad=True)\n",
      "intermediate output  tensor([[[-1.6929e-01, -7.5778e-02, -1.8339e-02,  ..., -8.0334e-02,\n",
      "          -1.2201e-01, -6.6786e-02],\n",
      "         [-3.8135e-02, -4.5369e-04, -7.9413e-02,  ...,  8.4197e-02,\n",
      "          -1.7497e-02, -4.0690e-02],\n",
      "         [-1.4887e-01, -1.6817e-01,  7.6548e-02,  ..., -1.6080e-01,\n",
      "          -1.1149e-01, -4.8716e-02],\n",
      "         ...,\n",
      "         [-1.4637e-01, -3.4908e-02, -3.3086e-04,  ..., -1.1346e-01,\n",
      "           2.3303e-02,  1.1269e-01],\n",
      "         [-8.5651e-02, -7.7931e-02, -3.9731e-04,  ..., -1.6008e-01,\n",
      "           8.0091e-02,  1.2571e-01],\n",
      "         [-1.2791e-01, -8.0882e-02, -1.9641e-04,  ..., -1.1902e-01,\n",
      "           8.7895e-02,  3.4737e-01]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.1698, -0.5307, -0.6242,  ..., -0.1200,  0.1597,  0.4773],\n",
      "         [-0.7938, -0.1090,  0.7108,  ...,  0.4576, -0.5454, -1.3904],\n",
      "         [-0.4679, -0.1531,  0.3411,  ..., -0.4670,  0.1977, -0.1903],\n",
      "         ...,\n",
      "         [-0.6615, -0.8491,  0.7529,  ...,  0.5292, -0.7504, -0.1584],\n",
      "         [-0.6878, -0.7739,  0.7142,  ...,  0.6860, -0.6350, -0.3030],\n",
      "         [-0.4130, -0.8915,  0.8246,  ...,  0.8307, -0.7879, -0.4366]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention output  tensor([[[-0.3289,  0.5208, -0.4548,  ..., -0.2294,  0.2134, -0.2056],\n",
      "         [ 0.1561,  0.0687, -0.4760,  ..., -0.0908, -0.1608,  0.0037],\n",
      "         [-0.0266, -0.2418, -0.1724,  ...,  0.0117,  0.2279,  0.0795],\n",
      "         ...,\n",
      "         [-0.0879, -0.0112,  0.1206,  ...,  0.1064, -0.3725,  0.1220],\n",
      "         [-0.0258, -0.0087,  0.1116,  ...,  0.1620, -0.3612,  0.0543],\n",
      "         [-0.0447,  0.0216,  0.0513,  ...,  0.1878, -0.3071,  0.0659]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.1698, -0.5307, -0.6242,  ..., -0.1200,  0.1597,  0.4773],\n",
      "         [-0.7938, -0.1090,  0.7108,  ...,  0.4576, -0.5454, -1.3904],\n",
      "         [-0.4679, -0.1531,  0.3411,  ..., -0.4670,  0.1977, -0.1903],\n",
      "         ...,\n",
      "         [-0.6615, -0.8491,  0.7529,  ...,  0.5292, -0.7504, -0.1584],\n",
      "         [-0.6878, -0.7739,  0.7142,  ...,  0.6860, -0.6350, -0.3030],\n",
      "         [-0.4130, -0.8915,  0.8246,  ...,  0.8307, -0.7879, -0.4366]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.5689,  0.0201, -1.2041,  ..., -0.3556,  0.4594,  0.3621],\n",
      "         [-0.6393, -0.0075,  0.2279,  ...,  0.3497, -0.5997, -1.4242],\n",
      "         [-0.4885, -0.3804,  0.1562,  ..., -0.4069,  0.4524, -0.0841],\n",
      "         ...,\n",
      "         [-0.8827, -1.0054,  0.9939,  ...,  0.6959, -1.1572, -0.0017],\n",
      "         [-0.8430, -0.9161,  0.9464,  ...,  0.9291, -1.0252, -0.2630],\n",
      "         [-0.5288, -1.0297,  1.0086,  ...,  1.1179, -1.1411, -0.4168]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.1698, -0.5307, -0.6242,  ..., -0.1200,  0.1597,  0.4773],\n",
      "         [-0.7938, -0.1090,  0.7108,  ...,  0.4576, -0.5454, -1.3904],\n",
      "         [-0.4679, -0.1531,  0.3411,  ..., -0.4670,  0.1977, -0.1903],\n",
      "         ...,\n",
      "         [-0.6615, -0.8491,  0.7529,  ...,  0.5292, -0.7504, -0.1584],\n",
      "         [-0.6878, -0.7739,  0.7142,  ...,  0.6860, -0.6350, -0.3030],\n",
      "         [-0.4130, -0.8915,  0.8246,  ...,  0.8307, -0.7879, -0.4366]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([ 9.0336e-03,  5.2285e-03, -2.8415e-02, -2.2355e-01,  2.8281e-01,\n",
      "        -1.0355e-01,  2.1058e-01,  5.2141e-02,  9.3133e-03, -1.0140e-01,\n",
      "        -9.5627e-02,  2.1383e-01, -6.5770e-02,  9.9821e-02,  1.5656e-02,\n",
      "         7.9254e-02, -1.1230e-01, -2.8406e-02, -7.8713e-02,  2.4294e-01,\n",
      "         2.4602e-01, -2.3040e-01,  1.1285e-01,  3.0359e-01, -2.3410e-02,\n",
      "         2.6385e-01,  1.2775e-01, -2.4427e-01, -2.4724e-01,  6.4968e-02,\n",
      "        -4.9040e-02,  3.0989e-02,  2.4190e-01, -1.9570e-01,  8.1310e-02,\n",
      "        -1.3567e-02, -1.4962e-01,  2.3354e-01,  1.6837e-01, -4.0372e-01,\n",
      "        -8.2497e-02, -1.8586e-01, -1.6642e-02,  1.1182e-01,  8.5592e-03,\n",
      "        -1.3928e-01, -1.0552e-01, -9.6952e-02,  2.1425e-02, -1.1056e-01,\n",
      "        -2.3767e-01, -5.2616e-02,  2.2837e-01, -3.9718e-01,  3.9357e-02,\n",
      "        -7.2787e-02,  8.0630e-02, -2.5140e-01, -1.3714e-03, -7.7234e-02,\n",
      "        -5.7403e-02,  4.9509e-02, -2.1094e-01, -1.5049e-01,  2.4436e-02,\n",
      "         1.0605e-01, -2.2542e-02, -6.6176e-02,  1.5422e-02, -1.9702e-01,\n",
      "        -3.3141e-01,  9.1690e-02, -9.0190e-02, -1.1617e-01,  9.8462e-02,\n",
      "        -6.1607e-02, -3.8342e-02,  1.5944e-01,  4.5236e-02,  5.7582e-02,\n",
      "        -1.0090e-02,  1.2384e-02,  7.6772e-02,  1.3772e-01,  4.2770e-03,\n",
      "        -7.8976e-02, -8.5676e-02,  5.8032e-02,  5.1380e-04,  8.9491e-02,\n",
      "        -3.2615e-02, -1.5720e-01,  7.2433e-01, -7.8350e-02,  6.4355e-02,\n",
      "        -8.4763e-02,  1.1380e-01, -1.4482e-01, -7.4009e-02, -7.2901e-02,\n",
      "         6.5144e-02,  1.1149e-01,  2.2596e-02, -1.2960e-01,  1.3151e-02,\n",
      "        -2.0588e-01, -1.1241e-01, -1.0558e-01, -8.9636e-03,  2.9718e-02,\n",
      "        -1.0388e-01,  7.0925e-03, -3.0181e-01, -1.5717e-01, -8.3690e-02,\n",
      "         1.2701e-02, -3.2623e-02,  1.3150e-02, -1.1205e-01,  1.2246e-01,\n",
      "        -9.7161e-02, -2.9682e-01, -8.3702e-02,  4.9577e-02,  7.1420e-02,\n",
      "        -1.9664e-01, -7.6017e-02,  8.8923e-02, -4.5538e-02, -7.7344e-02,\n",
      "        -3.5484e-03,  4.3289e-02, -1.1097e-01, -9.4680e-02, -9.9394e-03,\n",
      "        -1.1565e-01, -4.4032e-02,  9.9199e-02, -2.2014e-01, -1.9625e-01,\n",
      "        -1.1376e-01,  9.4815e-03,  2.3465e-03, -1.1495e-02,  5.0283e-02,\n",
      "        -6.0909e-01,  1.0951e-02,  3.0044e-02, -1.0283e-01,  4.0362e-01,\n",
      "        -9.3080e-03,  1.6931e-01, -2.8950e-01,  1.6660e-02,  6.6859e-02,\n",
      "        -1.8190e-02,  1.0559e-01,  2.1833e-01,  1.0011e-01, -1.0045e-01,\n",
      "         4.2853e-02, -8.7105e-02, -1.2163e-01,  7.0865e-02, -1.1551e-01,\n",
      "         7.9991e-02,  1.2663e-01, -9.7014e-02, -6.9683e-01, -1.0928e-01,\n",
      "        -1.3667e-01,  7.1320e-03, -4.5795e-02,  3.9001e-01,  4.4341e-02,\n",
      "         2.1284e-01,  4.4913e-02,  9.7137e-02,  4.4726e-02,  1.0720e-01,\n",
      "        -1.9162e-02,  3.9274e-02, -1.1940e-01,  1.4703e-01,  8.2512e-02,\n",
      "        -7.1100e-02,  2.1497e-01,  1.5583e-01,  1.3691e-01, -3.5807e-02,\n",
      "        -3.9637e-02, -2.5174e-01,  1.7311e-01, -2.5205e-02,  1.1391e-01,\n",
      "         9.6349e-02, -8.1582e-02,  1.8771e-01, -2.1290e-02, -1.1039e-01,\n",
      "        -2.4997e-01,  3.1039e-02,  7.1190e-02,  1.2845e-01, -1.3078e-01,\n",
      "         1.2907e-01,  6.6490e-02,  1.2243e-03,  7.6038e-02, -5.4327e-03,\n",
      "        -3.7124e-02, -1.5022e-01,  7.0269e-02,  1.3115e-01,  9.9734e-02,\n",
      "        -9.2058e-02, -4.5981e-02, -7.3266e-02,  6.7582e-02, -2.1370e-01,\n",
      "        -6.2263e-02,  1.0242e-01, -3.8111e-02,  5.4451e-02, -8.5856e-02,\n",
      "         3.6501e-01,  1.2673e-01, -6.6931e-02,  8.0951e-02, -2.8745e-02,\n",
      "         8.1686e-02, -7.4046e-02,  4.2015e-02, -5.8721e-02,  3.1116e-02,\n",
      "         1.2470e-01, -1.0286e-01, -9.5550e-03, -7.3585e-03, -2.3900e-01,\n",
      "        -4.5236e-02,  5.9745e-02, -7.4837e-02, -1.2526e-02,  3.2314e-02,\n",
      "        -4.9047e-02, -2.5798e-01,  5.7097e-02, -2.2542e-01, -5.0367e-01,\n",
      "         3.6182e-03,  2.2921e-01, -7.6835e-02, -1.0242e-01, -7.9557e-03,\n",
      "        -8.8306e-02, -2.1782e-01,  1.0700e-02,  3.6123e-02, -3.6438e-02,\n",
      "        -2.1702e-03, -9.3591e-02,  8.4398e-02,  2.6575e-03, -3.8941e-02,\n",
      "        -8.1292e-02,  2.6883e-02,  5.7721e-02,  4.6312e-02, -1.1495e-02,\n",
      "         3.8139e-02, -3.9406e-02, -8.9335e-02, -4.6443e-02, -8.4026e-02,\n",
      "         1.3272e-01, -1.4938e-01, -1.4192e-01, -1.5603e-01, -1.4415e-01,\n",
      "         1.2822e-01,  3.4662e-02,  2.3559e-02, -9.9429e-02, -1.8124e-01,\n",
      "         4.7738e-03,  1.8395e-01,  2.3316e-01,  1.3729e-01, -3.9689e-02,\n",
      "         6.0333e-02, -8.7207e-02, -1.5517e-01, -1.9343e-02,  1.3433e-01,\n",
      "        -1.1945e-01, -6.6630e-02,  2.6264e-01,  8.0230e-02, -3.6583e-02,\n",
      "        -3.4312e-02,  6.6418e-02, -1.2824e-01,  5.6244e-02,  3.5392e-01,\n",
      "        -4.5682e-02, -2.7011e-01, -9.9110e-02, -1.2970e+00,  1.0362e-02,\n",
      "         1.1781e-02, -5.5204e-02, -9.5896e-03, -1.7513e-02, -1.9234e-02,\n",
      "         3.4248e-02,  2.2463e-02,  6.6340e-02, -7.1592e-02,  2.0376e-01,\n",
      "        -1.8350e-01,  6.0965e-02,  1.6242e-01, -2.2975e-01, -5.2453e-02,\n",
      "         9.9357e-02, -5.8088e-02, -1.2977e-01, -3.8727e-02,  3.0141e-02,\n",
      "        -7.4669e-02, -4.1295e-02,  5.3212e-02,  1.7492e-01, -4.8128e-02,\n",
      "         2.5583e-02, -1.4061e-01, -1.3333e-01, -1.1822e-02,  7.4698e-02,\n",
      "        -3.6656e-04,  1.2344e-02,  5.2643e-02, -4.2516e-02, -3.8568e-02,\n",
      "         2.3489e-01, -2.1652e-02, -2.1351e-01, -1.0318e-01, -3.0506e-01,\n",
      "        -1.1249e-01,  1.0427e-01, -8.8108e-03,  1.7477e-01,  1.6618e-02,\n",
      "        -7.9492e-02,  2.6739e-02, -5.6260e-03, -8.4520e-02,  9.5812e-02,\n",
      "         3.4080e-02, -2.8043e-03, -5.0595e-02,  7.5903e-02, -1.4101e-02,\n",
      "        -1.4086e-01, -8.2914e-02,  1.0507e-01, -1.3649e-01, -2.7506e-02,\n",
      "        -1.2047e-01, -2.1823e-02,  6.4480e-02, -7.2969e-02, -1.4135e-01,\n",
      "         1.0568e-02,  4.7503e-03, -8.7234e-03, -1.0583e-01,  5.5112e-02,\n",
      "        -9.0419e-02, -1.0916e+00,  9.6388e-02, -2.8698e-01,  3.5710e-02,\n",
      "        -7.8352e-02, -2.4311e-01,  1.0061e-01, -6.7212e-02, -1.5293e-01,\n",
      "        -1.2709e-02, -3.9582e-02,  1.1981e-01, -6.5164e-03,  6.3466e-02,\n",
      "        -3.8860e-02, -1.8429e-01, -1.7751e-01,  4.9938e-02, -1.1186e-01,\n",
      "        -1.5652e-02, -1.1511e-01,  1.8332e-02, -1.2996e-01, -1.4776e-01,\n",
      "        -2.2325e-01,  9.1054e-02, -7.2694e-02,  1.9732e-01, -6.5375e-02,\n",
      "        -7.0374e-02, -1.5113e-01,  3.1656e-02,  7.3931e-02, -1.1310e-01,\n",
      "         2.4662e-02, -2.6566e-02,  5.9458e-03, -2.0863e-02, -5.4963e-02,\n",
      "        -6.4447e-02, -3.4899e-01,  3.8133e-02, -1.1241e-01, -1.9072e-01,\n",
      "         1.3851e-01,  5.5770e-02, -1.4842e-01, -1.9034e-01,  1.3091e-01,\n",
      "         4.9112e-02, -2.2440e-01, -2.6715e-01,  1.6301e-01, -6.8629e-02,\n",
      "         1.0853e-01, -2.0320e-01, -5.3612e-02, -1.5559e-01, -1.8887e-01,\n",
      "         7.1598e-02, -5.1105e-02,  1.0509e-01, -7.9962e-02, -7.1519e-03,\n",
      "         2.0742e-02, -2.2968e-01,  8.8782e-02, -9.2715e-02, -3.1657e-01,\n",
      "        -1.5412e-02,  1.3642e-02,  1.6419e-01, -1.9913e-01,  5.6982e-02,\n",
      "        -4.8904e-02, -1.6943e-02,  9.6511e-02, -4.6775e-02, -7.8662e-02,\n",
      "         3.2571e-02, -2.4914e-01, -7.4112e-02,  1.6399e-01,  1.7284e-01,\n",
      "        -6.7856e-02, -1.0784e-01,  4.1670e-02,  3.3421e-02,  3.6985e-03,\n",
      "         1.7555e-03, -2.4169e-01, -1.2191e-02, -1.0531e-01, -1.4383e-01,\n",
      "        -8.1091e-02, -1.5756e-01,  4.3449e-02, -2.0836e-01, -6.3087e-02,\n",
      "         4.7162e-02,  1.2487e-01, -1.7479e-01, -2.0048e-02,  4.4944e-02,\n",
      "         5.0981e-03,  8.1488e-02,  1.4498e-01, -2.5543e-01, -5.2586e-02,\n",
      "        -4.0249e-02,  1.4402e-02,  6.6907e-02, -3.8030e-02, -9.9229e-02,\n",
      "        -9.9703e-02, -1.4705e-01,  1.8648e-02, -4.8384e-02,  1.3715e-01,\n",
      "         6.0175e-02,  2.9476e-02, -8.5164e-02,  2.2541e-02, -6.7690e-02,\n",
      "        -1.1113e-02, -1.7944e-01, -2.6556e-02, -1.1250e-01, -3.4658e-02,\n",
      "        -1.1201e-01, -7.6722e-02,  3.9984e-02, -7.4275e-02, -1.1728e-01,\n",
      "         2.5502e-01, -9.2376e-02, -9.7613e-02, -3.2531e-01, -2.8721e-02,\n",
      "        -1.3918e-01,  2.1222e-01,  5.7275e-02, -1.8823e-01, -1.5024e-01,\n",
      "        -3.1565e-02,  3.5345e-02, -1.6787e-02,  6.9471e-02,  8.7983e-02,\n",
      "        -7.0008e-02, -5.9802e-02, -2.9278e-03, -6.8722e-02,  4.8875e-03,\n",
      "        -8.5533e-02, -6.0505e-02,  6.2219e-03,  1.2152e-02, -2.4958e-01,\n",
      "        -1.3554e-02, -8.6929e-02, -1.7285e-01,  5.2198e-02, -1.3743e-01,\n",
      "        -7.3373e-01, -1.6765e-01, -9.4990e-02,  2.0334e-01,  7.9054e-02,\n",
      "        -8.6463e-02, -5.9376e-02,  1.5502e-01,  4.9438e-02,  3.5991e-02,\n",
      "        -9.8729e-02,  2.2769e-02,  4.7792e-02,  9.3990e-02, -7.0500e-02,\n",
      "        -1.6993e-01, -2.5577e-01, -1.3172e-01, -5.3147e-03, -7.4893e-02,\n",
      "         1.5965e-03,  1.1410e-01, -5.6585e-02, -1.7878e-02, -6.0898e-03,\n",
      "        -8.3584e-03, -1.7030e-03,  3.7460e-02,  9.1694e-02, -1.4599e-01,\n",
      "        -6.2180e-02, -1.3732e-01, -1.5730e-01,  1.5761e-01, -5.8925e-02,\n",
      "        -1.6023e-01, -7.1060e-03,  1.2592e-01, -6.7088e-02,  2.3908e-03,\n",
      "         8.3223e-03,  4.0463e-01, -5.0263e-02, -5.7076e-02, -1.9179e-02,\n",
      "         4.5129e-02, -1.4537e-02,  9.5294e-02,  2.4088e-02,  2.0403e-01,\n",
      "        -1.2687e-02, -2.5072e-02, -6.1451e-03,  9.3198e-02, -3.2659e-02,\n",
      "        -2.7385e-02,  4.8588e-02, -1.8035e-01,  4.2851e-02,  5.0967e-02,\n",
      "         1.8662e-01,  3.7509e-02,  1.8329e-01, -5.8049e-03, -5.2903e-02,\n",
      "        -9.9441e-03, -1.3935e-01, -9.4371e-02,  2.9916e-03, -1.2807e-01,\n",
      "        -3.7715e-02, -3.5035e-02, -1.1794e-02, -8.1073e-02, -8.4328e-03,\n",
      "         7.7523e-02, -6.6890e-02,  8.6839e-02,  5.1683e-02, -9.7652e-02,\n",
      "         2.3360e-01,  2.5418e-01, -1.8245e-01,  1.2412e-01,  1.0944e-01,\n",
      "         1.7953e-01,  2.3462e-01,  4.6152e-02,  1.4194e-01,  3.1772e-02,\n",
      "         7.3054e-02, -1.2898e-02,  3.8977e-02,  3.6320e-02,  1.8327e-01,\n",
      "        -1.7349e-01,  1.5755e-01,  1.1643e-01,  4.3175e-03, -4.8101e-02,\n",
      "        -1.0999e-01,  1.4522e-01, -1.1640e-01, -8.7701e-02,  4.2940e-02,\n",
      "         6.3380e-03,  1.7349e-01,  8.3371e-02,  1.3219e-01, -1.6170e-01,\n",
      "        -9.8692e-02,  6.0366e-02,  1.5533e-02, -6.4581e-02, -2.6297e-02,\n",
      "         5.7827e-02, -2.6181e-02,  8.6905e-02, -1.7798e-01,  7.2166e-02,\n",
      "        -8.5554e-02,  8.3456e-02,  3.7982e-02,  1.7078e-02,  8.0956e-02,\n",
      "         5.8977e-02,  1.1211e-03, -1.1919e-01, -9.9886e-02,  5.3761e-02,\n",
      "        -1.3990e-01,  2.6795e-01,  1.9118e-01,  2.2404e-01, -1.1108e-01,\n",
      "         2.4671e-03, -1.2989e-01,  2.5949e-01, -8.8656e-02, -1.2257e-01,\n",
      "        -5.3595e-02,  1.3283e-01, -9.9118e-02, -1.4086e-01, -1.2031e-02,\n",
      "        -4.1843e-02,  4.4548e-02,  1.1791e-01,  6.6411e-02, -1.2471e-01,\n",
      "         8.0124e-02, -1.6481e-01, -9.5394e-02,  3.7181e-03, -9.0005e-02,\n",
      "         9.1140e-02,  1.2014e-02, -1.3411e-01,  5.2264e-03,  8.1993e-02,\n",
      "         1.4718e-03,  1.0597e-01,  7.7151e-02,  6.2308e-02,  1.5275e-01,\n",
      "        -1.6316e-01,  1.1115e-01,  1.5610e-01,  1.2349e-01, -2.1435e-02,\n",
      "         1.1163e-01,  3.0151e-03,  1.2209e-01,  3.5566e-02, -7.8555e-02,\n",
      "        -9.6028e-02, -1.9912e-01, -3.7808e-02, -4.6515e-02,  1.0525e-01,\n",
      "        -1.5944e-02, -7.2695e-02, -2.4133e-02, -7.9325e-02, -3.7141e-03,\n",
      "        -6.9687e-02,  1.0560e-01,  1.6557e-01,  1.4042e-01,  2.8165e-01,\n",
      "        -4.4383e-02,  1.1224e-01,  6.0685e-02, -3.6089e-02,  2.2756e-02,\n",
      "         7.1514e-02, -1.1817e-02,  5.3818e-02, -7.8305e-02, -4.7519e-03,\n",
      "         1.8717e-01, -9.5585e-02, -7.5553e-02,  7.0850e-02,  1.1952e-02,\n",
      "         3.2495e-02, -1.4560e-01, -1.1017e-01,  1.4976e-01, -2.1868e-01,\n",
      "        -3.3066e-02, -2.9233e-01,  1.1392e-01, -1.1354e-01,  7.1420e-02,\n",
      "         2.1520e-02,  1.8998e-01,  4.7666e-02, -2.5585e-01, -9.9891e-02,\n",
      "        -1.2896e-02,  3.6183e-02,  8.3095e-03], requires_grad=True) Parameter containing:\n",
      "tensor([0.8850, 0.8693, 0.8115, 0.8027, 0.9486, 0.8652, 0.8871, 0.8831, 0.8698,\n",
      "        0.8330, 0.7637, 0.7715, 0.7981, 0.7698, 0.8465, 0.8259, 0.7443, 0.7650,\n",
      "        0.9037, 0.9052, 0.9046, 0.8314, 0.8717, 0.8952, 0.7879, 0.9215, 0.8028,\n",
      "        0.8889, 0.8073, 0.8258, 0.8728, 0.8039, 0.8982, 0.8946, 0.8787, 0.8603,\n",
      "        0.8657, 0.8557, 0.8477, 0.9793, 0.8615, 0.8907, 0.8600, 0.8537, 0.8647,\n",
      "        0.8238, 0.9300, 0.8727, 0.8987, 0.8591, 0.8971, 0.8482, 0.9514, 0.7831,\n",
      "        0.8310, 0.7822, 0.8346, 0.7853, 0.8591, 0.7893, 0.8725, 0.7595, 0.6700,\n",
      "        0.8361, 0.8412, 0.8373, 0.8506, 0.8679, 0.8495, 0.9174, 0.8723, 0.8909,\n",
      "        0.8554, 0.7368, 0.8856, 0.7299, 0.8484, 0.6672, 0.7863, 0.8175, 0.8647,\n",
      "        0.9012, 0.8948, 0.8305, 0.6795, 0.7734, 0.8164, 0.8255, 0.8551, 0.8857,\n",
      "        0.8128, 0.8716, 1.0509, 0.8217, 0.8215, 0.7679, 0.6553, 0.8087, 0.8372,\n",
      "        0.8959, 0.8908, 0.9011, 0.8649, 0.8123, 0.7725, 0.6100, 0.8323, 0.8772,\n",
      "        0.8364, 0.7842, 0.8545, 0.8459, 0.8148, 0.8938, 0.8601, 0.8321, 0.8438,\n",
      "        0.8720, 0.8864, 0.8291, 0.8626, 0.7467, 0.8015, 0.8199, 0.7186, 0.7940,\n",
      "        0.8363, 0.7522, 0.8440, 0.8234, 0.8880, 0.7965, 0.6338, 0.8407, 0.8345,\n",
      "        0.9007, 0.8326, 0.7257, 0.8713, 0.6898, 0.8897, 0.8920, 0.8826, 0.8902,\n",
      "        0.7738, 0.8315, 0.8601, 0.8311, 0.8579, 0.8716, 0.7861, 0.8381, 0.7427,\n",
      "        0.8785, 0.8550, 0.8094, 0.8622, 0.8240, 0.8710, 0.6590, 0.8674, 0.8818,\n",
      "        0.7780, 0.8800, 0.8280, 0.8809, 0.8636, 0.8070, 0.9013, 0.8536, 0.8453,\n",
      "        0.8399, 0.7974, 0.9115, 0.8921, 0.7872, 0.7671, 0.8114, 0.8100, 0.8181,\n",
      "        0.8408, 0.8712, 0.8112, 0.8891, 0.8994, 0.8510, 0.9295, 0.8575, 0.8785,\n",
      "        0.8344, 0.8653, 0.9411, 0.8991, 0.8101, 0.8538, 0.8691, 0.8841, 0.8246,\n",
      "        0.8006, 0.7448, 0.8776, 0.8507, 0.6319, 0.8011, 0.8301, 0.8440, 0.8573,\n",
      "        0.8052, 0.8503, 0.8258, 0.7898, 0.8860, 0.8388, 0.8262, 0.8240, 0.5967,\n",
      "        0.6569, 0.8931, 0.7032, 0.8325, 0.8609, 0.8861, 0.8461, 0.7736, 0.8651,\n",
      "        0.6310, 0.8154, 0.8067, 0.8627, 0.8703, 0.8284, 0.8544, 0.8399, 0.8662,\n",
      "        0.8499, 0.8086, 0.8547, 0.7501, 0.8719, 0.7933, 0.8102, 0.8630, 0.8408,\n",
      "        0.7605, 0.7987, 0.6680, 0.8077, 0.8030, 0.8082, 0.7938, 0.8124, 0.9389,\n",
      "        0.8165, 0.8378, 0.8756, 0.8291, 0.8081, 0.8044, 0.7616, 0.8318, 0.8769,\n",
      "        0.8518, 0.8873, 0.8364, 0.7928, 0.7751, 0.8396, 0.8267, 0.8324, 0.8773,\n",
      "        0.8382, 0.8497, 0.8337, 0.8065, 0.8799, 0.8837, 0.7305, 0.8562, 0.8426,\n",
      "        0.8771, 0.7632, 0.8830, 0.8860, 0.6951, 0.8731, 0.7619, 0.8572, 0.9205,\n",
      "        0.8282, 0.8823, 0.8126, 0.7094, 0.8774, 0.8323, 0.8787, 0.7345, 0.6750,\n",
      "        0.9331, 0.8428, 0.8451, 0.8937, 0.8636, 0.8543, 0.8640, 0.9679, 0.8496,\n",
      "        0.8426, 0.8868, 1.6818, 0.8512, 0.7999, 0.6490, 0.7925, 0.8311, 0.8533,\n",
      "        0.7980, 0.8785, 0.8667, 0.8249, 0.6624, 0.8517, 0.8222, 0.8189, 0.8642,\n",
      "        0.8679, 0.8820, 0.7771, 0.8548, 0.8693, 0.8298, 0.7644, 0.6444, 0.8462,\n",
      "        0.9334, 0.8796, 0.8287, 0.8557, 0.8923, 0.8747, 0.8421, 0.7590, 0.9140,\n",
      "        0.7596, 0.8562, 0.7684, 0.6392, 0.8787, 0.8760, 0.8397, 0.9157, 0.8406,\n",
      "        0.8864, 0.8494, 0.6154, 0.8651, 0.8592, 0.8232, 0.8080, 0.8208, 0.8602,\n",
      "        0.8569, 0.7944, 0.8085, 0.7914, 0.8488, 0.8747, 0.7771, 0.8696, 0.8621,\n",
      "        0.7858, 0.8616, 0.8257, 0.8940, 0.8509, 0.8833, 0.8695, 0.8531, 0.8396,\n",
      "        0.8398, 0.9034, 0.8429, 3.7374, 0.8423, 0.9144, 0.8342, 0.8423, 0.8608,\n",
      "        0.8730, 0.8520, 0.8797, 0.7748, 0.8204, 0.8671, 0.7753, 0.8750, 0.8510,\n",
      "        0.8500, 0.8235, 0.8338, 0.8771, 0.8216, 0.7368, 0.7579, 0.8413, 0.7475,\n",
      "        0.8314, 0.8866, 0.7952, 0.8763, 0.8637, 0.8080, 0.8386, 0.8846, 0.8464,\n",
      "        0.8225, 0.8022, 0.8057, 0.8386, 0.8617, 0.7066, 0.8216, 0.8185, 0.8671,\n",
      "        0.8032, 0.8462, 0.8457, 0.8442, 0.8512, 0.8686, 0.7797, 0.8512, 0.8003,\n",
      "        0.8745, 0.7961, 0.8375, 0.9205, 0.8668, 0.8226, 0.8739, 0.8631, 0.8306,\n",
      "        0.8789, 0.8021, 0.5912, 0.8488, 0.8427, 0.9343, 0.8086, 0.8538, 0.8072,\n",
      "        0.8836, 0.8718, 0.9224, 0.8551, 0.9017, 0.8732, 0.8287, 0.8713, 0.8071,\n",
      "        0.8711, 0.8436, 0.6980, 0.8607, 0.8851, 0.8620, 0.7703, 0.7350, 0.7902,\n",
      "        0.7738, 0.7976, 0.9180, 0.7706, 0.8393, 0.8542, 0.7711, 0.8384, 0.7985,\n",
      "        0.8631, 0.8458, 0.8434, 0.8486, 0.8663, 0.6658, 0.8319, 0.7989, 0.8596,\n",
      "        0.8169, 0.8491, 0.8359, 0.8489, 0.8856, 0.8456, 0.8137, 0.8780, 0.8522,\n",
      "        0.8292, 0.6304, 0.7933, 0.8540, 0.8054, 0.7910, 0.8543, 0.7988, 0.8017,\n",
      "        0.7940, 0.8717, 0.8372, 0.8258, 0.8595, 0.8500, 0.7408, 0.7778, 0.8314,\n",
      "        0.8619, 0.8428, 0.8644, 0.8706, 0.6603, 0.9494, 0.8183, 0.8846, 0.7618,\n",
      "        0.8873, 0.8440, 0.8163, 0.8417, 0.8058, 0.8688, 0.8480, 0.6464, 0.8272,\n",
      "        0.8195, 0.8578, 0.8263, 0.8789, 0.8109, 0.8564, 0.8152, 0.7638, 0.9109,\n",
      "        0.7887, 0.6671, 0.8776, 0.8020, 0.8699, 0.9841, 0.8716, 0.8246, 0.8239,\n",
      "        0.8534, 0.8780, 0.7701, 0.8414, 0.8917, 0.8848, 0.8364, 0.8447, 0.7795,\n",
      "        0.8936, 0.8650, 0.8805, 0.8398, 0.8306, 0.8897, 0.8151, 0.8315, 0.7884,\n",
      "        0.8567, 0.8566, 0.8560, 0.8124, 0.8859, 0.8490, 0.6872, 0.8286, 0.8489,\n",
      "        0.8675, 0.6240, 0.8782, 0.8463, 0.8195, 0.8376, 0.8822, 0.8177, 0.8447,\n",
      "        0.8594, 0.9784, 0.8608, 0.8085, 0.8519, 0.8747, 0.8664, 0.8970, 0.7990,\n",
      "        0.7960, 0.7944, 0.8383, 0.8342, 0.8673, 0.8362, 0.8210, 0.8885, 0.8883,\n",
      "        0.8340, 0.8867, 0.8400, 0.8793, 0.7906, 0.8517, 0.8467, 0.8182, 0.7912,\n",
      "        0.8717, 0.7544, 0.6103, 0.8290, 0.7976, 0.8864, 0.8583, 0.7969, 0.7886,\n",
      "        0.8548, 0.8861, 0.8412, 0.8398, 0.8975, 0.9123, 0.8346, 0.8174, 0.8896,\n",
      "        0.8068, 0.8157, 0.8387, 0.8569, 0.8687, 0.8215, 0.8781, 0.8641, 0.7582,\n",
      "        0.8891, 0.8598, 0.8196, 0.8441, 0.7114, 0.8811, 0.7845, 0.8436, 0.8529,\n",
      "        0.8112, 0.8472, 0.8862, 0.8625, 0.8183, 0.7923, 0.6480, 0.7893, 0.7810,\n",
      "        0.8405, 0.8508, 0.7928, 0.8570, 0.8785, 0.8829, 0.9033, 0.8575, 0.8054,\n",
      "        0.8824, 0.8265, 0.8473, 0.8210, 0.5653, 0.8461, 0.8084, 0.8000, 0.8650,\n",
      "        0.8411, 0.8698, 0.8825, 0.9204, 0.8542, 0.8927, 0.8834, 0.8579, 0.8597,\n",
      "        0.8863, 0.8447, 0.7953, 0.8564, 0.8923, 0.8756, 0.8463, 0.8706, 0.8236,\n",
      "        0.8920, 0.8600, 0.8787, 0.8168, 0.7208, 0.8156, 0.6629, 0.8969, 0.8670,\n",
      "        0.8135, 0.8187, 0.8691, 0.8053, 0.8053, 0.8289, 0.8698, 0.8603, 0.7667,\n",
      "        0.8751, 0.8126, 0.8187, 0.7971, 0.8816, 0.8092, 0.8548, 0.8822, 0.7835,\n",
      "        0.7966, 0.8399, 0.8729, 0.8256, 0.8367, 0.8706, 0.8262, 0.8057, 0.8505,\n",
      "        0.8744, 0.8596, 0.8794, 0.8816, 0.9081, 0.8772, 0.8030, 0.8818, 0.7856,\n",
      "        0.8863, 0.7395, 0.8430, 0.8722, 0.8880, 0.8189, 0.7814, 0.8807, 0.8152,\n",
      "        0.8329, 0.8486, 0.8740, 0.8924, 0.8825, 0.8023, 0.8362, 0.8630, 0.7928,\n",
      "        0.8629, 0.8481, 0.8380, 0.7351, 0.8758, 0.8517, 0.8296, 0.8681, 0.8576,\n",
      "        0.7642, 0.7803, 0.8774], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0563, -0.0186, -0.1279,  ..., -0.0814, -0.1508, -0.0216],\n",
      "         [-0.0721, -0.1697, -0.1257,  ..., -0.1341, -0.0980, -0.0920],\n",
      "         [-0.1100, -0.1429, -0.0493,  ..., -0.1357, -0.0684, -0.0163],\n",
      "         ...,\n",
      "         [-0.0771, -0.0237, -0.1006,  ..., -0.1683, -0.1129, -0.0549],\n",
      "         [-0.0934, -0.0293, -0.0814,  ..., -0.1699, -0.1251, -0.0491],\n",
      "         [-0.0864, -0.0321, -0.0661,  ..., -0.1696, -0.1482, -0.0511]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.5411,  0.0484, -0.8960,  ..., -0.2680,  0.4297,  0.3099],\n",
      "         [-0.4545, -0.5788,  0.0844,  ...,  0.6725, -0.4826, -1.5543],\n",
      "         [-0.7410, -0.7121,  0.1387,  ..., -0.6626,  0.7991, -0.0159],\n",
      "         ...,\n",
      "         [-0.6218, -0.8655,  0.6140,  ..., -0.0747, -0.5749, -0.0835],\n",
      "         [-0.6722, -0.7621,  0.8133,  ...,  0.2186, -0.6011, -0.3968],\n",
      "         [-0.3682, -0.8973,  0.9659,  ...,  0.4001, -0.6689, -0.4845]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[ 7.2628e-02,  4.0294e-01, -3.8432e-01,  ...,  5.9218e-01,\n",
      "           8.2755e-02,  1.8247e-02],\n",
      "         [-5.9376e-02,  2.1403e-01, -4.5573e-04,  ...,  2.7048e-01,\n",
      "           7.5705e-03,  5.9704e-02],\n",
      "         [ 1.7826e-01,  2.1108e-01, -6.6046e-01,  ...,  9.4686e-02,\n",
      "           2.3390e-01, -3.3306e-01],\n",
      "         ...,\n",
      "         [ 1.2101e-03, -3.2614e-02, -5.2758e-02,  ...,  3.8921e-02,\n",
      "          -3.9195e-02, -2.8374e-02],\n",
      "         [-3.0043e-02, -8.9223e-03, -4.9991e-02,  ...,  1.0841e-02,\n",
      "          -4.1093e-02, -3.2105e-02],\n",
      "         [-4.7363e-02, -3.2502e-02, -5.8118e-02,  ..., -5.3052e-03,\n",
      "          -4.2852e-02, -4.6451e-02]]], grad_fn=<AddBackward0>) tensor([[[-0.5411,  0.0484, -0.8960,  ..., -0.2680,  0.4297,  0.3099],\n",
      "         [-0.4545, -0.5788,  0.0844,  ...,  0.6725, -0.4826, -1.5543],\n",
      "         [-0.7410, -0.7121,  0.1387,  ..., -0.6626,  0.7991, -0.0159],\n",
      "         ...,\n",
      "         [-0.6218, -0.8655,  0.6140,  ..., -0.0747, -0.5749, -0.0835],\n",
      "         [-0.6722, -0.7621,  0.8133,  ...,  0.2186, -0.6011, -0.3968],\n",
      "         [-0.3682, -0.8973,  0.9659,  ...,  0.4001, -0.6689, -0.4845]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.5427,  0.4760, -1.1227,  ...,  0.3425,  0.5023,  0.4148],\n",
      "         [-0.5874, -0.3714,  0.2006,  ...,  0.8967, -0.4439, -1.5360],\n",
      "         [-0.5845, -0.4654, -0.3390,  ..., -0.4047,  0.9121, -0.2742],\n",
      "         ...,\n",
      "         [-0.8401, -1.1315,  0.7966,  ...,  0.0250, -0.7034, -0.0711],\n",
      "         [-0.9512, -0.9744,  1.0434,  ...,  0.3182, -0.7409, -0.4919],\n",
      "         [-0.5757, -1.1763,  1.2139,  ...,  0.4997, -0.8217, -0.6257]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.5411,  0.0484, -0.8960,  ..., -0.2680,  0.4297,  0.3099],\n",
      "         [-0.4545, -0.5788,  0.0844,  ...,  0.6725, -0.4826, -1.5543],\n",
      "         [-0.7410, -0.7121,  0.1387,  ..., -0.6626,  0.7991, -0.0159],\n",
      "         ...,\n",
      "         [-0.6218, -0.8655,  0.6140,  ..., -0.0747, -0.5749, -0.0835],\n",
      "         [-0.6722, -0.7621,  0.8133,  ...,  0.2186, -0.6011, -0.3968],\n",
      "         [-0.3682, -0.8973,  0.9659,  ...,  0.4001, -0.6689, -0.4845]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-6.8413e-02, -1.4685e-02,  9.7925e-02, -2.3285e-01,  2.7856e-01,\n",
      "        -9.1128e-02,  8.8250e-02,  1.3665e-01, -2.8131e-02, -1.0412e-01,\n",
      "        -1.0079e-01,  7.0019e-02, -6.4326e-02,  8.9503e-02, -4.9994e-02,\n",
      "         8.9974e-02, -2.7961e-02,  7.5033e-03, -8.2368e-02,  8.2457e-02,\n",
      "         2.0410e-01, -1.3862e-01,  8.6699e-02,  4.0604e-01, -4.0876e-02,\n",
      "         1.9091e-01,  1.2762e-01, -1.3673e-01, -2.1507e-01,  2.0646e-04,\n",
      "        -4.1168e-03,  3.3988e-02,  1.4856e-01, -1.5327e-01,  6.1553e-02,\n",
      "        -3.2388e-02, -2.4307e-01,  1.2527e-01,  1.5861e-01, -2.6779e-01,\n",
      "        -1.2452e-01, -8.5194e-02,  7.9765e-02,  1.0477e-01, -5.1023e-02,\n",
      "        -1.9498e-01, -1.9637e-02, -9.9008e-02, -2.9685e-02, -1.3916e-02,\n",
      "        -1.6059e-01, -1.0926e-01,  1.9204e-01, -3.1858e-01,  5.9194e-02,\n",
      "         6.8880e-02,  5.2595e-02, -1.8563e-01, -3.1860e-02, -1.3400e-03,\n",
      "        -8.5109e-02,  1.1847e-01, -1.0633e-01, -1.0766e-01, -2.0041e-02,\n",
      "         1.0122e-01, -9.5441e-02, -2.5757e-02, -2.1580e-02, -1.7249e-01,\n",
      "        -1.9299e-01,  6.0964e-02, -1.0986e-01, -1.3449e-01,  7.8948e-02,\n",
      "        -4.2720e-02, -1.0930e-01,  1.0152e-01,  1.5338e-02,  4.7844e-02,\n",
      "        -6.2282e-02,  1.5118e-02,  1.0789e-01,  1.5356e-01, -4.9561e-02,\n",
      "        -2.2780e-01, -9.9362e-04, -4.3155e-02, -4.5890e-02,  4.2127e-02,\n",
      "        -8.4693e-02, -2.2918e-01,  6.2738e-01, -6.6771e-02,  9.6695e-02,\n",
      "        -1.0027e-01,  2.3973e-01, -1.1050e-01, -5.8350e-02, -8.1114e-02,\n",
      "         1.7023e-02,  7.5210e-02, -2.6273e-02, -5.0015e-02,  4.9011e-02,\n",
      "         6.9222e-03, -8.5743e-02, -1.1984e-01, -8.1934e-02, -2.8638e-02,\n",
      "        -1.6781e-01, -8.9566e-03, -2.9742e-01, -1.1436e-01, -2.5355e-02,\n",
      "         1.9245e-02,  3.0905e-02, -1.3231e-02, -1.7387e-01,  9.1912e-02,\n",
      "        -9.6294e-02, -1.4875e-01, -6.4018e-02,  6.9584e-02,  1.2863e-01,\n",
      "        -2.5615e-01, -6.9177e-02,  2.2179e-01, -1.2918e-01, -3.0907e-02,\n",
      "        -1.0416e-01,  1.4877e-01, -1.5592e-01, -1.0273e-01, -8.2931e-02,\n",
      "        -7.2520e-02,  3.3927e-03,  8.0055e-02, -1.3827e-01, -1.8813e-01,\n",
      "        -1.0798e-01, -3.1946e-02,  1.4345e-02,  5.4373e-02, -8.5776e-03,\n",
      "        -4.9808e-01, -7.0646e-02,  2.9841e-02, -8.7338e-02,  2.3898e-01,\n",
      "         1.0324e-01,  1.7185e-01, -9.4743e-02,  8.1135e-02, -8.8625e-03,\n",
      "         5.8927e-02,  1.0045e-01,  1.8999e-01,  4.9619e-02, -2.1625e-01,\n",
      "         9.8230e-03, -1.5053e-01, -8.8393e-02,  5.1734e-02, -1.7297e-02,\n",
      "         5.9621e-02,  1.6417e-01, -2.6357e-02, -4.6794e-01, -1.6534e-01,\n",
      "        -1.6453e-01, -6.0327e-02, -1.0038e-01,  2.1235e-01, -2.4608e-02,\n",
      "         2.2982e-01,  5.4501e-03,  1.4479e-01,  1.6694e-01, -5.9400e-02,\n",
      "        -1.0492e-02, -1.8781e-02, -1.2721e-01,  1.2032e-01,  8.6587e-02,\n",
      "        -7.8543e-02,  1.3054e-01,  1.7380e-01,  5.6252e-02,  9.5063e-02,\n",
      "        -6.2605e-03, -3.0282e-01,  2.1641e-01, -3.5409e-02,  1.0196e-01,\n",
      "         8.1524e-02, -2.2450e-02,  1.9720e-01,  2.9872e-02, -1.1372e-01,\n",
      "        -2.1850e-01,  4.2091e-02,  1.3902e-01,  8.0194e-02, -1.5349e-01,\n",
      "         1.2549e-01,  6.1810e-02, -1.4126e-01,  5.3815e-02,  9.5186e-02,\n",
      "        -6.1146e-02, -9.5448e-02,  6.6996e-03,  1.8820e-01,  7.1465e-02,\n",
      "        -7.1717e-02, -2.0179e-01, -8.3425e-02,  1.6248e-01, -1.1953e-01,\n",
      "        -6.5946e-02,  2.0023e-01, -4.7521e-02,  6.1980e-02, -1.0993e-01,\n",
      "         2.2824e-01,  5.7869e-02, -2.0957e-01,  2.6353e-02, -5.2742e-02,\n",
      "        -2.4369e-03, -3.0650e-02,  7.4169e-02, -1.0147e-02, -7.8982e-04,\n",
      "         1.6888e-01, -1.4766e-02, -1.9214e-01,  1.4586e-02, -2.0228e-01,\n",
      "         1.2323e-02,  1.0994e-01, -1.7867e-01,  2.2628e-02, -4.5112e-03,\n",
      "        -6.5385e-02, -1.1412e-01,  6.0078e-02, -1.9050e-01, -2.2420e-01,\n",
      "        -3.2391e-02,  4.2866e-01,  4.1080e-02, -6.4562e-02,  9.0099e-03,\n",
      "        -6.6397e-02, -3.8664e-01, -4.6759e-03,  3.3033e-02, -8.0993e-02,\n",
      "        -7.8662e-03, -1.3730e-01,  1.7759e-02,  7.2331e-03, -9.2599e-02,\n",
      "        -1.1557e-01, -5.1022e-02,  2.6566e-02,  7.4261e-02, -4.8146e-02,\n",
      "         9.9368e-03, -1.8030e-02, -1.1669e-01,  1.0394e-01, -3.5411e-02,\n",
      "         8.9891e-02, -1.3748e-01, -1.5533e-01, -1.6918e-01, -1.2368e-01,\n",
      "         9.7652e-02,  1.2463e-02, -1.0896e-01, -2.4045e-01, -2.4022e-01,\n",
      "         1.0053e-01,  1.2797e-01,  1.9155e-01,  2.8814e-02, -1.7075e-02,\n",
      "         6.6131e-02, -1.4223e-01, -9.9026e-02, -4.1128e-02,  9.7625e-02,\n",
      "        -6.7123e-02, -8.0236e-03,  2.8599e-01,  6.3024e-02, -2.4517e-02,\n",
      "        -3.7168e-02,  7.8311e-02, -1.4959e-01,  8.9837e-02,  3.1989e-01,\n",
      "        -3.9809e-02, -8.7327e-02, -6.4228e-02, -1.1159e+00,  7.2103e-03,\n",
      "        -6.4932e-02, -9.3647e-02,  1.1436e-01,  3.0608e-02, -9.0261e-02,\n",
      "         2.1614e-02,  4.2796e-02, -1.9691e-02, -1.6561e-01,  1.4114e-03,\n",
      "        -2.4439e-01,  5.8508e-02,  1.3969e-01, -2.9009e-01, -1.3038e-01,\n",
      "         1.0303e-01, -7.1625e-02, -1.8464e-01, -3.9552e-02, -9.8607e-02,\n",
      "        -4.6643e-02, -5.9559e-02,  7.7511e-02,  6.5654e-02, -6.9893e-02,\n",
      "         5.0171e-02, -6.8154e-02, -1.6579e-01, -3.8187e-02, -7.3564e-02,\n",
      "        -3.9211e-02, -2.9803e-02,  1.4025e-01, -4.5741e-02,  3.0368e-02,\n",
      "        -1.9968e-02,  5.6872e-02, -1.0294e-01, -1.7294e-01, -2.3538e-01,\n",
      "        -1.7551e-01,  1.8185e-02,  4.8804e-02,  3.8119e-03, -1.7166e-02,\n",
      "        -4.5914e-02,  1.1703e-01,  8.3309e-02, -1.3071e-02,  6.3116e-02,\n",
      "         2.2611e-02,  6.7043e-02, -8.9732e-03,  1.9178e-02,  1.8123e-02,\n",
      "        -8.4349e-02, -6.6815e-02,  1.3596e-01, -1.1523e-01, -1.4107e-01,\n",
      "        -8.0859e-02, -7.0844e-02,  3.7028e-02, -4.4255e-02, -1.8313e-01,\n",
      "        -7.2192e-03,  2.1698e-02, -1.4655e-01, -1.9548e-01,  6.9601e-02,\n",
      "        -2.3097e-02, -1.3960e+00,  5.7968e-02, -2.1191e-01,  7.7244e-02,\n",
      "        -1.2873e-01, -3.1151e-01,  7.3383e-02, -8.4914e-02, -3.3021e-02,\n",
      "         1.0113e-01,  5.9477e-02,  1.3377e-01, -1.0959e-01,  3.5372e-02,\n",
      "        -4.0199e-02, -1.7281e-01, -2.1721e-01,  6.9882e-02, -1.0425e-01,\n",
      "        -9.5598e-02, -1.7582e-01,  1.5404e-01, -1.3261e-01, -6.3073e-02,\n",
      "        -1.2300e-01,  5.1205e-02, -1.1124e-01,  1.4006e-01, -1.5453e-01,\n",
      "        -1.1277e-01, -6.3515e-02, -5.4086e-03, -1.1250e-03, -1.5718e-01,\n",
      "         4.9910e-02, -5.2535e-02, -2.8268e-02,  8.2641e-02,  3.6361e-02,\n",
      "        -1.5047e-01, -1.7742e-01,  4.1598e-02, -8.0393e-03, -2.1642e-01,\n",
      "         1.8160e-01,  3.9809e-02, -2.1557e-01, -2.3357e-01,  1.1748e-01,\n",
      "         7.5901e-02, -2.5071e-01, -2.2760e-01,  2.4158e-01, -1.9920e-01,\n",
      "         1.0629e-01, -2.0799e-01, -4.2341e-02, -1.6591e-01, -5.0407e-02,\n",
      "         5.1418e-02,  4.5848e-02,  1.5719e-01, -1.2281e-01,  1.6653e-02,\n",
      "        -3.8105e-02, -2.1069e-01,  4.5201e-02, -8.0104e-02, -2.3977e-01,\n",
      "        -1.7001e-02, -6.8664e-02,  1.1687e-01, -1.5591e-01,  9.1707e-02,\n",
      "         3.4915e-02, -1.2525e-01,  1.2920e-01, -9.6741e-02, -1.1362e-01,\n",
      "         3.5749e-02, -2.4381e-01, -9.8565e-02,  1.2220e-01,  9.5467e-02,\n",
      "        -1.1720e-01, -6.3551e-02, -1.0808e-01,  3.6753e-02, -1.1724e-02,\n",
      "         3.6011e-02, -1.2097e-01, -7.9562e-02, -5.0209e-02, -3.0134e-02,\n",
      "        -7.5588e-02, -1.4164e-01,  7.5756e-02, -1.9803e-01, -1.0804e-01,\n",
      "         1.0304e-01,  1.6737e-01, -1.6566e-01,  4.6699e-04, -5.1747e-02,\n",
      "        -1.3254e-03,  9.3415e-02,  8.7815e-02, -2.1817e-01, -3.3234e-02,\n",
      "        -6.0532e-02, -1.2367e-04,  4.5735e-02, -1.9991e-02, -1.6754e-02,\n",
      "        -7.4464e-02,  4.6651e-02,  2.7128e-02, -7.9539e-02,  1.0701e-02,\n",
      "         1.1578e-01,  1.1617e-01, -4.8222e-02, -2.4963e-02, -2.9761e-02,\n",
      "         3.9485e-02, -1.1065e-01, -5.3894e-02, -1.4495e-01,  4.0127e-02,\n",
      "        -1.4562e-01, -5.6944e-02, -3.2598e-02, -1.1129e-01, -1.0191e-01,\n",
      "         2.5794e-01, -6.5729e-02, -2.2104e-02, -2.3917e-01,  8.3269e-02,\n",
      "        -1.1649e-01,  2.9953e-01,  9.9733e-02, -1.2288e-01,  2.8718e-02,\n",
      "        -2.7494e-02,  2.1160e-02, -7.0543e-02,  8.0848e-02,  1.1905e-01,\n",
      "        -8.9125e-03, -1.6610e-01, -6.5870e-02, -7.9934e-02,  7.0267e-02,\n",
      "        -1.0885e-01, -5.3426e-02,  8.2237e-04,  4.8007e-02, -3.0076e-01,\n",
      "        -1.0827e-02,  1.0854e-02, -1.3745e-01, -1.3607e-03, -1.3428e-01,\n",
      "        -4.4595e-01, -2.2266e-01, -7.2535e-02,  2.4056e-01,  1.8615e-01,\n",
      "        -3.3728e-02, -9.5504e-02,  7.1057e-03,  5.9112e-02, -3.1857e-02,\n",
      "        -6.0401e-02,  1.2101e-02,  9.3061e-02,  7.1677e-02, -5.6540e-02,\n",
      "        -6.9476e-02, -1.7546e-01, -6.1857e-02, -5.5901e-02, -1.4457e-01,\n",
      "         5.0644e-02,  1.0009e-01, -1.2428e-01, -2.7285e-02, -9.2139e-02,\n",
      "         1.0749e-02,  3.4633e-02,  4.0659e-02, -7.6864e-02, -1.6425e-01,\n",
      "        -3.5354e-02, -1.8655e-01, -9.0537e-02,  1.4516e-01, -7.9572e-02,\n",
      "        -1.7576e-01,  7.7620e-02,  1.2878e-01, -1.8386e-01, -1.9528e-01,\n",
      "        -6.1632e-02,  3.7473e-01, -7.9596e-02, -1.5307e-01,  8.0662e-02,\n",
      "         2.2754e-02, -3.0187e-02,  9.6166e-02,  8.3238e-02,  6.2987e-02,\n",
      "        -1.6073e-02,  1.0959e-01,  4.7464e-02,  8.5676e-02,  2.2047e-02,\n",
      "         4.9718e-02,  2.8123e-02, -1.0709e-01,  5.4231e-02,  1.3411e-01,\n",
      "         1.7343e-01,  1.1455e-01,  7.2517e-02,  9.1527e-02, -3.9772e-04,\n",
      "         9.6347e-03, -1.8090e-01, -1.0493e-01,  7.0741e-02, -2.8755e-01,\n",
      "        -3.5062e-02, -8.3939e-02, -3.9652e-03, -9.9245e-02, -1.2798e-01,\n",
      "         1.0584e-01,  1.7753e-02,  1.1607e-01,  9.7722e-02, -3.9840e-02,\n",
      "         1.0661e-01,  2.2482e-01, -1.6806e-01,  5.4712e-02,  8.6336e-03,\n",
      "         1.8843e-01,  9.0219e-02,  2.8398e-02,  2.3419e-01, -1.5213e-02,\n",
      "        -9.6268e-03,  3.5601e-03,  1.8767e-03,  1.7073e-01,  9.7382e-02,\n",
      "        -5.7197e-02,  1.5349e-01,  8.9408e-02, -7.6702e-02, -4.9486e-02,\n",
      "        -1.6533e-01,  1.5982e-02, -1.6690e-02, -4.8249e-02,  1.0601e-03,\n",
      "        -6.5780e-02,  1.5702e-01,  1.2802e-01,  1.0183e-01, -1.8196e-01,\n",
      "        -1.2446e-02,  1.3715e-02, -2.0893e-03, -5.9811e-02, -1.0643e-01,\n",
      "         8.7622e-02, -2.7751e-02,  5.5097e-02, -1.3291e-01,  5.6253e-02,\n",
      "        -1.2122e-01,  1.3294e-01,  1.3929e-01,  8.8614e-02,  9.6920e-02,\n",
      "         3.7516e-02,  4.2057e-02, -1.5608e-01, -1.8384e-02,  6.6939e-02,\n",
      "        -1.3783e-01,  2.9469e-01,  1.9535e-01,  1.3888e-01, -7.3465e-02,\n",
      "        -1.5283e-02, -6.6779e-02,  1.9453e-01, -4.1716e-03, -1.6370e-01,\n",
      "        -3.6217e-02,  1.3067e-01, -1.1984e-03, -1.4443e-01, -2.4163e-02,\n",
      "        -6.3179e-02,  2.9602e-02,  8.7595e-02,  6.4204e-03, -2.6012e-02,\n",
      "         2.6654e-02, -1.8188e-01, -4.5173e-04,  1.7959e-04, -1.2388e-01,\n",
      "         5.5466e-02, -4.6545e-02, -5.4421e-02,  1.3971e-02,  9.4679e-02,\n",
      "        -2.9822e-03,  2.3863e-01,  8.6586e-02,  3.1484e-02,  1.1449e-01,\n",
      "         6.0149e-02,  1.7138e-02,  3.6635e-02,  1.1296e-01, -6.6452e-03,\n",
      "         1.7987e-01, -3.5096e-02,  6.2008e-02, -4.0595e-02, -2.5367e-02,\n",
      "        -7.8834e-02, -1.1187e-01, -7.6657e-02, -4.7279e-02,  9.4460e-02,\n",
      "        -3.4396e-02, -6.3446e-02, -6.6970e-02, -9.2655e-02,  2.0745e-02,\n",
      "         5.5555e-02,  1.4707e-01,  9.9179e-02,  1.2400e-01,  2.5084e-01,\n",
      "         1.4453e-02,  4.2043e-03,  1.2572e-01, -1.3068e-02,  7.3298e-02,\n",
      "         2.1079e-02, -4.0195e-02, -1.8587e-02, -1.3593e-02, -3.8091e-02,\n",
      "         8.1169e-03, -8.7447e-02, -2.1214e-02,  1.4264e-01,  1.5662e-03,\n",
      "         6.1930e-02, -1.2532e-01, -3.3435e-02,  1.4278e-01, -2.0842e-01,\n",
      "        -9.5605e-02, -1.1476e-01,  7.4646e-02, -8.6479e-02,  4.3559e-02,\n",
      "         4.6354e-02,  1.0192e-01, -1.3717e-02, -1.3720e-01, -2.0517e-01,\n",
      "         3.5646e-02, -9.0082e-03,  4.1935e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8908, 0.8788, 0.8164, 0.8048, 0.9654, 0.8963, 0.8816, 0.8757, 0.8651,\n",
      "        0.8503, 0.7664, 0.7568, 0.7937, 0.7722, 0.8678, 0.8452, 0.7927, 0.7914,\n",
      "        0.9183, 0.8864, 0.9019, 0.8231, 0.8737, 0.9217, 0.7941, 0.9489, 0.7704,\n",
      "        0.8952, 0.8165, 0.8367, 0.8928, 0.7792, 0.9094, 0.8905, 0.8767, 0.8622,\n",
      "        0.9040, 0.8477, 0.8894, 0.9514, 0.8804, 0.9021, 0.9087, 0.8814, 0.8505,\n",
      "        0.8007, 0.9343, 0.8689, 0.9156, 0.8826, 0.8891, 0.8573, 0.9496, 0.7666,\n",
      "        0.8615, 0.7722, 0.8453, 0.7536, 0.8582, 0.7698, 0.8772, 0.7749, 0.6608,\n",
      "        0.8216, 0.8478, 0.8528, 0.8442, 0.8817, 0.8462, 0.9146, 0.9024, 0.9180,\n",
      "        0.8691, 0.7560, 0.9035, 0.7610, 0.8606, 0.7004, 0.7597, 0.8067, 0.8652,\n",
      "        0.9091, 0.9170, 0.8574, 0.7027, 0.8067, 0.8134, 0.8344, 0.8666, 0.9005,\n",
      "        0.8231, 0.8854, 1.0371, 0.8027, 0.8288, 0.7859, 0.6780, 0.8018, 0.8596,\n",
      "        0.8926, 0.9100, 0.9256, 0.8634, 0.8238, 0.7748, 0.6194, 0.8344, 0.8917,\n",
      "        0.8711, 0.7706, 0.8724, 0.8399, 0.7956, 0.8823, 0.8686, 0.7918, 0.8294,\n",
      "        0.8789, 0.8905, 0.8331, 0.8492, 0.7266, 0.8159, 0.7940, 0.6938, 0.8148,\n",
      "        0.8577, 0.7830, 0.8516, 0.8245, 0.8907, 0.8240, 0.6747, 0.8707, 0.8536,\n",
      "        0.8959, 0.8640, 0.7483, 0.8731, 0.6711, 0.9239, 0.9064, 0.8956, 0.9171,\n",
      "        0.8029, 0.8463, 0.8880, 0.8416, 0.8722, 0.8806, 0.7999, 0.8578, 0.7188,\n",
      "        0.8973, 0.8984, 0.8117, 0.8624, 0.8186, 0.8781, 0.6559, 0.8868, 0.8851,\n",
      "        0.7878, 0.9152, 0.8266, 0.9007, 0.8710, 0.7917, 0.8624, 0.8765, 0.8728,\n",
      "        0.8460, 0.7989, 0.8973, 0.9258, 0.8109, 0.7471, 0.8387, 0.8069, 0.8431,\n",
      "        0.8429, 0.8901, 0.8488, 0.8992, 0.8999, 0.8551, 0.9280, 0.8629, 0.8911,\n",
      "        0.8487, 0.8864, 0.9773, 0.9304, 0.8316, 0.8771, 0.8801, 0.8869, 0.8117,\n",
      "        0.8275, 0.7544, 0.8871, 0.8603, 0.6859, 0.7912, 0.8320, 0.8471, 0.8507,\n",
      "        0.7920, 0.8711, 0.8214, 0.7989, 0.8993, 0.8374, 0.8499, 0.8509, 0.6132,\n",
      "        0.6951, 0.9149, 0.7354, 0.8341, 0.8751, 0.8963, 0.8635, 0.7770, 0.8539,\n",
      "        0.7097, 0.7930, 0.8162, 0.8697, 0.9053, 0.8251, 0.8822, 0.8428, 0.8770,\n",
      "        0.8609, 0.8360, 0.8552, 0.7617, 0.8873, 0.7741, 0.8112, 0.8678, 0.8439,\n",
      "        0.7749, 0.7968, 0.6860, 0.7877, 0.8312, 0.8163, 0.7448, 0.8139, 0.9919,\n",
      "        0.8192, 0.8435, 0.8946, 0.8531, 0.8369, 0.8194, 0.7870, 0.8148, 0.8991,\n",
      "        0.8371, 0.8928, 0.8408, 0.7998, 0.7640, 0.8475, 0.8149, 0.8343, 0.9014,\n",
      "        0.8460, 0.8532, 0.8335, 0.8171, 0.8946, 0.8827, 0.7228, 0.8371, 0.8271,\n",
      "        0.8711, 0.7799, 0.8890, 0.8948, 0.7150, 0.8847, 0.7962, 0.8817, 0.9214,\n",
      "        0.8282, 0.8916, 0.8238, 0.7211, 0.8743, 0.8313, 0.8848, 0.7219, 0.6998,\n",
      "        0.9251, 0.8599, 0.8724, 0.9042, 0.8620, 0.8663, 0.8632, 1.0249, 0.8289,\n",
      "        0.8143, 0.9038, 1.3370, 0.8474, 0.7927, 0.6685, 0.8016, 0.8646, 0.8719,\n",
      "        0.8014, 0.8917, 0.8774, 0.8691, 0.6290, 0.8708, 0.8092, 0.8343, 0.8830,\n",
      "        0.8786, 0.8668, 0.7881, 0.8862, 0.8787, 0.8580, 0.7634, 0.6533, 0.8373,\n",
      "        0.9250, 0.9081, 0.8397, 0.8398, 0.8995, 0.8849, 0.8411, 0.7698, 0.9235,\n",
      "        0.7673, 0.8548, 0.7791, 0.6511, 0.9051, 0.8719, 0.8421, 0.9104, 0.8582,\n",
      "        0.8966, 0.8828, 0.6337, 0.8788, 0.8649, 0.8321, 0.8038, 0.8214, 0.8364,\n",
      "        0.8936, 0.8064, 0.8134, 0.8003, 0.8533, 0.8695, 0.7749, 0.8725, 0.8836,\n",
      "        0.8074, 0.8631, 0.8339, 0.9178, 0.8650, 0.8996, 0.9094, 0.8760, 0.8360,\n",
      "        0.8272, 0.9241, 0.8695, 3.3955, 0.8705, 0.9307, 0.8362, 0.8600, 0.8674,\n",
      "        0.9013, 0.8620, 0.8814, 0.8066, 0.8540, 0.8729, 0.8042, 0.8855, 0.8820,\n",
      "        0.8643, 0.8228, 0.8592, 0.9016, 0.8140, 0.7394, 0.7688, 0.8432, 0.7460,\n",
      "        0.8320, 0.8900, 0.8326, 0.8837, 0.8803, 0.8304, 0.8520, 0.8970, 0.8736,\n",
      "        0.8414, 0.7892, 0.8245, 0.8396, 0.8649, 0.7499, 0.8315, 0.7823, 0.8751,\n",
      "        0.7950, 0.8506, 0.8596, 0.8398, 0.8522, 0.8790, 0.7818, 0.8682, 0.8193,\n",
      "        0.8768, 0.7926, 0.8768, 0.9270, 0.8686, 0.8125, 0.8756, 0.8695, 0.8302,\n",
      "        0.8992, 0.8179, 0.6337, 0.8627, 0.8705, 0.9479, 0.8082, 0.8865, 0.8142,\n",
      "        0.8899, 0.8872, 0.9221, 0.8657, 0.9245, 0.8903, 0.8326, 0.8639, 0.7905,\n",
      "        0.8835, 0.8553, 0.7531, 0.8894, 0.8985, 0.8819, 0.7673, 0.7294, 0.8140,\n",
      "        0.7866, 0.8018, 0.9243, 0.7718, 0.8411, 0.8732, 0.7769, 0.8354, 0.8183,\n",
      "        0.8826, 0.8527, 0.8261, 0.8701, 0.8735, 0.6823, 0.8464, 0.8214, 0.8574,\n",
      "        0.8139, 0.8448, 0.8107, 0.8711, 0.8859, 0.8486, 0.8124, 0.9055, 0.8704,\n",
      "        0.8226, 0.6548, 0.7979, 0.8874, 0.8145, 0.7870, 0.8673, 0.7866, 0.8187,\n",
      "        0.7935, 0.8874, 0.8211, 0.8397, 0.8894, 0.8631, 0.7687, 0.7802, 0.8299,\n",
      "        0.8466, 0.8492, 0.8821, 0.8843, 0.6359, 0.9250, 0.8460, 0.8857, 0.7897,\n",
      "        0.9135, 0.8448, 0.8255, 0.8548, 0.8158, 0.8644, 0.8687, 0.7091, 0.8403,\n",
      "        0.7895, 0.8971, 0.8605, 0.8955, 0.7915, 0.8699, 0.8484, 0.7965, 0.9194,\n",
      "        0.7932, 0.7071, 0.8655, 0.8113, 0.8930, 0.9162, 0.8680, 0.8283, 0.8483,\n",
      "        0.8693, 0.8951, 0.7679, 0.8522, 0.8959, 0.8939, 0.8475, 0.8574, 0.7872,\n",
      "        0.8984, 0.8715, 0.8784, 0.8214, 0.8416, 0.9117, 0.8196, 0.8597, 0.7963,\n",
      "        0.8704, 0.8504, 0.8727, 0.8340, 0.8597, 0.8509, 0.6890, 0.8435, 0.8476,\n",
      "        0.8708, 0.6328, 0.9185, 0.8723, 0.8200, 0.8756, 0.8918, 0.8217, 0.8699,\n",
      "        0.8485, 0.9967, 0.8811, 0.7856, 0.8631, 0.8897, 0.8498, 0.9262, 0.8388,\n",
      "        0.7901, 0.8331, 0.8688, 0.8569, 0.8537, 0.8365, 0.8182, 0.8942, 0.8866,\n",
      "        0.8463, 0.8869, 0.8394, 0.9231, 0.7984, 0.8740, 0.8726, 0.8353, 0.7919,\n",
      "        0.8872, 0.7689, 0.6218, 0.8400, 0.7966, 0.9079, 0.8769, 0.7981, 0.7941,\n",
      "        0.8575, 0.8995, 0.8624, 0.8585, 0.8805, 0.9133, 0.8333, 0.8086, 0.9206,\n",
      "        0.8325, 0.7797, 0.8181, 0.8854, 0.8561, 0.8234, 0.8734, 0.8858, 0.7785,\n",
      "        0.8895, 0.8619, 0.8108, 0.8599, 0.7322, 0.8980, 0.8275, 0.8324, 0.8470,\n",
      "        0.8063, 0.8376, 0.9021, 0.8721, 0.8379, 0.8141, 0.6811, 0.8273, 0.7728,\n",
      "        0.8397, 0.8688, 0.8014, 0.8723, 0.9012, 0.8971, 0.9158, 0.8628, 0.8225,\n",
      "        0.8963, 0.8268, 0.8531, 0.7969, 0.6276, 0.8526, 0.8314, 0.8010, 0.8807,\n",
      "        0.8678, 0.8879, 0.8922, 0.9053, 0.8772, 0.8971, 0.8988, 0.8562, 0.8525,\n",
      "        0.8877, 0.8507, 0.7839, 0.8583, 0.8941, 0.8664, 0.8485, 0.8495, 0.8426,\n",
      "        0.8950, 0.8668, 0.8842, 0.8485, 0.7265, 0.8237, 0.6463, 0.9183, 0.8831,\n",
      "        0.8201, 0.8409, 0.8950, 0.7923, 0.8142, 0.8448, 0.8879, 0.8663, 0.7791,\n",
      "        0.8568, 0.8015, 0.8253, 0.8048, 0.9013, 0.8313, 0.8361, 0.8892, 0.8230,\n",
      "        0.7587, 0.8609, 0.8843, 0.8487, 0.8526, 0.8886, 0.8225, 0.8283, 0.8455,\n",
      "        0.8859, 0.8674, 0.8982, 0.8950, 0.9144, 0.8502, 0.8186, 0.8886, 0.8253,\n",
      "        0.8982, 0.7530, 0.8619, 0.8681, 0.9076, 0.8465, 0.7898, 0.8664, 0.8131,\n",
      "        0.8316, 0.8657, 0.8760, 0.9139, 0.8908, 0.8030, 0.8493, 0.8637, 0.7874,\n",
      "        0.8309, 0.8933, 0.8474, 0.7458, 0.8771, 0.8393, 0.8172, 0.8739, 0.8716,\n",
      "        0.7530, 0.8105, 0.9045], requires_grad=True)\n",
      "intermediate output  tensor([[[-3.7603e-02, -1.3060e-01, -2.3905e-02,  ..., -5.0952e-02,\n",
      "          -6.2634e-02, -1.4820e-02],\n",
      "         [-1.5866e-01, -2.4475e-02, -1.6613e-01,  ..., -1.3515e-03,\n",
      "          -7.4088e-02, -1.3123e-01],\n",
      "         [-2.2732e-02, -4.5823e-03, -1.4927e-01,  ..., -2.2058e-03,\n",
      "          -1.4228e-01, -4.9609e-04],\n",
      "         ...,\n",
      "         [-1.5796e-01, -3.3156e-02,  8.5630e-01,  ..., -6.2689e-02,\n",
      "          -2.8837e-02, -7.6053e-02],\n",
      "         [-1.6401e-01, -3.9030e-02,  8.2154e-01,  ..., -5.6073e-02,\n",
      "          -1.9823e-02, -4.7399e-02],\n",
      "         [-1.6638e-01, -1.1535e-02,  8.4555e-01,  ..., -6.7024e-02,\n",
      "          -8.4104e-03, -5.3640e-02]]], grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.5173,  0.5457, -1.4603,  ..., -0.1739,  0.1156,  0.2521],\n",
      "         [-0.5127, -0.5614,  0.3318,  ...,  0.7016, -0.4298, -1.6294],\n",
      "         [-0.4164, -0.5546, -0.2031,  ..., -0.6549,  0.6181, -0.0542],\n",
      "         ...,\n",
      "         [-1.0400, -0.9742,  0.3995,  ...,  0.0785, -0.6036,  0.0591],\n",
      "         [-1.0173, -0.8893,  0.8362,  ...,  0.4261, -0.6401, -0.2722],\n",
      "         [-0.6841, -1.0132,  0.9382,  ...,  0.4669, -0.7166, -0.4316]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[ 0.2840,  0.4983, -0.3057,  ...,  0.1253,  0.5239,  0.2982],\n",
      "         [-0.0841, -0.1911, -0.2085,  ..., -0.3092,  0.2663, -0.1001],\n",
      "         [ 0.1120, -0.3601, -0.3009,  ..., -0.1793,  0.3527, -0.2388],\n",
      "         ...,\n",
      "         [ 0.0649,  0.1508, -0.0998,  ..., -0.1283,  0.5047,  0.0872],\n",
      "         [-0.0124,  0.0505, -0.0065,  ..., -0.1251,  0.3274,  0.0059],\n",
      "         [-0.0426, -0.0157, -0.0316,  ..., -0.1136,  0.1966, -0.0203]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.5173,  0.5457, -1.4603,  ..., -0.1739,  0.1156,  0.2521],\n",
      "         [-0.5127, -0.5614,  0.3318,  ...,  0.7016, -0.4298, -1.6294],\n",
      "         [-0.4164, -0.5546, -0.2031,  ..., -0.6549,  0.6181, -0.0542],\n",
      "         ...,\n",
      "         [-1.0400, -0.9742,  0.3995,  ...,  0.0785, -0.6036,  0.0591],\n",
      "         [-1.0173, -0.8893,  0.8362,  ...,  0.4261, -0.6401, -0.2722],\n",
      "         [-0.6841, -1.0132,  0.9382,  ...,  0.4669, -0.7166, -0.4316]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.2793,  1.1225, -1.4711,  ...,  0.1283,  0.5878,  0.6167],\n",
      "         [-0.6733, -0.7237,  0.3003,  ...,  0.5214, -0.1645, -1.7822],\n",
      "         [-0.3532, -0.8628, -0.2868,  ..., -0.5572,  0.8853, -0.2542],\n",
      "         ...,\n",
      "         [-1.1572, -0.8648,  0.4972,  ...,  0.1238, -0.1073,  0.2178],\n",
      "         [-1.2699, -0.9218,  1.0870,  ...,  0.4791, -0.3405, -0.2666],\n",
      "         [-0.9239, -1.1704,  1.1925,  ...,  0.5409, -0.5745, -0.4989]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.5173,  0.5457, -1.4603,  ..., -0.1739,  0.1156,  0.2521],\n",
      "         [-0.5127, -0.5614,  0.3318,  ...,  0.7016, -0.4298, -1.6294],\n",
      "         [-0.4164, -0.5546, -0.2031,  ..., -0.6549,  0.6181, -0.0542],\n",
      "         ...,\n",
      "         [-1.0400, -0.9742,  0.3995,  ...,  0.0785, -0.6036,  0.0591],\n",
      "         [-1.0173, -0.8893,  0.8362,  ...,  0.4261, -0.6401, -0.2722],\n",
      "         [-0.6841, -1.0132,  0.9382,  ...,  0.4669, -0.7166, -0.4316]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-6.7936e-02,  3.1578e-02,  1.5648e-01, -1.5025e-01,  1.4727e-01,\n",
      "        -4.2515e-02,  1.5221e-01,  4.7048e-02, -6.1620e-02, -1.6018e-02,\n",
      "        -6.4231e-02, -5.0208e-03, -1.3631e-01,  1.7899e-01,  2.5055e-02,\n",
      "         2.0497e-01,  5.9091e-02,  8.2099e-02, -3.8929e-02,  7.4394e-02,\n",
      "         1.7318e-01, -1.0463e-01,  1.5208e-01,  2.9461e-01,  8.0460e-02,\n",
      "         1.6074e-01,  9.9356e-02, -2.2881e-02, -2.4802e-01,  1.3612e-01,\n",
      "        -1.8634e-02, -6.6393e-02,  1.7255e-01, -1.8609e-01,  1.2682e-01,\n",
      "        -4.6187e-02, -1.2697e-01, -5.9533e-02,  1.4078e-01, -3.0814e-01,\n",
      "        -5.4271e-02, -1.5952e-01, -6.1325e-02, -5.5490e-03, -2.0443e-02,\n",
      "        -1.0424e-01, -2.1095e-02, -2.4339e-02, -9.2003e-02, -1.2576e-01,\n",
      "        -7.5278e-02, -1.2179e-01,  2.6206e-01, -1.4358e-01,  1.1244e-01,\n",
      "         9.6011e-02,  9.4407e-02, -2.1980e-01, -1.6647e-01, -5.6020e-02,\n",
      "        -4.1293e-02,  1.1602e-01, -6.2996e-02,  1.0435e-02, -5.0696e-02,\n",
      "         1.6687e-01, -6.8411e-02,  5.9983e-02,  8.5984e-03, -1.9892e-01,\n",
      "        -1.2073e-01,  7.9578e-02, -5.6859e-02, -9.8340e-02,  7.1650e-02,\n",
      "         6.4307e-02, -7.3766e-02, -2.0682e-02,  1.4140e-01,  5.0717e-02,\n",
      "        -1.3890e-02, -7.9260e-03,  4.3695e-02,  1.2548e-01,  6.4592e-03,\n",
      "        -1.1162e-01, -7.2433e-02, -2.2999e-02, -1.1168e-01,  1.7434e-02,\n",
      "        -6.8272e-02, -1.8966e-01,  6.3337e-01,  8.7435e-02,  1.9547e-01,\n",
      "        -1.4045e-01,  2.1174e-01, -1.1560e-01, -1.3142e-01, -8.7810e-02,\n",
      "        -1.0087e-01,  1.8780e-02, -2.7278e-02, -4.2809e-02, -2.3384e-04,\n",
      "        -1.1848e-01, -1.2392e-01, -9.4572e-02, -2.1064e-01,  2.8821e-02,\n",
      "        -1.4498e-01, -8.4430e-02, -1.9311e-01, -1.2905e-01, -3.6538e-02,\n",
      "        -1.4816e-02,  4.5727e-02, -1.5212e-02, -1.3613e-01,  1.3006e-01,\n",
      "        -9.8392e-02, -1.0147e-01, -5.8493e-02,  7.8605e-02,  2.4887e-01,\n",
      "        -1.3633e-01, -1.2866e-01,  1.7872e-01, -1.1006e-01, -6.0457e-02,\n",
      "        -1.1736e-02,  1.4017e-01, -1.1579e-01, -8.6049e-02, -3.8850e-02,\n",
      "        -5.8156e-02,  1.2445e-01,  1.0673e-01, -2.1039e-01, -7.8962e-02,\n",
      "        -1.1990e-01,  3.2712e-02,  3.5324e-02,  8.5162e-02,  1.0228e-01,\n",
      "        -4.9327e-01, -4.7141e-02,  4.2122e-02, -1.6448e-01,  2.2386e-01,\n",
      "        -7.3167e-03,  8.4590e-02, -2.4692e-01,  6.0218e-02,  8.4576e-02,\n",
      "         2.8577e-02,  3.4687e-02,  7.5134e-03,  7.8150e-03, -2.5552e-01,\n",
      "         4.7889e-02, -1.0869e-01, -2.5363e-02,  6.5480e-02, -7.2423e-02,\n",
      "         8.0864e-02,  7.3754e-02,  9.1629e-02, -3.7840e-01, -4.9376e-02,\n",
      "        -7.1923e-02, -3.3675e-02, -8.9839e-02,  7.2827e-02, -9.7517e-02,\n",
      "         2.1261e-01,  6.7663e-02,  4.8916e-02,  1.1473e-01, -6.8217e-02,\n",
      "         2.0152e-02,  9.5584e-02, -1.6020e-01,  1.0648e-01,  9.8983e-03,\n",
      "        -1.1244e-01,  9.1176e-02,  1.6547e-01,  1.3610e-01,  4.4776e-03,\n",
      "        -5.8887e-02, -3.7258e-01,  2.8920e-01, -1.0734e-01,  1.7613e-01,\n",
      "         4.0804e-02, -2.9469e-02,  1.7464e-01,  4.4790e-02, -4.3585e-02,\n",
      "        -1.1599e-01,  3.0010e-02,  1.0212e-01,  2.9072e-02, -1.6182e-01,\n",
      "         9.9715e-02,  2.6969e-02, -6.5524e-02,  2.0609e-02,  1.0495e-01,\n",
      "        -1.6100e-01, -1.0561e-01,  1.7956e-01,  1.6084e-01,  9.2078e-03,\n",
      "        -1.8615e-02, -1.9930e-01, -7.4069e-02,  9.5005e-02, -1.6319e-02,\n",
      "         7.1734e-02,  2.3848e-01, -9.0983e-02,  2.4761e-02, -1.7546e-01,\n",
      "         3.3100e-01,  1.6409e-01, -6.4080e-02,  4.7998e-02, -1.0464e-01,\n",
      "        -4.3132e-02, -1.9657e-02,  5.0045e-02,  4.1152e-02, -3.1279e-02,\n",
      "         1.6035e-02, -7.1043e-03, -1.8470e-01,  1.1808e-01, -2.7260e-01,\n",
      "         6.4889e-02,  2.9104e-02, -1.3412e-01,  3.0322e-03, -4.0973e-02,\n",
      "        -5.5794e-02, -9.2068e-02,  6.3257e-02, -2.2327e-01, -2.3490e-01,\n",
      "        -2.9395e-02,  4.3944e-01, -1.0098e-02, -1.9856e-02,  7.2339e-02,\n",
      "        -1.2023e-01, -2.9103e-01, -7.4717e-02,  1.9599e-02, -1.1578e-02,\n",
      "        -6.2657e-02, -4.8798e-02, -1.1575e-01,  3.7147e-02, -4.8477e-02,\n",
      "        -2.6367e-01,  7.7258e-03,  3.1153e-02,  6.0121e-03, -1.3257e-01,\n",
      "         3.8915e-03, -5.7706e-02, -1.5804e-01,  6.2585e-03, -7.7101e-02,\n",
      "        -5.1661e-02, -1.0924e-01, -1.4369e-01, -1.1446e-01, -8.7998e-02,\n",
      "         1.1470e-01,  2.9228e-02, -9.8710e-02, -1.5051e-01, -1.0675e-01,\n",
      "         1.3892e-01,  7.4824e-02,  3.3203e-02, -3.8883e-03,  2.2480e-02,\n",
      "         9.8746e-02,  4.8845e-03, -1.7025e-01, -9.1816e-02,  1.6679e-01,\n",
      "         9.7284e-02,  6.8027e-02,  2.2613e-01,  1.4146e-01, -6.3099e-02,\n",
      "         9.9177e-02,  7.1848e-02, -8.9442e-02,  1.0882e-01,  3.9499e-01,\n",
      "        -2.1378e-01, -8.6686e-02, -1.1241e-01, -9.1226e-01, -6.0548e-02,\n",
      "        -1.3436e-01, -1.4847e-01,  2.6446e-02,  6.9003e-03, -4.0872e-02,\n",
      "         6.6189e-02,  3.7712e-02,  4.2333e-02, -1.9464e-01,  3.7394e-02,\n",
      "        -2.0832e-01,  1.2962e-01,  2.6202e-01, -2.1355e-01, -3.4018e-02,\n",
      "         1.0503e-02, -7.0151e-02, -1.5101e-01, -5.4392e-02, -1.5883e-01,\n",
      "        -1.2751e-02,  1.2471e-01,  1.5871e-02,  1.4900e-01, -1.8499e-01,\n",
      "        -1.1662e-02, -1.0970e-01, -9.6448e-02, -5.1222e-02, -1.8194e-01,\n",
      "         3.5967e-03,  5.7948e-02,  3.1753e-02,  4.7772e-02,  9.8228e-02,\n",
      "        -1.6950e-02, -3.8783e-03, -1.9597e-01, -5.0622e-02, -2.1524e-01,\n",
      "        -2.7248e-01, -7.4193e-02,  4.7427e-02,  8.7588e-02, -7.8666e-02,\n",
      "        -1.1951e-01,  3.0464e-02,  6.7082e-02, -8.5915e-02,  1.2746e-02,\n",
      "         5.4255e-02,  3.1415e-02,  1.7812e-02, -1.0409e-02,  1.1163e-02,\n",
      "        -3.4429e-02, -8.5781e-02,  1.2418e-01, -6.7287e-02,  3.8630e-02,\n",
      "        -9.4615e-03, -7.6570e-02,  7.7781e-03, -1.1548e-01, -5.4246e-02,\n",
      "         5.2068e-03,  5.1190e-02, -1.6957e-01, -1.7396e-01,  3.6970e-02,\n",
      "        -2.4187e-02, -2.0029e+00, -1.6407e-02, -1.4806e-01,  4.4287e-02,\n",
      "        -1.2524e-01, -2.5100e-01,  6.7725e-02, -4.2504e-02, -8.3310e-02,\n",
      "         1.2121e-01,  1.1843e-01,  1.1215e-01,  1.8345e-03,  7.0474e-02,\n",
      "         3.0512e-03, -2.2673e-01, -1.7632e-01,  8.5775e-02, -1.7479e-01,\n",
      "        -5.9883e-02, -9.3512e-02, -6.6348e-03,  1.6602e-02, -3.7331e-02,\n",
      "        -1.5731e-01, -4.0634e-03,  2.1437e-02,  1.9360e-01, -1.7529e-01,\n",
      "        -3.3552e-02, -3.4553e-02,  3.9529e-02, -5.6082e-02, -1.5954e-01,\n",
      "         9.0531e-02, -5.4529e-02, -8.4778e-02, -3.7641e-02, -3.2706e-02,\n",
      "        -7.4008e-02, -2.4567e-01, -3.3352e-04, -1.0277e-01, -1.9596e-01,\n",
      "         2.0938e-02,  2.5293e-03, -2.6667e-01, -1.8043e-01,  1.5065e-01,\n",
      "         1.2584e-02, -1.7929e-01, -2.3455e-01,  1.6269e-01, -1.7313e-01,\n",
      "         1.5415e-02, -2.3525e-01, -3.3482e-03, -7.4471e-02, -4.5411e-02,\n",
      "         2.7673e-02,  4.9929e-02,  1.7176e-01, -1.6694e-02, -2.4801e-02,\n",
      "         7.3893e-02, -2.4601e-01,  1.5642e-02, -8.7047e-02, -1.8164e-01,\n",
      "        -1.1513e-02, -4.8777e-02,  3.3416e-02, -8.9597e-02,  6.5705e-02,\n",
      "         1.0950e-01, -1.7611e-01, -7.0539e-02, -1.7197e-01, -1.0761e-01,\n",
      "         6.4050e-03, -3.4898e-01, -8.8903e-02,  7.7862e-02,  5.3062e-02,\n",
      "        -1.2054e-01,  5.5860e-02, -9.5683e-02,  8.8151e-02, -5.5596e-02,\n",
      "        -1.0295e-02, -1.0288e-01, -2.8387e-03,  1.1371e-02, -7.9308e-02,\n",
      "        -1.2272e-01, -1.3956e-01, -3.4266e-02, -1.7620e-01, -2.7926e-01,\n",
      "         2.8323e-02,  1.7076e-01, -1.0466e-01, -2.1172e-02, -6.9358e-02,\n",
      "        -1.4097e-02,  1.6239e-01,  2.1466e-02, -1.7813e-01, -9.3061e-02,\n",
      "        -2.5623e-02, -1.1394e-01,  2.8787e-02, -1.6209e-02, -4.7511e-02,\n",
      "        -1.4398e-02, -1.4870e-02, -8.2321e-02, -7.8860e-04,  4.8956e-02,\n",
      "        -3.0010e-02,  8.6538e-02, -6.4840e-02, -1.7205e-01, -4.1212e-02,\n",
      "         1.9081e-01, -6.0681e-02, -9.5607e-03, -1.3612e-02,  2.2754e-02,\n",
      "        -1.2482e-01, -4.8257e-02, -2.5563e-03, -5.4207e-02, -8.7969e-02,\n",
      "         2.5370e-01, -8.3992e-02,  2.2473e-02, -2.8809e-01,  4.6314e-02,\n",
      "        -1.8008e-01,  1.9084e-01,  1.1078e-01, -2.0788e-01,  2.0804e-02,\n",
      "        -2.3081e-02,  4.3814e-03, -1.8993e-01,  9.6563e-03,  7.8980e-02,\n",
      "        -7.3118e-02, -1.6445e-01, -1.1687e-01,  3.1638e-02,  1.3645e-01,\n",
      "        -3.6535e-02, -8.1213e-02,  1.1619e-01,  3.2536e-02, -2.0412e-01,\n",
      "         1.1926e-01, -5.3010e-02, -1.1263e-01, -3.1319e-04, -9.0229e-02,\n",
      "        -3.1579e-01, -2.0241e-01,  5.4120e-02,  2.7079e-01,  8.5984e-02,\n",
      "        -1.0727e-01, -8.8298e-02,  1.1348e-01,  5.0311e-02, -7.7398e-02,\n",
      "        -1.2515e-01,  1.1163e-02,  1.6500e-01,  3.9051e-02, -3.2798e-02,\n",
      "        -7.8474e-02, -6.2953e-02, -1.1776e-01, -3.1446e-02, -1.9581e-01,\n",
      "        -7.0297e-02,  3.3361e-02, -8.2373e-02, -5.6821e-02, -1.0619e-01,\n",
      "        -8.0503e-02,  6.5672e-02,  1.0622e-01, -1.5506e-03, -2.2421e-01,\n",
      "         2.0279e-02, -2.0314e-02,  3.7313e-02,  6.9882e-02, -2.0116e-01,\n",
      "        -1.2390e-01,  1.7524e-01,  1.0243e-01, -1.4295e-01, -7.6353e-02,\n",
      "        -1.1770e-01,  4.1140e-01, -1.1487e-01, -5.4943e-02,  2.4503e-02,\n",
      "        -2.5039e-02, -9.9587e-03,  1.3072e-01,  4.5926e-02,  6.9544e-02,\n",
      "        -7.6750e-02,  9.1954e-03,  5.9608e-02, -3.2196e-02,  9.0296e-02,\n",
      "         5.4665e-02,  4.9343e-03, -1.0184e-01, -3.1093e-02,  1.8238e-02,\n",
      "         9.9516e-02,  5.0099e-02,  7.2726e-02,  7.4661e-02,  5.8317e-02,\n",
      "         4.7681e-02, -2.1888e-01, -1.2824e-01, -9.2001e-03, -1.6924e-01,\n",
      "        -6.6007e-02, -1.3145e-01,  4.5609e-03, -1.6963e-02, -1.3881e-01,\n",
      "         1.0726e-01,  2.6847e-02,  3.4012e-02,  4.9686e-02, -1.8051e-02,\n",
      "         8.1117e-02,  7.6936e-02, -1.9907e-01,  6.5518e-02,  3.7947e-02,\n",
      "         1.7338e-01,  1.2515e-01,  9.0665e-02,  1.9960e-01, -4.3784e-02,\n",
      "         5.2904e-02,  2.0929e-02, -1.6854e-02,  1.5526e-01,  4.2797e-02,\n",
      "        -1.6666e-01,  8.8338e-02,  3.4787e-02, -1.3183e-01,  6.8483e-03,\n",
      "        -1.1000e-01,  2.1009e-03,  6.8067e-02, -8.9118e-02,  1.4793e-02,\n",
      "        -1.6388e-04,  1.5776e-01,  1.5675e-01,  6.6618e-02, -7.9542e-02,\n",
      "        -3.3404e-02,  7.5338e-02, -5.6420e-04, -3.5606e-02, -9.3587e-02,\n",
      "         9.2658e-02, -9.2517e-02,  7.0557e-02, -5.0336e-02,  2.2185e-02,\n",
      "        -1.4681e-01,  8.1938e-02,  1.2301e-01, -9.9737e-04,  1.2787e-01,\n",
      "         1.2926e-01,  7.8904e-03, -1.3817e-01, -1.2626e-01,  7.1758e-02,\n",
      "         1.2637e-02,  1.4702e-01,  1.2193e-01,  5.9977e-02, -8.4220e-02,\n",
      "        -6.7759e-02,  9.1227e-03,  1.9597e-01, -4.7183e-02, -1.7960e-01,\n",
      "        -1.3921e-02,  1.3795e-01,  9.7247e-02, -1.0703e-01, -7.5525e-03,\n",
      "        -1.1469e-01,  7.2541e-02,  3.9003e-02,  6.0241e-02, -1.7668e-02,\n",
      "         2.8879e-02, -1.7712e-01,  5.5653e-02, -9.0197e-02,  7.4972e-03,\n",
      "         7.0543e-02, -7.4826e-02, -1.9866e-01,  1.0185e-02,  1.5340e-01,\n",
      "         1.0733e-01,  9.5544e-02,  2.8146e-02,  5.1150e-02,  7.7695e-02,\n",
      "        -1.9368e-03, -2.9277e-02,  2.9241e-02,  7.2927e-04, -6.6681e-02,\n",
      "         9.3101e-02, -4.8289e-02,  2.2985e-03, -5.3752e-02,  4.0573e-02,\n",
      "        -5.7318e-02, -8.1010e-03, -6.3683e-02, -7.2191e-02,  9.4628e-02,\n",
      "        -5.2318e-02, -1.1188e-02, -1.4373e-01, -1.0095e-01,  9.2550e-02,\n",
      "         1.4976e-02,  1.3774e-01,  3.3899e-02,  1.1200e-01,  2.6986e-01,\n",
      "         3.4651e-03, -7.7195e-03,  5.8274e-02, -2.8220e-02, -2.8953e-02,\n",
      "        -1.3285e-02, -6.1327e-02, -8.6871e-02, -8.7978e-02,  2.3760e-02,\n",
      "         3.0763e-02,  4.1973e-02, -5.4904e-02,  4.5272e-02,  6.6180e-02,\n",
      "        -6.3239e-02, -1.8968e-01, -2.3140e-02,  1.1470e-01, -2.4152e-01,\n",
      "        -1.0144e-02, -1.4280e-01,  1.4466e-01,  1.6719e-02,  3.3186e-02,\n",
      "         6.6674e-02,  7.5298e-02, -3.1020e-02, -2.2641e-01, -1.7237e-01,\n",
      "         1.4496e-01, -3.4159e-02,  1.8037e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8882, 0.8705, 0.8029, 0.7737, 0.9233, 0.8613, 0.8651, 0.8595, 0.8431,\n",
      "        0.8290, 0.7691, 0.7188, 0.7791, 0.7648, 0.8316, 0.8349, 0.7515, 0.7844,\n",
      "        0.9252, 0.8440, 0.8759, 0.8030, 0.8607, 0.9052, 0.7658, 0.9120, 0.7668,\n",
      "        0.8896, 0.7807, 0.8382, 0.8737, 0.7606, 0.8819, 0.8739, 0.8686, 0.8333,\n",
      "        0.8616, 0.8090, 0.8758, 0.9091, 0.8491, 0.8887, 0.8879, 0.8460, 0.8262,\n",
      "        0.7833, 0.9008, 0.8638, 0.8917, 0.8618, 0.8790, 0.8376, 0.9470, 0.7400,\n",
      "        0.8520, 0.7660, 0.8255, 0.7111, 0.8391, 0.7484, 0.8679, 0.7583, 0.6782,\n",
      "        0.7847, 0.8373, 0.8244, 0.8480, 0.8636, 0.8056, 0.9040, 0.8738, 0.9071,\n",
      "        0.8632, 0.7288, 0.8821, 0.7437, 0.8235, 0.7086, 0.7610, 0.7965, 0.8479,\n",
      "        0.8865, 0.8985, 0.8220, 0.6690, 0.7675, 0.8065, 0.8193, 0.8474, 0.8783,\n",
      "        0.8065, 0.8388, 1.0387, 0.7745, 0.8001, 0.7706, 0.6955, 0.7660, 0.8370,\n",
      "        0.8755, 0.9011, 0.9001, 0.8662, 0.8175, 0.7603, 0.6286, 0.7920, 0.8799,\n",
      "        0.8429, 0.7867, 0.8382, 0.8181, 0.7970, 0.8636, 0.8415, 0.7793, 0.8194,\n",
      "        0.8602, 0.8445, 0.8133, 0.8415, 0.7265, 0.7852, 0.7824, 0.7036, 0.7916,\n",
      "        0.8440, 0.7817, 0.8299, 0.7891, 0.8667, 0.8006, 0.7027, 0.8540, 0.8323,\n",
      "        0.8769, 0.8234, 0.7644, 0.8505, 0.6803, 0.8968, 0.8783, 0.8830, 0.9116,\n",
      "        0.7893, 0.7780, 0.8531, 0.8193, 0.8623, 0.8335, 0.7775, 0.8203, 0.7129,\n",
      "        0.8810, 0.8711, 0.7750, 0.8264, 0.7797, 0.8539, 0.6880, 0.8551, 0.8790,\n",
      "        0.7656, 0.8994, 0.7828, 0.8867, 0.8332, 0.7469, 0.8200, 0.8579, 0.8550,\n",
      "        0.8288, 0.7634, 0.8618, 0.8990, 0.7835, 0.7695, 0.7812, 0.7895, 0.8122,\n",
      "        0.7978, 0.8957, 0.8300, 0.8824, 0.8794, 0.8354, 0.9273, 0.8315, 0.8805,\n",
      "        0.8113, 0.8516, 0.9687, 0.9233, 0.7877, 0.8459, 0.8537, 0.8607, 0.7979,\n",
      "        0.8205, 0.7391, 0.8640, 0.8108, 0.6632, 0.7625, 0.7884, 0.8288, 0.8209,\n",
      "        0.7770, 0.8608, 0.8037, 0.7650, 0.8453, 0.8197, 0.8395, 0.7984, 0.6391,\n",
      "        0.6980, 0.8933, 0.7109, 0.8055, 0.8497, 0.8814, 0.8470, 0.7568, 0.8430,\n",
      "        0.6954, 0.7977, 0.7798, 0.8543, 0.8833, 0.8033, 0.8467, 0.8251, 0.8670,\n",
      "        0.8246, 0.8112, 0.8279, 0.7682, 0.8681, 0.7707, 0.7741, 0.8295, 0.8195,\n",
      "        0.7521, 0.7894, 0.6608, 0.7607, 0.8185, 0.7835, 0.7484, 0.7874, 0.9625,\n",
      "        0.8093, 0.8241, 0.8755, 0.8304, 0.7966, 0.7980, 0.7531, 0.7918, 0.8844,\n",
      "        0.8150, 0.8587, 0.8082, 0.7785, 0.7825, 0.8141, 0.8227, 0.7814, 0.8896,\n",
      "        0.8154, 0.8465, 0.8383, 0.8018, 0.8816, 0.8644, 0.7010, 0.8349, 0.7994,\n",
      "        0.8619, 0.7419, 0.8734, 0.8767, 0.7105, 0.8503, 0.7739, 0.8835, 0.9097,\n",
      "        0.8079, 0.8755, 0.7999, 0.7319, 0.8741, 0.8243, 0.8519, 0.7224, 0.7170,\n",
      "        0.8942, 0.8630, 0.8501, 0.9165, 0.8561, 0.8317, 0.8554, 1.0119, 0.8167,\n",
      "        0.7876, 0.9025, 1.2222, 0.8446, 0.7796, 0.6850, 0.7692, 0.8237, 0.8505,\n",
      "        0.7923, 0.8813, 0.8592, 0.8471, 0.6437, 0.8395, 0.8182, 0.8216, 0.8648,\n",
      "        0.8441, 0.8527, 0.7438, 0.8334, 0.8561, 0.8394, 0.7437, 0.6512, 0.8207,\n",
      "        0.9215, 0.9155, 0.8162, 0.8244, 0.8885, 0.8679, 0.8340, 0.7514, 0.9164,\n",
      "        0.7275, 0.8341, 0.7449, 0.6736, 0.8710, 0.8561, 0.8202, 0.8938, 0.8457,\n",
      "        0.8643, 0.8617, 0.6730, 0.8543, 0.8496, 0.8068, 0.7789, 0.8104, 0.8218,\n",
      "        0.8504, 0.7812, 0.7815, 0.7983, 0.8272, 0.8571, 0.7634, 0.8555, 0.8495,\n",
      "        0.7804, 0.8402, 0.8359, 0.8862, 0.8377, 0.9037, 0.8772, 0.8472, 0.8217,\n",
      "        0.8099, 0.9047, 0.8489, 3.3445, 0.8512, 0.9063, 0.8134, 0.8351, 0.8672,\n",
      "        0.8851, 0.8455, 0.8568, 0.7831, 0.8216, 0.8503, 0.7740, 0.8707, 0.8748,\n",
      "        0.8655, 0.7891, 0.8419, 0.8790, 0.7904, 0.7256, 0.7347, 0.8237, 0.7436,\n",
      "        0.8020, 0.8723, 0.7900, 0.8527, 0.8650, 0.8323, 0.8423, 0.8807, 0.8627,\n",
      "        0.7978, 0.7756, 0.8189, 0.8115, 0.8358, 0.7310, 0.8210, 0.7347, 0.8545,\n",
      "        0.7896, 0.8247, 0.8317, 0.8238, 0.8387, 0.8517, 0.7923, 0.8158, 0.7999,\n",
      "        0.8641, 0.7756, 0.8680, 0.9047, 0.8405, 0.7877, 0.8798, 0.8426, 0.8045,\n",
      "        0.8671, 0.7909, 0.6701, 0.8234, 0.8594, 0.9320, 0.7794, 0.8440, 0.8069,\n",
      "        0.8695, 0.8802, 0.9099, 0.8205, 0.9157, 0.8940, 0.8149, 0.8499, 0.7898,\n",
      "        0.8699, 0.8149, 0.7534, 0.8605, 0.8858, 0.8554, 0.7511, 0.7481, 0.7842,\n",
      "        0.7665, 0.7684, 0.9338, 0.7248, 0.8127, 0.8240, 0.7484, 0.8201, 0.7972,\n",
      "        0.8627, 0.8236, 0.8201, 0.8485, 0.8531, 0.6803, 0.7943, 0.8345, 0.8308,\n",
      "        0.8235, 0.8371, 0.7801, 0.8551, 0.8783, 0.8388, 0.7973, 0.8819, 0.8521,\n",
      "        0.7975, 0.6687, 0.7696, 0.8772, 0.7991, 0.7594, 0.8550, 0.7472, 0.7861,\n",
      "        0.7670, 0.8773, 0.7959, 0.8113, 0.8619, 0.8411, 0.7269, 0.7533, 0.7981,\n",
      "        0.8265, 0.8323, 0.8984, 0.8578, 0.6245, 0.9083, 0.8059, 0.8872, 0.7735,\n",
      "        0.8857, 0.8253, 0.8111, 0.8284, 0.7953, 0.8582, 0.8415, 0.7058, 0.8140,\n",
      "        0.8058, 0.8630, 0.8194, 0.8705, 0.7825, 0.8360, 0.8108, 0.7841, 0.9037,\n",
      "        0.7749, 0.7092, 0.8483, 0.8062, 0.8705, 0.9177, 0.8554, 0.8023, 0.8533,\n",
      "        0.8329, 0.8860, 0.7780, 0.8538, 0.8952, 0.8878, 0.8408, 0.8242, 0.7670,\n",
      "        0.8658, 0.8434, 0.8529, 0.8149, 0.8361, 0.8928, 0.8307, 0.8312, 0.7437,\n",
      "        0.8623, 0.8484, 0.8765, 0.8006, 0.8790, 0.8308, 0.6945, 0.8171, 0.8305,\n",
      "        0.8538, 0.6569, 0.8860, 0.8519, 0.7863, 0.8689, 0.8617, 0.7771, 0.8457,\n",
      "        0.8339, 0.9770, 0.8790, 0.7564, 0.8662, 0.8757, 0.8467, 0.9003, 0.7980,\n",
      "        0.7872, 0.7997, 0.8401, 0.8416, 0.8507, 0.8467, 0.7880, 0.8952, 0.8861,\n",
      "        0.8215, 0.8750, 0.8089, 0.9032, 0.7438, 0.8452, 0.8476, 0.8160, 0.7681,\n",
      "        0.8636, 0.7271, 0.6455, 0.8214, 0.7902, 0.9005, 0.8469, 0.7760, 0.7839,\n",
      "        0.8214, 0.8759, 0.8188, 0.8409, 0.8463, 0.8653, 0.8123, 0.7826, 0.8826,\n",
      "        0.8169, 0.7693, 0.7947, 0.8671, 0.8443, 0.7886, 0.8498, 0.8654, 0.7643,\n",
      "        0.8580, 0.8364, 0.7808, 0.8315, 0.7153, 0.8904, 0.8265, 0.8260, 0.8425,\n",
      "        0.8090, 0.8182, 0.8919, 0.8530, 0.8225, 0.7834, 0.6588, 0.8072, 0.7863,\n",
      "        0.8275, 0.8606, 0.7992, 0.8501, 0.8932, 0.8908, 0.8999, 0.8363, 0.7745,\n",
      "        0.8924, 0.8136, 0.8288, 0.7885, 0.6333, 0.8526, 0.8107, 0.7834, 0.8649,\n",
      "        0.8312, 0.8468, 0.8530, 0.8971, 0.8643, 0.8762, 0.8892, 0.8374, 0.8172,\n",
      "        0.8710, 0.8545, 0.7402, 0.8328, 0.8749, 0.8435, 0.8192, 0.8252, 0.7987,\n",
      "        0.8700, 0.8562, 0.8601, 0.8172, 0.7134, 0.7949, 0.6630, 0.9130, 0.8745,\n",
      "        0.7872, 0.8019, 0.8693, 0.7607, 0.7836, 0.8066, 0.8745, 0.8564, 0.7235,\n",
      "        0.8337, 0.8020, 0.7976, 0.7865, 0.8942, 0.8157, 0.8139, 0.8792, 0.7882,\n",
      "        0.7006, 0.8483, 0.8653, 0.8163, 0.8547, 0.8718, 0.8091, 0.8107, 0.8151,\n",
      "        0.8541, 0.8282, 0.8909, 0.8638, 0.8861, 0.8236, 0.7769, 0.8727, 0.8008,\n",
      "        0.8849, 0.7288, 0.8408, 0.8556, 0.8806, 0.8329, 0.7735, 0.8462, 0.8047,\n",
      "        0.8172, 0.8413, 0.8595, 0.9027, 0.8736, 0.7631, 0.8301, 0.8499, 0.7712,\n",
      "        0.8395, 0.8815, 0.8135, 0.7105, 0.8673, 0.8161, 0.8203, 0.8548, 0.8062,\n",
      "        0.7516, 0.7964, 0.8844], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0942,  0.2515, -0.1697,  ..., -0.0719, -0.0141, -0.0004],\n",
      "         [-0.0189, -0.0627, -0.1553,  ..., -0.0823,  0.2726, -0.0018],\n",
      "         [-0.1579, -0.1695, -0.1659,  ..., -0.0143, -0.0885, -0.0210],\n",
      "         ...,\n",
      "         [-0.1633, -0.0866, -0.1003,  ..., -0.1088, -0.0712, -0.0039],\n",
      "         [-0.1205, -0.0706, -0.1335,  ..., -0.1264, -0.1044, -0.0077],\n",
      "         [-0.0931, -0.0815, -0.1432,  ..., -0.1220, -0.0425, -0.0015]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.0829,  0.7191, -1.3990,  ..., -0.5157,  0.0309,  0.9185],\n",
      "         [-0.9727, -0.6233, -0.0820,  ...,  0.3272, -0.0810, -1.5465],\n",
      "         [-0.1557, -0.7989, -0.1984,  ..., -0.5445,  0.3464, -0.3426],\n",
      "         ...,\n",
      "         [-0.8498, -0.9032, -0.3419,  ..., -0.0404,  0.0497,  0.5605],\n",
      "         [-1.0008, -0.8326,  0.6643,  ...,  0.4020, -0.2111,  0.0095],\n",
      "         [-0.6988, -0.9785,  0.8207,  ...,  0.5492, -0.4265, -0.2492]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[ 0.2230,  0.0950, -0.0947,  ...,  0.0173,  0.0478,  0.0131],\n",
      "         [ 0.1433, -0.1560, -0.1576,  ..., -0.2613,  0.1446, -0.0446],\n",
      "         [ 0.2114, -0.1792, -0.1160,  ..., -0.1338,  0.5526, -0.0741],\n",
      "         ...,\n",
      "         [ 0.1707,  0.0485,  0.1718,  ...,  0.0184,  0.0877,  0.0099],\n",
      "         [ 0.1263,  0.0316,  0.1486,  ..., -0.0727,  0.0669, -0.0497],\n",
      "         [ 0.1075,  0.0222,  0.1814,  ..., -0.0607,  0.0511, -0.0490]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.0829,  0.7191, -1.3990,  ..., -0.5157,  0.0309,  0.9185],\n",
      "         [-0.9727, -0.6233, -0.0820,  ...,  0.3272, -0.0810, -1.5465],\n",
      "         [-0.1557, -0.7989, -0.1984,  ..., -0.5445,  0.3464, -0.3426],\n",
      "         ...,\n",
      "         [-0.8498, -0.9032, -0.3419,  ..., -0.0404,  0.0497,  0.5605],\n",
      "         [-1.0008, -0.8326,  0.6643,  ...,  0.4020, -0.2111,  0.0095],\n",
      "         [-0.6988, -0.9785,  0.8207,  ...,  0.5492, -0.4265, -0.2492]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[ 0.1666,  1.0384, -1.5302,  ..., -0.3887,  0.1353,  1.1274],\n",
      "         [-0.9148, -0.7837, -0.1576,  ...,  0.1940,  0.1135, -1.7486],\n",
      "         [ 0.0534, -0.9510, -0.2197,  ..., -0.4932,  0.8985, -0.4296],\n",
      "         ...,\n",
      "         [-0.8003, -0.9307, -0.0990,  ...,  0.1089,  0.1979,  0.6881],\n",
      "         [-1.0310, -0.8679,  0.9526,  ...,  0.4726, -0.1021, -0.0333],\n",
      "         [-0.7051, -1.0648,  1.1709,  ...,  0.6451, -0.3539, -0.3429]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.0829,  0.7191, -1.3990,  ..., -0.5157,  0.0309,  0.9185],\n",
      "         [-0.9727, -0.6233, -0.0820,  ...,  0.3272, -0.0810, -1.5465],\n",
      "         [-0.1557, -0.7989, -0.1984,  ..., -0.5445,  0.3464, -0.3426],\n",
      "         ...,\n",
      "         [-0.8498, -0.9032, -0.3419,  ..., -0.0404,  0.0497,  0.5605],\n",
      "         [-1.0008, -0.8326,  0.6643,  ...,  0.4020, -0.2111,  0.0095],\n",
      "         [-0.6988, -0.9785,  0.8207,  ...,  0.5492, -0.4265, -0.2492]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-2.8268e-02,  4.4276e-02,  5.6783e-02, -4.7591e-02,  1.6136e-01,\n",
      "        -2.6716e-02,  1.7337e-01,  9.8786e-02, -1.6353e-02, -9.7341e-02,\n",
      "         4.8042e-02,  7.7096e-02, -1.6100e-01,  2.1559e-02,  1.0660e-03,\n",
      "         2.9267e-01,  1.5741e-01,  1.1006e-01, -8.4956e-02,  1.6614e-02,\n",
      "         1.8766e-01, -1.2081e-01,  6.8637e-03,  2.7939e-01,  1.1396e-01,\n",
      "         1.0982e-01, -5.4954e-03, -5.1910e-02, -2.8676e-01,  1.2391e-01,\n",
      "        -4.1713e-02, -5.9358e-02,  1.8512e-01, -3.6515e-02,  2.4761e-02,\n",
      "         8.1520e-02, -1.7233e-01, -8.6288e-02, -3.6852e-02, -1.3840e-01,\n",
      "        -1.4638e-01, -1.5844e-01, -1.2759e-01,  1.5428e-01, -2.4942e-02,\n",
      "        -1.5789e-01,  3.5075e-02,  7.0320e-02, -1.0181e-01, -9.3846e-02,\n",
      "        -6.0247e-02, -1.0749e-01,  2.5149e-01, -1.5908e-01,  1.3973e-01,\n",
      "         1.4689e-01,  7.5141e-02, -2.6053e-01, -4.2137e-02, -1.3690e-01,\n",
      "         7.5296e-02,  7.8063e-02, -4.5173e-02,  4.6669e-02, -8.9273e-02,\n",
      "         7.6568e-02, -8.2824e-02,  3.6501e-02, -5.6920e-03, -1.7847e-01,\n",
      "        -6.3578e-02,  7.4202e-02, -5.4559e-02, -1.9573e-01,  8.4951e-02,\n",
      "        -3.1908e-02, -1.0666e-01,  6.8259e-02,  9.3664e-02,  9.7941e-02,\n",
      "         4.3357e-02, -3.5638e-02,  1.1909e-01,  2.0871e-01,  2.3439e-03,\n",
      "        -1.6842e-01,  1.1458e-01, -1.0271e-01, -2.0387e-01, -1.0699e-02,\n",
      "        -1.9424e-01, -2.3220e-01,  3.7621e-01, -4.3973e-02,  1.0098e-01,\n",
      "        -8.4324e-02,  1.0339e-01,  7.9633e-02,  1.1059e-02, -6.3272e-02,\n",
      "        -5.1921e-02, -6.1275e-02,  1.7479e-01, -6.7259e-02, -5.0341e-02,\n",
      "        -1.0619e-03, -2.0729e-02, -1.3634e-01, -1.9683e-01, -3.2036e-02,\n",
      "        -1.3236e-01, -5.7521e-02, -3.4630e-01, -6.4637e-02,  3.6494e-02,\n",
      "         1.7505e-02, -2.6015e-02, -1.2163e-02, -8.1621e-02, -9.3776e-03,\n",
      "        -6.5891e-02, -8.7841e-02,  1.0725e-02,  1.2659e-01,  1.2424e-01,\n",
      "        -2.2254e-01, -1.6078e-02,  1.9209e-01, -1.3527e-01, -5.6404e-02,\n",
      "        -6.6353e-02, -5.4212e-02, -1.0381e-01,  3.4605e-02, -3.0050e-01,\n",
      "        -4.5255e-02,  8.9341e-02,  1.1804e-01,  9.5022e-03, -1.0324e-01,\n",
      "        -3.0620e-02,  1.3307e-02,  1.3261e-02,  9.4517e-02,  1.2996e-01,\n",
      "        -1.7492e-01, -5.9026e-02, -2.6309e-02, -1.7795e-01,  2.0803e-01,\n",
      "        -1.5328e-01,  1.7577e-01, -1.2653e-01, -3.9010e-02, -3.5661e-02,\n",
      "         3.3999e-02,  2.5544e-02, -1.6806e-02,  8.2910e-02, -9.1002e-02,\n",
      "        -1.5496e-02, -1.5013e-01, -4.5431e-02, -1.2167e-01, -2.3623e-02,\n",
      "         5.2435e-02,  2.1404e-01,  6.9965e-02, -2.8291e-01, -1.4104e-01,\n",
      "        -9.4718e-02, -8.2851e-02, -1.1222e-01,  4.4198e-02, -7.8396e-02,\n",
      "         3.3971e-01,  1.8752e-02, -4.4763e-02,  9.4552e-02, -1.0529e-01,\n",
      "        -7.5466e-02,  3.6245e-02, -1.1586e-01,  2.9750e-03,  6.8061e-02,\n",
      "        -1.5763e-01,  3.4784e-02,  1.2822e-01,  3.6380e-02, -9.1957e-02,\n",
      "        -1.2504e-01, -2.4507e-01,  2.5012e-01, -1.6512e-01,  1.2243e-01,\n",
      "         1.1731e-02,  6.0062e-03,  3.2122e-01,  4.8764e-02,  2.6239e-02,\n",
      "        -1.5216e-01, -3.0201e-02,  8.7813e-02, -2.5850e-01, -8.8874e-02,\n",
      "        -5.3887e-02,  1.0036e-02, -5.7196e-02,  1.3768e-01,  9.6501e-02,\n",
      "        -1.6174e-01, -1.2477e-01,  1.1353e-02,  2.7988e-01, -1.0664e-01,\n",
      "        -1.8549e-01, -2.0121e-01, -1.5777e-01,  4.3846e-01, -2.8523e-01,\n",
      "         9.3301e-02,  2.0301e-01, -9.2839e-02,  3.9086e-02, -1.2385e-01,\n",
      "         2.4979e-01,  2.3708e-02, -1.5108e-01,  8.3141e-02, -3.5331e-03,\n",
      "        -1.7984e-01,  1.9994e-03,  2.6601e-01, -1.4723e-01, -4.1575e-02,\n",
      "         1.3083e-01, -8.3399e-02,  8.3065e-03,  7.0052e-02, -2.5480e-01,\n",
      "         6.9214e-02,  8.1520e-02, -2.0864e-01,  2.0148e-01,  2.5747e-02,\n",
      "        -7.8647e-02, -1.0126e-02, -1.3787e-02, -1.5976e-01, -8.8022e-02,\n",
      "        -1.3740e-01,  3.4907e-01,  6.0001e-02, -1.2469e-02,  1.3836e-01,\n",
      "        -1.3186e-01, -1.7634e-01, -9.6318e-02,  3.3908e-02,  3.0317e-02,\n",
      "         2.4922e-02, -4.9435e-02, -6.0610e-02, -5.5831e-02, -1.4417e-02,\n",
      "        -3.5476e-01, -1.3866e-03, -5.5342e-02, -7.4446e-02, -1.1136e-01,\n",
      "        -3.7768e-02,  4.0438e-02, -1.4194e-01,  1.3208e-01,  3.3465e-02,\n",
      "        -6.6132e-02,  1.4698e-02, -1.5118e-02, -6.6952e-02, -1.4034e-01,\n",
      "         5.3979e-02,  1.2093e-01, -1.2158e-01, -4.5033e-02, -1.3276e-01,\n",
      "         4.0652e-02,  1.1958e-01, -7.1177e-02, -2.3218e-01,  1.8437e-02,\n",
      "        -4.1149e-02, -6.6646e-03, -1.2588e-01, -8.9321e-02,  1.6280e-01,\n",
      "         6.0108e-03,  7.9002e-02,  1.4609e-01,  2.2289e-01, -1.2587e-01,\n",
      "        -1.0448e-01,  1.1856e-01, -7.2612e-02, -2.2706e-02,  3.1659e-01,\n",
      "        -1.5246e-01, -6.7406e-02, -8.6253e-02, -5.6389e-01, -1.5100e-01,\n",
      "        -5.1388e-02, -9.4064e-02,  1.1220e-01,  1.0446e-01, -1.6443e-02,\n",
      "         4.5157e-02, -8.3340e-02, -4.9264e-02, -1.4150e-01,  7.8216e-02,\n",
      "        -1.9172e-01,  8.6144e-02,  1.3987e-01, -3.2545e-01, -8.4355e-02,\n",
      "        -1.0346e-01,  3.4998e-02,  1.4397e-02, -9.1520e-02, -1.2840e-01,\n",
      "         9.2272e-02, -6.9746e-02, -5.2818e-02,  1.9239e-01, -1.9417e-01,\n",
      "        -2.1014e-03, -1.4803e-01, -9.9406e-02, -5.3005e-02, -2.2686e-01,\n",
      "         1.2517e-01,  2.6623e-02,  1.3905e-01,  8.6167e-02, -4.1232e-02,\n",
      "        -2.8962e-01,  3.2187e-02, -2.6401e-01,  1.3374e-03, -2.3567e-01,\n",
      "        -1.8503e-01, -2.2663e-01,  2.7782e-04,  8.6231e-03, -3.9813e-02,\n",
      "        -1.4127e-01, -3.8811e-02,  8.9442e-02, -2.9726e-02,  1.2284e-01,\n",
      "        -2.4176e-02, -4.1873e-02,  3.8731e-02,  1.1997e-02,  1.1626e-01,\n",
      "        -7.5114e-03, -1.1712e-01,  1.6217e-01, -1.1859e-01, -4.4971e-02,\n",
      "        -8.9147e-02, -1.1365e-01, -4.2771e-02, -2.0340e-01, -2.7154e-01,\n",
      "        -5.7725e-02,  4.4303e-02, -1.5249e-01, -2.4284e-01,  5.7491e-03,\n",
      "        -5.5762e-02, -2.0484e+00, -6.0178e-02, -1.7351e-01,  5.5744e-02,\n",
      "        -9.7892e-02, -2.3720e-01,  9.6660e-03, -4.3097e-02, -1.6294e-01,\n",
      "         2.3769e-02,  3.0890e-02,  2.5946e-02, -7.0973e-02, -1.2905e-02,\n",
      "        -1.3035e-01, -2.7947e-01, -1.3301e-01,  3.0920e-02, -1.9128e-01,\n",
      "        -1.2652e-01, -1.0617e-01,  6.7705e-02, -9.6890e-02, -2.5993e-01,\n",
      "        -2.1838e-01, -1.9038e-02, -1.5787e-01,  1.3233e-01, -1.0564e-01,\n",
      "        -1.1181e-01, -1.7376e-01, -1.0348e-02,  1.6123e-02, -9.7438e-02,\n",
      "         1.7677e-01, -1.0880e-01, -1.8208e-01,  2.6175e-02, -5.0911e-02,\n",
      "        -8.0408e-02, -3.5280e-01, -1.6439e-03, -1.1595e-01, -1.0296e-01,\n",
      "         5.9526e-02, -3.0105e-02, -2.3980e-01, -1.9466e-01,  5.1443e-02,\n",
      "        -1.3634e-02, -2.0180e-01, -1.4303e-01,  2.4293e-01, -1.4119e-01,\n",
      "         6.6241e-02, -2.5119e-02, -2.2634e-01,  4.9455e-02, -3.5843e-02,\n",
      "         9.3880e-02, -2.9561e-02,  2.1177e-02,  8.8282e-02, -3.4878e-02,\n",
      "        -1.0529e-01, -8.5900e-02,  5.5853e-02, -6.7429e-02, -2.4318e-01,\n",
      "        -7.0433e-02, -5.6187e-02, -1.7537e-02, -9.2450e-02,  5.5404e-02,\n",
      "         1.1693e-01,  2.1411e-03,  1.0852e-01, -1.6258e-01, -8.9486e-02,\n",
      "         3.3185e-02, -3.9967e-01, -8.6833e-02,  1.0122e-01,  7.9135e-02,\n",
      "        -4.8679e-02,  3.5908e-02, -2.0967e-01,  3.0276e-02, -3.0248e-02,\n",
      "        -1.8635e-02, -2.2179e-02,  3.0483e-02, -1.2172e-02, -1.5896e-02,\n",
      "        -5.1308e-02,  7.9169e-03, -3.1560e-03, -2.3369e-01, -1.8727e-01,\n",
      "        -1.8138e-02,  1.5583e-01, -1.5750e-01, -1.7268e-01, -7.2275e-02,\n",
      "        -1.1359e-01,  1.0372e-01,  9.5279e-02, -2.3700e-01,  6.7927e-02,\n",
      "        -5.4689e-02, -6.6795e-02, -7.8034e-03,  5.1142e-02,  4.1345e-02,\n",
      "        -5.0390e-02, -4.8646e-02, -5.1691e-02, -1.4760e-02, -3.1041e-02,\n",
      "         5.9051e-02,  2.0838e-01, -1.7838e-01, -1.8990e-01, -1.2479e-02,\n",
      "         8.9026e-02,  2.7359e-02, -1.0191e-02, -2.6955e-01, -1.6075e-01,\n",
      "        -1.0076e-01, -1.2747e-01, -5.9050e-02, -5.4247e-02, -3.2529e-02,\n",
      "         3.0500e-01, -6.4284e-02,  6.8344e-02, -1.1188e-01, -2.5651e-02,\n",
      "        -9.5037e-02,  1.6071e-02,  9.5468e-02, -6.5318e-02, -1.7407e-02,\n",
      "        -9.1978e-02, -1.6614e-01, -6.4525e-02, -5.0356e-02,  1.9642e-01,\n",
      "         2.4656e-01, -1.2083e-01, -1.9730e-01, -4.6121e-02,  2.3815e-02,\n",
      "        -2.8763e-01, -3.8729e-02, -2.4116e-02,  1.9294e-02, -2.5607e-01,\n",
      "         6.0059e-02, -4.0951e-02,  3.9169e-02, -7.8183e-02,  5.9337e-04,\n",
      "        -8.9413e-02, -1.5221e-01, -7.1792e-02,  2.0419e-01,  1.6194e-01,\n",
      "        -1.8657e-01, -5.6089e-02,  8.5113e-02, -2.9219e-02, -8.0909e-02,\n",
      "        -1.1971e-01, -8.0849e-02,  1.6673e-01,  6.3270e-03, -1.8537e-01,\n",
      "        -1.0277e-01, -1.1674e-01, -3.2493e-02, -1.4575e-01, -1.9211e-01,\n",
      "         3.4198e-02,  1.4530e-01, -6.4132e-02,  2.3739e-02, -3.8200e-02,\n",
      "        -1.3475e-02,  2.5719e-02,  9.7640e-02,  7.7657e-02, -1.7459e-01,\n",
      "         2.7276e-02, -1.9434e-02, -3.2010e-01,  7.3161e-02, -3.0124e-01,\n",
      "        -1.8524e-01,  9.7382e-02,  1.1597e-01, -8.5432e-02, -4.8757e-02,\n",
      "        -1.6589e-01,  3.1417e-01, -1.0791e-01, -1.5835e-01,  3.9003e-03,\n",
      "        -1.8781e-02, -4.5276e-02,  1.4544e-01,  4.4114e-02,  5.0566e-02,\n",
      "        -1.6837e-01, -5.6981e-02,  1.1292e-02, -1.2216e-01,  3.7134e-02,\n",
      "        -1.5265e-02,  1.0105e-02, -2.1152e-01,  4.4661e-02,  7.5812e-02,\n",
      "         1.5040e-01,  4.2573e-02, -6.6854e-02, -5.5766e-02, -1.1715e-02,\n",
      "         2.4275e-02, -1.1002e-01, -1.6192e-02,  5.5343e-02, -1.1090e-01,\n",
      "         1.0976e-02, -4.9886e-02,  3.3283e-02, -9.2426e-02, -9.7246e-02,\n",
      "         2.4438e-01,  2.7026e-02, -4.9492e-02,  1.9705e-01, -6.5251e-02,\n",
      "         1.1531e-01,  2.2064e-01, -1.6844e-01,  2.1337e-02, -6.3539e-02,\n",
      "         6.5378e-02,  9.7468e-02,  1.6086e-01,  2.1043e-01, -8.2199e-02,\n",
      "        -2.1462e-02,  5.7733e-02, -3.4789e-02,  1.2938e-01, -6.9835e-02,\n",
      "        -1.2751e-01,  9.4966e-02, -7.0440e-04, -1.8504e-01, -9.7284e-03,\n",
      "        -1.7465e-01,  4.4449e-02,  6.8175e-02,  2.6017e-02,  2.0283e-01,\n",
      "         5.1417e-02,  1.1998e-01,  2.2961e-01,  1.5199e-01, -2.1730e-01,\n",
      "        -1.0882e-01,  1.5819e-01,  1.2677e-02,  7.7333e-02, -1.5196e-01,\n",
      "        -2.0030e-02, -1.6815e-01,  2.8220e-05, -1.4256e-01, -3.0758e-03,\n",
      "        -7.1610e-02,  3.0683e-02,  2.1486e-01,  5.1699e-02,  1.4317e-01,\n",
      "         1.3573e-01, -7.2106e-02, -2.9970e-01,  8.5391e-02,  7.7219e-02,\n",
      "        -1.6371e-01,  1.9799e-01,  2.7386e-02, -7.1478e-02, -1.2126e-02,\n",
      "        -8.9737e-02, -8.5347e-02,  1.4199e-01,  6.1397e-03, -7.6073e-02,\n",
      "         2.0152e-02,  2.9340e-02,  1.1531e-01, -2.3834e-01, -8.1233e-02,\n",
      "        -9.7873e-02, -3.1257e-02,  9.0707e-02, -2.5569e-02, -5.6624e-02,\n",
      "         8.3951e-02, -1.7097e-01,  4.8173e-02, -1.9606e-01, -4.9548e-02,\n",
      "         1.1117e-01,  3.0434e-02, -1.5272e-01,  5.3138e-02,  5.9988e-02,\n",
      "         1.1033e-01,  8.7653e-02,  3.7318e-02, -2.5699e-02,  2.7871e-02,\n",
      "         8.9237e-02,  1.4625e-02,  2.2827e-01, -1.2385e-02, -8.7442e-02,\n",
      "         9.6131e-02, -2.9917e-02,  4.2278e-03,  1.0227e-02, -7.8839e-02,\n",
      "        -1.0954e-01,  4.7135e-02, -1.5765e-01, -1.3107e-01,  1.7482e-01,\n",
      "        -1.2868e-01,  1.1598e-02, -7.7091e-02, -5.2503e-02, -8.4464e-03,\n",
      "         1.3035e-01,  7.8458e-02, -3.6364e-02, -2.6009e-02,  2.8693e-01,\n",
      "        -1.2584e-01, -1.9092e-02,  2.8566e-01, -4.6883e-02, -1.0753e-02,\n",
      "         1.7076e-02,  2.5229e-02, -5.8726e-02, -1.0248e-01, -1.0920e-01,\n",
      "         1.2650e-01, -4.0981e-02, -6.7747e-02,  6.3172e-02,  1.0677e-01,\n",
      "        -1.1568e-01, -3.6971e-02, -3.8060e-02,  1.7460e-01, -2.7730e-01,\n",
      "         6.3047e-02, -4.0602e-02,  1.2599e-01, -7.0001e-02, -5.4202e-02,\n",
      "        -2.6306e-02,  1.3636e-02, -1.6198e-03, -4.4283e-02, -2.2699e-01,\n",
      "         1.0663e-01,  2.5218e-02, -1.5580e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8742, 0.8706, 0.7915, 0.7595, 0.9224, 0.8445, 0.8532, 0.8569, 0.8421,\n",
      "        0.8229, 0.7422, 0.6822, 0.7783, 0.7567, 0.8391, 0.8450, 0.7797, 0.7830,\n",
      "        0.9127, 0.8371, 0.8571, 0.7927, 0.8307, 0.8820, 0.7855, 0.8957, 0.7591,\n",
      "        0.8718, 0.7878, 0.8350, 0.8610, 0.7771, 0.8634, 0.8458, 0.8442, 0.8601,\n",
      "        0.8424, 0.8067, 0.8565, 0.8412, 0.8503, 0.8805, 0.8780, 0.8435, 0.7906,\n",
      "        0.7628, 0.9501, 0.8636, 0.8851, 0.8454, 0.8912, 0.8351, 0.9049, 0.7563,\n",
      "        0.8250, 0.7826, 0.8182, 0.7070, 0.8028, 0.7404, 0.8376, 0.7333, 0.7212,\n",
      "        0.7898, 0.8234, 0.8341, 0.8192, 0.8594, 0.8030, 0.8848, 0.8729, 0.8908,\n",
      "        0.8324, 0.7495, 0.8646, 0.7457, 0.8090, 0.7369, 0.7608, 0.8042, 0.8352,\n",
      "        0.8813, 0.9096, 0.8153, 0.6590, 0.7578, 0.8149, 0.8025, 0.8442, 0.8797,\n",
      "        0.8204, 0.8248, 0.8654, 0.7549, 0.7939, 0.7806, 0.7259, 0.7672, 0.8263,\n",
      "        0.8593, 0.9033, 0.8954, 0.8615, 0.7968, 0.7549, 0.6647, 0.7691, 0.8833,\n",
      "        0.8285, 0.7983, 0.8311, 0.7893, 0.7821, 0.8382, 0.8391, 0.7853, 0.7990,\n",
      "        0.8612, 0.8488, 0.8005, 0.8187, 0.7255, 0.7692, 0.7733, 0.7095, 0.7993,\n",
      "        0.8277, 0.7521, 0.8435, 0.7821, 0.8648, 0.7955, 0.6887, 0.8512, 0.8251,\n",
      "        0.8696, 0.8123, 0.7644, 0.8271, 0.6795, 0.8820, 0.8848, 0.8754, 0.9218,\n",
      "        0.7805, 0.7063, 0.8623, 0.8005, 0.8548, 0.8355, 0.7796, 0.8098, 0.7208,\n",
      "        0.8715, 0.8474, 0.7715, 0.8335, 0.7731, 0.8838, 0.6790, 0.8499, 0.8608,\n",
      "        0.7732, 0.9097, 0.7869, 0.8834, 0.8442, 0.7643, 0.7676, 0.8513, 0.8392,\n",
      "        0.8126, 0.7639, 0.8652, 0.8967, 0.7696, 0.7491, 0.8106, 0.7832, 0.8119,\n",
      "        0.7602, 0.8817, 0.8108, 0.8725, 0.8759, 0.8410, 0.9012, 0.8330, 0.8760,\n",
      "        0.8228, 0.8419, 0.9283, 0.8720, 0.7790, 0.8465, 0.8660, 0.8480, 0.8047,\n",
      "        0.8156, 0.7114, 0.8452, 0.8114, 0.7014, 0.7856, 0.7785, 0.8279, 0.8170,\n",
      "        0.7640, 0.8470, 0.7927, 0.7429, 0.8660, 0.8057, 0.8439, 0.8074, 0.6624,\n",
      "        0.7288, 0.9173, 0.7916, 0.7921, 0.8555, 0.8610, 0.8390, 0.7464, 0.8252,\n",
      "        0.6929, 0.7881, 0.7619, 0.8433, 0.8920, 0.8172, 0.8403, 0.8299, 0.8530,\n",
      "        0.8228, 0.8199, 0.7983, 0.7594, 0.8543, 0.7667, 0.7394, 0.8494, 0.8489,\n",
      "        0.7709, 0.7868, 0.6926, 0.7687, 0.7939, 0.7712, 0.7460, 0.7924, 0.9201,\n",
      "        0.8243, 0.8110, 0.8769, 0.8132, 0.7753, 0.7926, 0.7558, 0.7842, 0.8867,\n",
      "        0.7854, 0.8586, 0.8022, 0.7588, 0.8059, 0.8177, 0.8049, 0.7878, 0.8598,\n",
      "        0.8082, 0.8269, 0.8307, 0.7816, 0.8612, 0.8523, 0.6922, 0.8105, 0.7777,\n",
      "        0.8507, 0.7672, 0.8822, 0.8740, 0.7007, 0.8394, 0.7765, 0.8771, 0.8935,\n",
      "        0.7880, 0.8643, 0.7821, 0.7464, 0.8781, 0.8316, 0.8422, 0.7411, 0.7462,\n",
      "        0.8319, 0.8680, 0.8409, 0.9301, 0.8380, 0.8515, 0.8370, 0.9871, 0.8251,\n",
      "        0.7899, 0.8813, 0.9003, 0.8455, 0.7966, 0.6880, 0.8204, 0.8119, 0.8549,\n",
      "        0.7846, 0.8735, 0.8426, 0.8237, 0.6502, 0.8312, 0.8120, 0.7962, 0.8729,\n",
      "        0.8504, 0.8479, 0.7438, 0.8267, 0.8497, 0.8218, 0.7572, 0.6613, 0.8101,\n",
      "        0.9096, 0.9058, 0.8072, 0.7877, 0.8749, 0.8686, 0.8128, 0.7603, 0.9434,\n",
      "        0.7391, 0.8317, 0.7337, 0.7035, 0.8579, 0.8406, 0.8283, 0.9067, 0.8295,\n",
      "        0.8537, 0.8595, 0.6476, 0.8600, 0.8353, 0.7895, 0.7721, 0.8004, 0.8137,\n",
      "        0.8573, 0.7628, 0.7840, 0.7888, 0.8355, 0.8721, 0.7735, 0.8153, 0.8347,\n",
      "        0.7596, 0.8336, 0.8168, 0.8737, 0.8318, 0.8871, 0.8848, 0.8656, 0.7969,\n",
      "        0.7990, 0.9082, 0.8330, 3.3634, 0.8581, 0.8947, 0.8201, 0.8336, 0.8389,\n",
      "        0.8876, 0.8424, 0.8254, 0.7774, 0.8138, 0.8262, 0.7721, 0.8589, 0.8759,\n",
      "        0.8408, 0.7871, 0.8417, 0.8640, 0.7772, 0.7035, 0.7515, 0.8229, 0.7483,\n",
      "        0.7748, 0.8444, 0.7969, 0.8564, 0.8676, 0.8175, 0.8222, 0.8948, 0.8486,\n",
      "        0.7743, 0.7809, 0.8099, 0.8058, 0.8270, 0.7409, 0.8143, 0.7306, 0.8507,\n",
      "        0.7844, 0.8089, 0.8119, 0.8071, 0.8077, 0.8396, 0.7949, 0.8203, 0.7958,\n",
      "        0.8204, 0.7811, 0.8558, 0.9097, 0.8101, 0.7816, 0.8528, 0.8314, 0.8014,\n",
      "        0.8585, 0.8005, 0.6590, 0.8192, 0.8719, 0.9241, 0.7866, 0.8415, 0.7909,\n",
      "        0.8366, 0.8686, 0.8967, 0.8153, 0.9138, 0.8828, 0.7999, 0.8375, 0.7892,\n",
      "        0.8604, 0.8115, 0.7749, 0.8478, 0.8762, 0.8683, 0.7459, 0.7498, 0.8132,\n",
      "        0.7425, 0.7776, 0.9298, 0.7249, 0.8224, 0.8406, 0.7667, 0.8309, 0.7886,\n",
      "        0.8780, 0.8305, 0.8109, 0.8382, 0.8353, 0.7046, 0.8225, 0.7973, 0.8246,\n",
      "        0.8343, 0.8352, 0.7639, 0.8442, 0.8686, 0.8338, 0.8009, 0.9060, 0.8574,\n",
      "        0.7957, 0.7103, 0.7729, 0.8840, 0.8009, 0.7703, 0.8694, 0.7686, 0.8222,\n",
      "        0.7798, 0.8688, 0.7718, 0.8135, 0.8574, 0.8361, 0.7410, 0.7653, 0.7890,\n",
      "        0.7976, 0.8331, 0.8829, 0.8479, 0.6464, 0.8896, 0.7900, 0.8774, 0.7634,\n",
      "        0.8850, 0.7892, 0.8215, 0.8134, 0.7795, 0.8353, 0.8394, 0.7434, 0.8147,\n",
      "        0.7849, 0.8778, 0.8216, 0.8699, 0.7895, 0.8332, 0.7969, 0.7647, 0.9407,\n",
      "        0.8012, 0.7328, 0.8521, 0.7999, 0.8750, 0.8686, 0.8167, 0.7896, 0.8458,\n",
      "        0.8165, 0.8841, 0.7564, 0.8430, 0.8745, 0.8657, 0.8414, 0.8202, 0.7863,\n",
      "        0.8679, 0.8429, 0.8433, 0.8058, 0.8027, 0.8861, 0.8141, 0.8327, 0.7534,\n",
      "        0.8534, 0.8196, 0.8460, 0.7719, 0.8254, 0.8328, 0.7174, 0.8005, 0.8272,\n",
      "        0.8237, 0.7130, 0.8870, 0.8702, 0.8095, 0.8647, 0.8607, 0.7713, 0.8326,\n",
      "        0.8113, 0.9605, 0.8666, 0.7768, 0.8452, 0.8857, 0.8291, 0.8966, 0.7931,\n",
      "        0.7571, 0.8018, 0.8181, 0.8325, 0.8596, 0.8266, 0.7767, 0.8910, 0.8854,\n",
      "        0.7922, 0.8448, 0.7875, 0.9236, 0.7562, 0.8417, 0.8180, 0.8185, 0.7692,\n",
      "        0.8637, 0.7508, 0.6709, 0.7953, 0.7818, 0.8895, 0.8453, 0.7848, 0.7871,\n",
      "        0.8054, 0.8817, 0.8508, 0.8341, 0.8214, 0.8471, 0.8060, 0.7931, 0.8855,\n",
      "        0.7967, 0.7501, 0.7977, 0.8524, 0.8393, 0.7816, 0.8424, 0.8449, 0.7513,\n",
      "        0.8308, 0.8503, 0.7837, 0.8207, 0.7546, 0.8841, 0.8259, 0.8116, 0.8446,\n",
      "        0.7752, 0.8112, 0.9094, 0.8270, 0.8270, 0.7956, 0.6856, 0.7839, 0.7819,\n",
      "        0.8362, 0.8408, 0.8019, 0.8705, 0.9095, 0.8903, 0.8952, 0.8285, 0.7812,\n",
      "        0.8781, 0.8224, 0.8101, 0.8002, 0.6754, 0.8520, 0.8435, 0.7775, 0.8491,\n",
      "        0.8365, 0.8525, 0.8197, 0.8797, 0.8596, 0.8809, 0.9051, 0.8205, 0.8153,\n",
      "        0.8701, 0.8441, 0.7449, 0.8517, 0.8928, 0.8239, 0.8124, 0.8054, 0.8222,\n",
      "        0.8844, 0.8406, 0.8479, 0.8032, 0.7365, 0.7840, 0.6837, 0.9191, 0.8627,\n",
      "        0.7827, 0.7864, 0.8427, 0.7638, 0.7736, 0.7977, 0.8816, 0.8444, 0.7238,\n",
      "        0.8052, 0.8148, 0.8003, 0.7758, 0.8781, 0.8220, 0.8125, 0.8694, 0.7638,\n",
      "        0.6692, 0.8202, 0.8858, 0.8224, 0.8404, 0.8863, 0.8180, 0.8190, 0.8127,\n",
      "        0.8383, 0.8318, 0.8400, 0.8538, 0.9363, 0.8061, 0.7673, 0.8749, 0.8193,\n",
      "        0.8827, 0.7328, 0.8170, 0.8441, 0.8664, 0.8283, 0.7531, 0.8355, 0.7835,\n",
      "        0.8076, 0.8409, 0.8717, 0.8975, 0.8484, 0.7663, 0.8446, 0.8345, 0.7717,\n",
      "        0.7996, 0.8722, 0.8174, 0.6842, 0.8522, 0.8086, 0.7876, 0.8423, 0.8231,\n",
      "        0.7645, 0.7922, 0.8778], requires_grad=True)\n",
      "intermediate output  tensor([[[ 0.4457, -0.1159, -0.0878,  ..., -0.0128, -0.0055, -0.1466],\n",
      "         [-0.1268, -0.1581, -0.0966,  ..., -0.1614, -0.0386, -0.1554],\n",
      "         [-0.1697, -0.1220, -0.0064,  ..., -0.1684, -0.1149, -0.1672],\n",
      "         ...,\n",
      "         [-0.1578, -0.1554, -0.0421,  ..., -0.0160, -0.0210, -0.0916],\n",
      "         [-0.1284, -0.1594, -0.0814,  ..., -0.0379, -0.0111, -0.0989],\n",
      "         [-0.1308, -0.1699, -0.0901,  ..., -0.0555, -0.0058, -0.0656]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.4004,  0.5992, -1.2786,  ..., -0.6002,  0.2266,  0.8129],\n",
      "         [-0.7130, -1.0456, -0.2311,  ...,  0.3153, -0.1896, -0.8812],\n",
      "         [ 0.0565, -1.3469, -0.1829,  ..., -0.0752,  0.5674,  0.0990],\n",
      "         ...,\n",
      "         [-1.0065, -0.8484, -0.2749,  ..., -0.3799, -0.3107,  0.3713],\n",
      "         [-1.0856, -0.6758,  0.6757,  ...,  0.0845, -0.3226, -0.1875],\n",
      "         [-0.8265, -0.7934,  0.7650,  ...,  0.3130, -0.5202, -0.3989]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[-0.1295,  0.0539, -0.2085,  ...,  0.0453,  0.2162,  0.0420],\n",
      "         [ 0.0047, -0.1624, -0.2667,  ...,  0.0201,  0.0891, -0.0498],\n",
      "         [-0.0496, -0.0933, -0.2612,  ...,  0.1081,  0.1773, -0.1180],\n",
      "         ...,\n",
      "         [ 0.0144, -0.1558,  0.0645,  ...,  0.0127, -0.0308,  0.0457],\n",
      "         [ 0.0730,  0.0007,  0.0450,  ..., -0.0264,  0.0396,  0.0331],\n",
      "         [ 0.0963, -0.0483,  0.0866,  ...,  0.0686,  0.0611, -0.0063]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.4004,  0.5992, -1.2786,  ..., -0.6002,  0.2266,  0.8129],\n",
      "         [-0.7130, -1.0456, -0.2311,  ...,  0.3153, -0.1896, -0.8812],\n",
      "         [ 0.0565, -1.3469, -0.1829,  ..., -0.0752,  0.5674,  0.0990],\n",
      "         ...,\n",
      "         [-1.0065, -0.8484, -0.2749,  ..., -0.3799, -0.3107,  0.3713],\n",
      "         [-1.0856, -0.6758,  0.6757,  ...,  0.0845, -0.3226, -0.1875],\n",
      "         [-0.8265, -0.7934,  0.7650,  ...,  0.3130, -0.5202, -0.3989]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.7408,  0.9625, -1.7111,  ..., -0.6178,  0.4873,  1.1185],\n",
      "         [-0.8840, -1.3831, -0.4320,  ...,  0.3820, -0.1664, -1.1309],\n",
      "         [-0.0215, -1.6216, -0.3622,  ...,  0.0621,  0.7369, -0.0455],\n",
      "         ...,\n",
      "         [-1.1303, -1.0386, -0.0959,  ..., -0.3233, -0.3994,  0.4372],\n",
      "         [-1.0622, -0.6119,  0.7877,  ...,  0.0821, -0.3179, -0.1766],\n",
      "         [-0.7853, -0.7956,  0.9229,  ...,  0.3747, -0.4875, -0.4355]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.4004,  0.5992, -1.2786,  ..., -0.6002,  0.2266,  0.8129],\n",
      "         [-0.7130, -1.0456, -0.2311,  ...,  0.3153, -0.1896, -0.8812],\n",
      "         [ 0.0565, -1.3469, -0.1829,  ..., -0.0752,  0.5674,  0.0990],\n",
      "         ...,\n",
      "         [-1.0065, -0.8484, -0.2749,  ..., -0.3799, -0.3107,  0.3713],\n",
      "         [-1.0856, -0.6758,  0.6757,  ...,  0.0845, -0.3226, -0.1875],\n",
      "         [-0.8265, -0.7934,  0.7650,  ...,  0.3130, -0.5202, -0.3989]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-6.0043e-02,  4.5727e-02,  8.6881e-02, -1.4417e-01, -5.5005e-02,\n",
      "        -1.0474e-02,  2.5043e-01,  9.0392e-02, -8.2090e-02, -1.7247e-01,\n",
      "         4.2001e-02,  1.0978e-01, -1.4906e-01,  2.4737e-02, -8.9109e-02,\n",
      "         2.9581e-01,  1.9528e-01,  9.7223e-02, -6.8090e-02,  4.3327e-04,\n",
      "         1.7526e-01, -6.0136e-03, -4.7597e-02,  9.9404e-02,  4.7321e-02,\n",
      "         8.6658e-02, -6.5222e-03, -2.8466e-02, -1.7913e-01,  7.4106e-02,\n",
      "         7.8910e-04,  5.6712e-02,  2.0257e-01, -9.9306e-02, -8.5908e-02,\n",
      "         1.0107e-01, -2.1124e-01, -1.4615e-01, -5.0545e-02, -1.0593e-01,\n",
      "        -1.7662e-01, -1.1018e-01, -4.3122e-02,  9.1960e-02,  6.0818e-02,\n",
      "        -4.0597e-02,  5.4103e-02,  9.0968e-02, -9.6644e-02, -4.9721e-02,\n",
      "        -1.3164e-01,  6.5675e-02,  2.3594e-01,  2.8029e-02,  8.5331e-02,\n",
      "         8.3291e-02,  5.0295e-02, -2.5654e-01, -5.1320e-02, -2.0234e-01,\n",
      "         4.7726e-02,  1.8026e-01, -1.8444e-03, -9.4534e-02, -4.9336e-02,\n",
      "         1.2817e-01, -1.1062e-01,  1.0549e-02, -1.2287e-01, -1.2144e-01,\n",
      "        -8.1803e-02,  1.2332e-01, -9.2295e-02, -6.5228e-02,  3.2322e-02,\n",
      "         7.1861e-04, -8.9351e-02,  1.8048e-02,  1.2386e-01,  5.9615e-02,\n",
      "        -6.2845e-04, -3.5705e-02,  1.2916e-01,  1.1454e-01,  6.1890e-02,\n",
      "        -1.3117e-01, -2.9013e-02, -9.9673e-02, -1.8641e-01,  8.3851e-02,\n",
      "        -1.2004e-01, -2.0634e-01,  2.1076e-01, -2.1131e-02,  1.6728e-02,\n",
      "        -2.2085e-02,  6.7730e-02, -5.8642e-02,  8.0292e-02, -1.6058e-01,\n",
      "        -4.2313e-02, -1.0736e-01,  1.5815e-01, -8.4193e-02, -2.3722e-03,\n",
      "        -2.3385e-02, -6.1833e-02, -1.6079e-01, -1.0358e-01,  3.9939e-02,\n",
      "        -7.7048e-02,  1.8026e-02, -7.5784e-02, -9.0620e-02, -9.3887e-02,\n",
      "         2.6258e-02, -5.9140e-02, -1.2390e-01, -2.1069e-02,  1.8856e-02,\n",
      "        -2.0466e-02,  2.4606e-02,  4.5392e-02,  2.1690e-01,  1.1361e-01,\n",
      "        -2.7944e-01,  5.0200e-02,  5.4876e-02, -1.8211e-01, -2.6893e-02,\n",
      "        -8.2728e-02,  8.3858e-03, -1.3455e-01, -1.3325e-02, -1.9354e-01,\n",
      "        -4.4718e-02,  1.3886e-01, -1.9235e-02, -2.3992e-03, -2.3986e-02,\n",
      "        -7.4799e-02, -3.4916e-02, -6.4329e-02,  1.1733e-01,  1.3396e-01,\n",
      "        -1.8087e-01, -2.8187e-02, -1.0119e-01, -1.9703e-01,  1.7343e-01,\n",
      "        -1.9150e-01,  1.3186e-01, -1.6688e-01, -2.2447e-03, -1.5719e-01,\n",
      "         4.8532e-02, -6.9248e-03, -5.6537e-03, -5.4428e-02, -2.2368e-02,\n",
      "        -5.9284e-02, -1.0984e-01, -5.9494e-02, -1.0380e-01,  9.7611e-02,\n",
      "         1.5778e-02,  1.5968e-01,  8.5311e-02, -2.0677e-01,  1.7927e-02,\n",
      "        -7.6926e-02, -1.3392e-01, -6.8154e-02, -8.3804e-02, -4.3180e-02,\n",
      "         1.2472e-01,  1.0270e-01, -7.3696e-02,  2.5068e-02, -2.8000e-02,\n",
      "        -8.2574e-02, -4.1151e-03, -6.2540e-02, -1.2839e-01,  1.0978e-02,\n",
      "        -1.1873e-01,  1.8827e-02, -1.3362e-01,  1.6466e-01, -2.8234e-02,\n",
      "        -1.2024e-01, -1.3766e-01,  1.3024e-01, -1.9420e-01,  1.1318e-01,\n",
      "        -4.5297e-02, -5.0479e-02,  1.6549e-01, -3.7485e-02, -9.7978e-02,\n",
      "        -1.6516e-01, -2.8699e-02,  1.7943e-02, -2.0056e-01, -9.9338e-02,\n",
      "        -1.9311e-03, -5.4235e-02, -1.1880e-02,  2.3034e-02,  1.1718e-01,\n",
      "        -1.4116e-01, -9.8754e-02,  2.1158e-02,  2.5026e-01, -1.0230e-01,\n",
      "        -3.5182e-02, -2.7351e-01, -7.4305e-02,  2.4034e-01, -1.7391e-01,\n",
      "         4.2071e-02,  1.1843e-01, -6.8441e-02,  6.3684e-02, -1.4631e-01,\n",
      "         3.6316e-01,  6.8980e-02, -1.7664e-01,  5.1110e-02, -1.6003e-02,\n",
      "        -2.5529e-01,  3.1410e-02,  1.7439e-01, -1.2248e-01, -9.4493e-02,\n",
      "         2.1938e-02,  1.7721e-02, -1.4337e-01,  1.2564e-01, -1.6899e-01,\n",
      "         6.0880e-03,  1.8392e-01, -2.8349e-01,  1.4458e-02, -7.3361e-02,\n",
      "         7.2291e-03, -3.2562e-02, -6.2404e-02, -9.7708e-02, -2.6319e-02,\n",
      "        -1.7973e-01,  2.5711e-01,  9.0294e-02, -9.4203e-02,  1.4639e-01,\n",
      "        -2.6298e-02, -1.9028e-01, -2.0116e-01,  1.3522e-01,  5.8927e-02,\n",
      "         1.1420e-02, -1.5650e-02, -5.3074e-02, -2.7232e-02, -9.8585e-02,\n",
      "        -3.1707e-01, -1.4956e-02, -6.1652e-02,  5.5418e-02, -2.8167e-03,\n",
      "        -7.4810e-02,  9.4731e-02, -1.6868e-01,  9.3867e-02, -6.9943e-03,\n",
      "        -3.1934e-03,  6.3079e-04,  1.2580e-02, -4.1347e-02, -1.5103e-01,\n",
      "        -1.0031e-01,  1.2231e-01, -1.8633e-01, -6.6870e-02, -7.8260e-02,\n",
      "        -2.5207e-02,  1.4011e-02, -1.2848e-01, -9.8367e-02, -8.6895e-02,\n",
      "        -9.4088e-03,  5.7982e-03, -1.3161e-01, -1.3052e-02,  1.0334e-01,\n",
      "        -8.9752e-03,  2.7451e-02,  1.2971e-01,  1.9561e-01, -1.2013e-01,\n",
      "        -1.1927e-01,  9.9004e-02, -8.2399e-02,  1.3359e-01,  2.7740e-01,\n",
      "        -1.0971e-01, -9.1488e-02, -1.4878e-01, -2.7871e-01, -1.6647e-01,\n",
      "        -1.3939e-01, -2.0124e-01,  1.2942e-01,  4.5379e-02, -3.1917e-02,\n",
      "         4.2488e-02, -5.8852e-02, -6.8999e-02, -1.0001e-01, -4.0453e-02,\n",
      "        -3.6279e-02,  7.7971e-02,  9.2112e-02, -3.0593e-01, -1.0182e-01,\n",
      "        -1.3465e-01,  8.3798e-02, -3.6713e-02, -1.1855e-01, -9.5890e-02,\n",
      "         1.0298e-01,  6.3240e-03,  8.0082e-02,  3.9121e-03, -1.8091e-01,\n",
      "         4.6502e-02, -1.1558e-01, -8.1314e-02, -8.5864e-02, -2.0801e-01,\n",
      "         7.6071e-02, -1.1224e-02,  9.5689e-02,  1.0339e-02,  4.1806e-02,\n",
      "        -1.7120e-01, -1.6901e-02, -2.8978e-01, -9.2227e-03, -1.8044e-01,\n",
      "        -1.5280e-01, -1.8074e-01,  6.0165e-02, -9.9807e-02, -5.1576e-02,\n",
      "        -4.9287e-02, -7.5768e-02,  1.8297e-01, -6.2253e-02,  4.7436e-02,\n",
      "        -8.9444e-02, -2.2755e-02,  5.2678e-02,  9.6818e-02,  1.1571e-01,\n",
      "         7.6331e-02, -6.5298e-02,  4.1881e-02, -1.5688e-01, -5.6627e-02,\n",
      "        -1.0717e-01, -1.5422e-01, -3.5659e-02, -8.0523e-02, -1.8417e-01,\n",
      "        -5.6303e-02,  1.7676e-02, -1.5988e-01, -1.7127e-01, -1.2788e-01,\n",
      "        -1.7134e-02, -1.7460e+00, -1.3341e-01, -1.7917e-01,  1.1111e-01,\n",
      "        -7.4008e-02, -1.0803e-01,  7.1065e-02,  2.5900e-02, -1.9544e-01,\n",
      "        -9.4846e-02,  6.1110e-02,  1.7410e-03, -1.7919e-01, -5.5191e-02,\n",
      "        -5.9096e-02, -7.0384e-02, -4.9825e-02,  1.5515e-01, -1.3629e-01,\n",
      "        -1.2711e-01, -1.9731e-02,  1.2140e-01, -4.4647e-02, -3.2350e-01,\n",
      "        -1.2600e-01, -3.3796e-02, -1.9992e-01,  1.6340e-01, -9.0332e-02,\n",
      "        -6.7900e-02, -4.6519e-02, -1.2251e-01, -7.5851e-02, -8.3525e-02,\n",
      "         1.1551e-01, -2.8183e-02, -2.3382e-01, -5.1152e-02, -4.0449e-02,\n",
      "        -1.6998e-02, -1.3577e-01,  6.7647e-02, -1.7192e-01, -6.8739e-02,\n",
      "         9.3880e-02, -5.3132e-02, -2.8285e-01, -1.5483e-01,  6.7752e-02,\n",
      "        -2.2627e-01, -2.4394e-01, -1.9411e-01,  1.3652e-01, -1.0448e-02,\n",
      "         6.7057e-02,  1.2362e-01, -1.4181e-01,  5.9248e-02, -2.6126e-02,\n",
      "         2.5776e-02, -7.1952e-02, -2.0760e-02, -1.6529e-01, -7.0032e-02,\n",
      "        -6.9645e-03, -1.5499e-01,  6.8741e-02,  1.3762e-02, -3.5218e-02,\n",
      "         4.9549e-02, -7.5311e-02,  1.3664e-02, -8.7883e-02,  1.8292e-01,\n",
      "         9.6423e-02, -1.2774e-01,  1.4762e-02, -3.0042e-01, -1.3641e-01,\n",
      "        -8.3239e-02, -1.5048e-01,  5.6542e-02,  1.6011e-01,  1.5608e-01,\n",
      "        -4.6714e-02,  1.1039e-01, -3.3949e-01,  9.2549e-02,  5.1425e-04,\n",
      "        -3.2317e-02, -3.0743e-02,  7.0931e-03,  1.6091e-01,  5.1715e-04,\n",
      "        -1.4044e-01, -1.7579e-02, -7.7398e-02, -5.3600e-02, -2.5986e-01,\n",
      "         1.5481e-02,  6.2275e-02, -2.0836e-01, -4.1511e-02, -1.7083e-01,\n",
      "        -7.5686e-02,  1.4853e-02,  1.4612e-01,  4.8640e-03, -7.4044e-03,\n",
      "        -1.2766e-01, -9.5883e-02, -4.2891e-02, -4.4178e-02,  7.9012e-03,\n",
      "        -6.1651e-03, -5.3386e-02,  5.6057e-03,  6.7158e-03,  7.4913e-02,\n",
      "         9.0871e-02,  1.8149e-01, -1.7557e-01, -2.5779e-01, -7.8879e-02,\n",
      "         4.9418e-02, -4.4383e-02,  6.0883e-02, -1.6172e-01, -1.2085e-01,\n",
      "        -1.5022e-01, -6.5437e-02, -8.1825e-02, -1.3083e-01,  1.2543e-02,\n",
      "         2.5780e-01,  1.9836e-02,  1.6917e-02, -6.5315e-02, -4.9451e-02,\n",
      "        -7.9245e-02, -5.8966e-03,  1.2537e-01, -1.8426e-01, -3.1040e-02,\n",
      "        -1.6180e-01, -1.8361e-01, -1.0397e-01, -1.1564e-02,  6.4058e-02,\n",
      "         9.8893e-02, -1.1950e-01, -1.1321e-01, -7.6212e-03,  5.7129e-02,\n",
      "        -2.4944e-01,  8.9628e-02, -1.0366e-01,  6.5120e-02, -2.0047e-01,\n",
      "         1.0281e-02,  1.6675e-02,  3.5397e-02, -3.1448e-04, -8.9842e-02,\n",
      "        -8.5394e-02, -4.1007e-02, -4.3577e-02,  1.4979e-01,  1.1193e-01,\n",
      "        -2.9281e-02, -4.0975e-02, -1.3547e-02,  1.1486e-02,  2.8582e-02,\n",
      "        -1.6404e-01, -2.0149e-02,  1.2374e-01, -5.7698e-03, -7.0315e-02,\n",
      "        -5.0054e-02, -2.9453e-03,  1.1590e-01, -1.4309e-01, -1.1349e-01,\n",
      "         6.3019e-02,  1.9525e-02,  2.9545e-02,  5.0243e-02, -2.3065e-02,\n",
      "        -6.4063e-02,  3.0920e-03,  1.7227e-01, -2.4722e-02, -7.5007e-02,\n",
      "         6.2572e-02,  8.5089e-02, -2.5624e-01,  7.9046e-02, -3.0510e-01,\n",
      "        -1.3232e-01,  8.3871e-02,  1.9946e-02, -3.8351e-02, -1.4455e-01,\n",
      "        -1.3392e-01,  2.0380e-01, -7.6327e-02, -1.4311e-01, -8.4853e-02,\n",
      "        -5.9510e-02, -1.0840e-01,  3.0047e-02,  1.2915e-02,  3.1888e-02,\n",
      "         2.9743e-02, -8.6132e-02,  4.9797e-02, -6.4389e-02,  1.3277e-01,\n",
      "        -5.3022e-02,  9.5512e-04, -1.5403e-01, -7.9885e-03,  9.7154e-02,\n",
      "         1.1641e-01, -1.3559e-02,  1.0035e-01, -1.1032e-01,  3.5650e-02,\n",
      "        -2.3193e-02, -1.5914e-01, -2.4085e-02,  1.0567e-01, -9.2590e-02,\n",
      "        -9.5098e-02, -6.9623e-02, -2.8955e-02, -1.0071e-01, -1.6246e-01,\n",
      "         1.0943e-01, -5.3557e-02, -4.6878e-02,  7.7014e-02,  2.3381e-02,\n",
      "         2.0776e-02,  2.0008e-01, -3.7203e-02,  4.1036e-02, -5.5812e-02,\n",
      "         4.0951e-02,  1.0148e-01,  1.2284e-01,  2.5115e-01, -1.2672e-02,\n",
      "         3.0992e-02,  1.3421e-01, -4.7092e-02,  1.4394e-01, -1.0366e-01,\n",
      "        -8.5281e-02,  1.0297e-01, -6.8762e-02, -2.4550e-01, -9.6800e-02,\n",
      "        -2.1981e-01, -3.0771e-02,  4.6445e-02,  7.3241e-02,  3.0206e-01,\n",
      "         3.4701e-02,  1.6843e-02,  1.3209e-01,  1.4769e-01, -4.9531e-02,\n",
      "        -6.9604e-02,  1.3091e-01,  1.3750e-01,  9.0891e-02, -1.6751e-01,\n",
      "         3.2562e-02, -2.1636e-01,  1.3645e-01, -1.4492e-01,  3.0106e-02,\n",
      "        -8.8176e-02, -7.3143e-02,  1.6979e-01,  1.7351e-01,  8.7303e-02,\n",
      "         2.2707e-01, -8.7175e-02, -1.7258e-01, -1.8927e-02,  1.4742e-01,\n",
      "        -2.4594e-03,  1.2034e-01,  2.8535e-02, -1.2334e-01, -8.3777e-02,\n",
      "        -4.2388e-02, -1.6021e-01,  9.3676e-02, -9.8286e-02, -5.2567e-02,\n",
      "         1.0417e-01,  5.7425e-02,  1.2465e-01, -1.3079e-01, -3.1143e-02,\n",
      "         2.7148e-02,  1.6231e-02, -1.2204e-01, -1.0895e-01, -1.0246e-02,\n",
      "         4.4583e-02, -1.9248e-01, -8.7517e-02, -1.3809e-01, -9.5399e-02,\n",
      "         1.4375e-01,  3.0366e-03, -6.9659e-02,  1.8529e-02,  1.5480e-01,\n",
      "         1.0105e-01,  2.6293e-01, -2.0592e-02, -3.5529e-02,  4.4947e-02,\n",
      "        -5.5688e-02,  8.0089e-03,  6.0394e-02,  7.8154e-02, -5.8051e-02,\n",
      "         5.8847e-02, -1.1170e-01, -6.5167e-03,  9.4915e-02, -5.5376e-02,\n",
      "        -1.7667e-01,  6.4343e-03, -8.6166e-02, -8.9625e-02,  1.2279e-01,\n",
      "        -1.1036e-01, -7.4790e-02, -1.3269e-01,  4.7373e-02, -1.3886e-01,\n",
      "        -2.3142e-02,  7.9503e-02, -1.1848e-02, -1.0863e-01,  2.1293e-01,\n",
      "        -9.9184e-02, -1.2978e-02,  3.1980e-01, -1.2452e-01, -2.7241e-02,\n",
      "        -4.6770e-02,  3.8339e-02, -1.4157e-01, -8.0027e-02, -3.7430e-02,\n",
      "         9.3445e-02,  2.0585e-02, -1.4760e-01,  1.1813e-02,  1.3194e-01,\n",
      "        -7.9695e-02, -8.6726e-02, -5.4680e-02,  1.4956e-01, -2.0107e-01,\n",
      "         5.8653e-02, -8.7965e-02,  7.8894e-02, -8.2017e-02, -1.2939e-01,\n",
      "        -4.2024e-02, -7.4845e-02,  8.6725e-02, -1.2495e-01, -2.3734e-01,\n",
      "         2.1638e-03, -8.5525e-02, -5.3259e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8908, 0.8912, 0.8116, 0.7810, 0.9046, 0.8539, 0.8949, 0.8632, 0.8615,\n",
      "        0.8553, 0.7586, 0.7825, 0.7810, 0.7848, 0.8592, 0.8605, 0.8111, 0.8043,\n",
      "        0.9238, 0.8334, 0.8583, 0.7965, 0.8517, 0.8636, 0.7861, 0.8805, 0.7569,\n",
      "        0.8774, 0.7820, 0.8401, 0.8882, 0.7960, 0.8682, 0.8827, 0.8477, 0.8515,\n",
      "        0.8630, 0.8383, 0.8712, 0.8518, 0.8452, 0.8744, 0.8910, 0.8639, 0.8180,\n",
      "        0.7845, 0.9137, 0.8614, 0.8759, 0.8616, 0.8605, 0.8446, 0.9184, 0.7615,\n",
      "        0.8322, 0.7946, 0.8233, 0.7546, 0.7876, 0.7706, 0.8726, 0.7936, 0.7487,\n",
      "        0.8152, 0.8308, 0.8398, 0.8232, 0.8691, 0.8340, 0.8828, 0.8837, 0.8959,\n",
      "        0.8483, 0.7884, 0.8597, 0.7645, 0.8296, 0.7724, 0.7680, 0.8156, 0.8494,\n",
      "        0.9142, 0.8876, 0.8160, 0.7374, 0.8071, 0.8276, 0.8066, 0.8716, 0.8896,\n",
      "        0.8099, 0.8310, 0.8187, 0.7616, 0.8063, 0.7962, 0.7728, 0.7769, 0.8203,\n",
      "        0.8972, 0.9047, 0.9095, 0.8652, 0.8382, 0.7504, 0.6943, 0.7765, 0.8866,\n",
      "        0.8538, 0.8118, 0.8457, 0.8148, 0.7895, 0.8516, 0.8314, 0.7906, 0.8118,\n",
      "        0.8653, 0.8519, 0.8154, 0.8068, 0.7781, 0.8017, 0.8584, 0.7417, 0.8088,\n",
      "        0.8404, 0.7744, 0.8649, 0.8039, 0.8376, 0.8145, 0.7568, 0.8571, 0.8112,\n",
      "        0.8787, 0.8299, 0.8236, 0.8412, 0.7311, 0.8927, 0.9002, 0.8956, 0.9526,\n",
      "        0.8160, 0.7533, 0.8686, 0.8274, 0.8601, 0.8464, 0.7887, 0.8272, 0.7487,\n",
      "        0.8802, 0.8552, 0.7876, 0.8271, 0.8050, 0.8548, 0.7456, 0.8622, 0.8773,\n",
      "        0.8034, 0.9093, 0.8087, 0.8698, 0.8437, 0.7889, 0.8080, 0.8516, 0.8541,\n",
      "        0.8410, 0.7939, 0.8558, 0.9246, 0.7931, 0.7500, 0.8215, 0.7794, 0.8158,\n",
      "        0.8194, 0.8959, 0.8334, 0.8791, 0.8777, 0.8736, 0.8987, 0.8401, 0.8618,\n",
      "        0.7983, 0.8627, 0.8933, 0.8764, 0.7962, 0.8502, 0.8755, 0.8680, 0.8051,\n",
      "        0.8403, 0.7528, 0.8400, 0.8171, 0.7393, 0.7880, 0.7982, 0.8491, 0.8180,\n",
      "        0.7542, 0.8392, 0.7935, 0.7821, 0.8689, 0.8121, 0.8717, 0.8074, 0.7300,\n",
      "        0.7703, 0.9228, 0.7964, 0.8053, 0.8403, 0.8694, 0.8544, 0.7474, 0.8244,\n",
      "        0.8529, 0.8174, 0.8006, 0.8451, 0.9096, 0.8334, 0.8603, 0.8288, 0.8595,\n",
      "        0.8513, 0.8241, 0.8387, 0.7908, 0.8525, 0.7837, 0.7803, 0.8755, 0.8537,\n",
      "        0.7834, 0.8043, 0.7114, 0.7878, 0.8242, 0.7866, 0.7796, 0.8351, 0.8923,\n",
      "        0.8236, 0.8335, 0.8933, 0.8200, 0.7647, 0.8362, 0.7864, 0.8011, 0.9132,\n",
      "        0.8127, 0.8728, 0.8181, 0.8005, 0.8268, 0.8181, 0.8178, 0.8092, 0.8571,\n",
      "        0.8283, 0.8543, 0.8434, 0.7961, 0.8637, 0.8499, 0.7304, 0.8047, 0.7929,\n",
      "        0.8762, 0.8035, 0.9016, 0.8823, 0.7172, 0.8587, 0.7966, 0.8824, 0.8976,\n",
      "        0.8129, 0.8565, 0.8010, 0.7947, 0.8766, 0.8497, 0.8542, 0.7666, 0.7894,\n",
      "        0.8424, 0.8681, 0.8573, 0.9176, 0.8383, 0.8636, 0.8612, 0.9353, 0.8505,\n",
      "        0.8138, 0.9061, 0.6144, 0.8552, 0.8251, 0.7582, 0.8132, 0.8204, 0.8560,\n",
      "        0.7999, 0.8871, 0.8531, 0.8443, 0.7255, 0.8467, 0.8181, 0.8060, 0.8861,\n",
      "        0.8707, 0.8313, 0.7747, 0.8537, 0.8461, 0.8346, 0.7796, 0.6952, 0.7939,\n",
      "        0.9130, 0.9183, 0.8161, 0.8095, 0.8728, 0.8895, 0.8295, 0.7896, 0.9133,\n",
      "        0.7711, 0.8350, 0.7630, 0.7445, 0.8752, 0.8700, 0.8233, 0.9533, 0.8351,\n",
      "        0.8709, 0.8997, 0.7026, 0.8592, 0.8451, 0.8210, 0.8102, 0.7948, 0.8165,\n",
      "        0.8504, 0.7895, 0.7816, 0.8117, 0.8293, 0.8808, 0.8025, 0.8145, 0.8593,\n",
      "        0.7861, 0.8604, 0.8284, 0.8838, 0.8390, 0.9301, 0.8983, 0.8570, 0.8159,\n",
      "        0.8291, 0.9241, 0.8437, 3.4025, 0.8622, 0.8977, 0.8531, 0.8417, 0.8489,\n",
      "        0.9048, 0.8595, 0.8661, 0.8051, 0.8298, 0.8414, 0.8068, 0.8558, 0.8749,\n",
      "        0.8263, 0.8196, 0.8555, 0.8793, 0.8215, 0.7524, 0.7894, 0.8324, 0.7785,\n",
      "        0.7926, 0.8605, 0.8168, 0.8578, 0.8750, 0.8242, 0.8330, 0.9023, 0.8560,\n",
      "        0.8105, 0.7883, 0.7978, 0.8335, 0.8215, 0.7734, 0.8058, 0.7318, 0.8638,\n",
      "        0.8398, 0.8118, 0.8254, 0.8009, 0.8072, 0.8383, 0.8164, 0.8269, 0.8372,\n",
      "        0.8352, 0.8031, 0.8644, 0.9068, 0.8410, 0.7822, 0.8493, 0.8297, 0.8099,\n",
      "        0.8726, 0.7960, 0.7305, 0.8213, 0.9007, 0.9344, 0.8041, 0.8674, 0.8086,\n",
      "        0.8351, 0.9016, 0.9196, 0.8307, 0.9276, 0.8836, 0.8135, 0.8452, 0.8368,\n",
      "        0.8793, 0.8405, 0.7980, 0.8582, 0.8969, 0.8847, 0.7909, 0.8089, 0.8608,\n",
      "        0.7822, 0.7908, 0.9202, 0.7465, 0.8523, 0.8637, 0.7618, 0.8501, 0.7981,\n",
      "        0.9070, 0.8185, 0.8287, 0.8591, 0.8356, 0.7556, 0.8299, 0.8172, 0.8345,\n",
      "        0.8250, 0.8551, 0.7645, 0.8471, 0.8827, 0.8612, 0.8263, 0.9176, 0.8656,\n",
      "        0.7951, 0.7627, 0.7865, 0.9047, 0.7967, 0.8391, 0.8771, 0.7732, 0.8569,\n",
      "        0.7854, 0.8818, 0.8078, 0.8280, 0.8746, 0.8532, 0.7675, 0.7908, 0.8141,\n",
      "        0.8202, 0.8285, 0.8988, 0.8489, 0.7125, 0.8795, 0.8200, 0.8836, 0.7692,\n",
      "        0.8953, 0.8182, 0.8134, 0.8332, 0.8025, 0.8398, 0.8481, 0.7610, 0.7973,\n",
      "        0.7827, 0.8684, 0.8341, 0.8832, 0.7954, 0.8414, 0.8262, 0.7873, 0.8964,\n",
      "        0.7905, 0.7861, 0.8404, 0.8136, 0.8673, 0.8462, 0.8371, 0.7794, 0.8470,\n",
      "        0.8483, 0.8981, 0.8013, 0.8592, 0.8954, 0.8770, 0.8335, 0.8438, 0.8172,\n",
      "        0.8766, 0.8461, 0.8540, 0.8068, 0.8132, 0.9077, 0.8307, 0.8664, 0.7748,\n",
      "        0.8673, 0.8287, 0.8649, 0.7932, 0.8382, 0.8437, 0.7486, 0.8215, 0.8351,\n",
      "        0.8260, 0.7630, 0.9100, 0.8811, 0.8021, 0.8530, 0.8757, 0.7984, 0.8577,\n",
      "        0.8063, 0.9532, 0.8885, 0.7906, 0.8843, 0.8951, 0.8479, 0.8964, 0.8463,\n",
      "        0.7880, 0.8156, 0.8228, 0.8268, 0.8376, 0.8557, 0.7977, 0.8970, 0.8879,\n",
      "        0.8272, 0.8630, 0.8020, 0.9120, 0.7860, 0.8591, 0.8219, 0.8207, 0.8158,\n",
      "        0.8674, 0.7504, 0.7055, 0.8135, 0.7805, 0.9132, 0.8418, 0.8258, 0.8220,\n",
      "        0.8092, 0.8873, 0.8419, 0.8420, 0.8269, 0.8598, 0.8256, 0.7964, 0.8981,\n",
      "        0.8372, 0.7730, 0.8037, 0.8646, 0.8230, 0.7974, 0.8706, 0.8562, 0.7691,\n",
      "        0.8452, 0.8311, 0.8114, 0.8247, 0.7928, 0.8806, 0.8628, 0.8399, 0.8610,\n",
      "        0.7891, 0.8500, 0.9076, 0.8322, 0.8328, 0.8067, 0.7474, 0.8107, 0.8018,\n",
      "        0.8562, 0.8479, 0.8404, 0.8762, 0.9094, 0.9014, 0.9081, 0.8485, 0.7985,\n",
      "        0.8785, 0.8346, 0.8532, 0.8013, 0.7661, 0.8480, 0.8435, 0.8264, 0.8738,\n",
      "        0.8541, 0.8386, 0.8279, 0.8713, 0.8745, 0.8828, 0.9309, 0.8279, 0.8401,\n",
      "        0.8710, 0.8519, 0.7929, 0.8608, 0.8962, 0.8455, 0.8347, 0.7980, 0.8267,\n",
      "        0.8964, 0.8296, 0.8335, 0.8335, 0.7587, 0.7868, 0.7578, 0.9360, 0.8573,\n",
      "        0.7926, 0.8074, 0.8578, 0.7929, 0.8097, 0.7944, 0.9015, 0.8594, 0.7642,\n",
      "        0.8223, 0.8303, 0.8192, 0.7780, 0.8906, 0.8487, 0.8197, 0.8667, 0.8022,\n",
      "        0.6826, 0.8482, 0.8942, 0.8156, 0.8485, 0.8896, 0.8325, 0.8286, 0.8003,\n",
      "        0.8506, 0.8416, 0.8493, 0.8530, 0.8996, 0.8223, 0.7813, 0.8961, 0.8402,\n",
      "        0.8854, 0.7511, 0.8210, 0.8531, 0.8829, 0.8752, 0.7644, 0.8361, 0.8323,\n",
      "        0.8168, 0.8641, 0.8670, 0.9012, 0.8673, 0.7959, 0.8590, 0.8414, 0.8077,\n",
      "        0.8279, 0.8624, 0.8288, 0.7537, 0.8382, 0.8065, 0.8248, 0.8501, 0.8324,\n",
      "        0.7729, 0.8069, 0.8780], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.1571, -0.1310, -0.1239,  ..., -0.1493, -0.0176,  0.0253],\n",
      "         [-0.0178, -0.1024, -0.0880,  ..., -0.1479, -0.1144, -0.0297],\n",
      "         [-0.0122, -0.0061, -0.1499,  ..., -0.1385, -0.0172, -0.0012],\n",
      "         ...,\n",
      "         [-0.0492, -0.0957, -0.0816,  ...,  0.3573, -0.0999, -0.0260],\n",
      "         [-0.0263,  0.1129, -0.0557,  ...,  0.1425, -0.0962, -0.0142],\n",
      "         [-0.0264, -0.1248, -0.0213,  ...,  0.0263, -0.0963, -0.0387]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.8203,  0.5189, -1.0328,  ..., -0.2933,  0.4075,  0.5765],\n",
      "         [-0.7625, -1.0912, -0.1534,  ...,  0.0975, -0.2056, -0.7339],\n",
      "         [ 0.0950, -1.2515,  0.0804,  ...,  0.3938,  0.1743, -0.0889],\n",
      "         ...,\n",
      "         [-0.8641, -0.5202, -0.2260,  ..., -0.4962, -0.2556,  0.3251],\n",
      "         [-0.8583, -0.3209,  0.3757,  ..., -0.0717, -0.1641, -0.0800],\n",
      "         [-0.6929, -0.6796,  0.4471,  ...,  0.3612, -0.2246, -0.4955]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[ 1.6017e-02,  8.8447e-02,  2.1283e-01,  ...,  1.5139e-01,\n",
      "           3.8974e-01, -1.1154e-01],\n",
      "         [ 5.8657e-02, -1.7832e-01, -6.6836e-02,  ...,  6.2756e-05,\n",
      "           4.5781e-01, -2.2193e-01],\n",
      "         [-4.2919e-03, -8.0466e-02, -1.3849e-01,  ..., -8.8581e-04,\n",
      "           4.4586e-01, -9.8381e-02],\n",
      "         ...,\n",
      "         [ 7.2335e-02, -2.5942e-01,  5.5330e-02,  ...,  9.3046e-02,\n",
      "           1.8961e-02, -7.2386e-02],\n",
      "         [ 4.7062e-02, -2.7236e-01, -2.1170e-02,  ...,  1.2451e-01,\n",
      "           9.4301e-02, -7.2067e-02],\n",
      "         [ 3.0744e-02, -2.2472e-01, -2.9123e-02,  ...,  1.5160e-01,\n",
      "           5.3456e-02, -1.6505e-02]]], grad_fn=<AddBackward0>) tensor([[[-0.8203,  0.5189, -1.0328,  ..., -0.2933,  0.4075,  0.5765],\n",
      "         [-0.7625, -1.0912, -0.1534,  ...,  0.0975, -0.2056, -0.7339],\n",
      "         [ 0.0950, -1.2515,  0.0804,  ...,  0.3938,  0.1743, -0.0889],\n",
      "         ...,\n",
      "         [-0.8641, -0.5202, -0.2260,  ..., -0.4962, -0.2556,  0.3251],\n",
      "         [-0.8583, -0.3209,  0.3757,  ..., -0.0717, -0.1641, -0.0800],\n",
      "         [-0.6929, -0.6796,  0.4471,  ...,  0.3612, -0.2246, -0.4955]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-1.1730,  0.7810, -0.9617,  ..., -0.1999,  0.9844,  0.6110],\n",
      "         [-0.8749, -1.4603, -0.1429,  ...,  0.0767,  0.2267, -1.1511],\n",
      "         [ 0.0396, -1.4863,  0.0326,  ...,  0.3701,  0.6021, -0.2508],\n",
      "         ...,\n",
      "         [-0.8690, -0.8077, -0.0645,  ..., -0.3921, -0.2671,  0.2275],\n",
      "         [-0.8213, -0.5742,  0.4097,  ...,  0.0239, -0.1024, -0.1782],\n",
      "         [-0.6824, -0.8607,  0.4657,  ...,  0.4136, -0.1920, -0.5212]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.8203,  0.5189, -1.0328,  ..., -0.2933,  0.4075,  0.5765],\n",
      "         [-0.7625, -1.0912, -0.1534,  ...,  0.0975, -0.2056, -0.7339],\n",
      "         [ 0.0950, -1.2515,  0.0804,  ...,  0.3938,  0.1743, -0.0889],\n",
      "         ...,\n",
      "         [-0.8641, -0.5202, -0.2260,  ..., -0.4962, -0.2556,  0.3251],\n",
      "         [-0.8583, -0.3209,  0.3757,  ..., -0.0717, -0.1641, -0.0800],\n",
      "         [-0.6929, -0.6796,  0.4471,  ...,  0.3612, -0.2246, -0.4955]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-8.9634e-02, -6.3622e-02,  6.7667e-02, -9.8957e-02,  8.3189e-02,\n",
      "         9.3695e-02,  1.6163e-01,  1.6849e-01, -1.2009e-01, -1.6283e-01,\n",
      "         1.2824e-02,  1.3595e-02, -2.0002e-01,  6.5053e-02,  3.3547e-03,\n",
      "         2.4297e-01,  2.0000e-01,  6.8249e-02, -8.8831e-02, -4.7633e-02,\n",
      "         1.9888e-01, -2.2325e-02, -3.4413e-02,  9.2640e-02,  3.4294e-03,\n",
      "         8.9293e-02,  6.9186e-02, -1.2650e-01, -9.4288e-02, -3.3893e-02,\n",
      "         1.3517e-01, -4.3623e-04,  1.3734e-01, -1.4416e-01, -8.7846e-02,\n",
      "         5.1723e-02, -1.6302e-01, -8.8349e-02, -4.6989e-02, -1.6174e-01,\n",
      "        -9.8409e-02, -1.7043e-01, -8.8724e-02,  8.7026e-02,  3.2200e-02,\n",
      "        -1.7585e-02,  1.5776e-01,  8.5899e-02, -3.0859e-02, -2.2616e-02,\n",
      "        -2.9177e-01, -3.1198e-02,  1.9825e-01, -7.7385e-02,  9.8072e-02,\n",
      "         1.2666e-01, -5.6339e-02, -1.3905e-01, -1.5887e-01, -1.5905e-01,\n",
      "        -6.4038e-02,  8.5224e-02, -3.4700e-02, -1.0963e-01,  1.3282e-02,\n",
      "         1.4645e-01, -3.2465e-02,  1.9734e-02, -1.8064e-01, -5.0230e-02,\n",
      "        -9.6340e-02,  6.3890e-02, -9.1495e-02, -6.3184e-02,  1.0972e-01,\n",
      "        -2.0805e-02, -1.2959e-01,  6.1818e-02,  4.8875e-02,  3.1672e-02,\n",
      "        -3.2338e-02,  9.3389e-02, -3.8628e-02,  8.0285e-02, -1.3373e-02,\n",
      "        -1.1157e-01,  6.8738e-02, -1.0489e-01, -1.8389e-01,  1.1481e-01,\n",
      "        -1.2556e-01, -1.4697e-01,  8.5729e-02, -8.7324e-02,  7.8940e-02,\n",
      "        -1.0211e-02,  8.6786e-02, -5.9400e-02,  5.5322e-02, -1.2584e-01,\n",
      "        -1.1083e-01, -8.0483e-02,  1.0781e-01, -7.5506e-02, -8.0584e-02,\n",
      "         3.6466e-02, -8.5659e-02, -1.7817e-01, -1.0515e-01,  3.6775e-02,\n",
      "        -7.7444e-02, -1.4709e-02, -6.7841e-02, -1.7277e-01, -7.6749e-02,\n",
      "         4.1880e-02, -3.5353e-02, -1.0811e-01, -7.8314e-02, -5.6127e-02,\n",
      "        -5.6506e-02, -7.9387e-02,  2.3372e-03,  1.0449e+00,  2.5767e-02,\n",
      "        -2.7539e-01, -3.5457e-02,  6.5390e-02, -1.1383e-01,  2.4931e-02,\n",
      "        -8.7080e-03,  2.5219e-02, -7.1286e-02, -7.8882e-02, -1.6375e-01,\n",
      "        -2.0500e-02,  2.7776e-02, -6.5971e-02, -7.1774e-02, -8.0701e-02,\n",
      "        -1.0329e-01,  6.0130e-03, -9.9378e-03,  1.0897e-01,  1.4376e-01,\n",
      "        -5.2824e-02, -8.3945e-02, -2.7456e-01, -1.0571e-01,  8.9165e-02,\n",
      "        -6.2144e-02,  1.1374e-01, -9.9546e-02, -5.9904e-02, -1.2443e-01,\n",
      "         5.3822e-02, -1.3767e-02,  2.7864e-02,  2.8030e-02, -1.6972e-01,\n",
      "        -1.9637e-02, -1.0432e-01, -4.1808e-02, -1.2087e-02,  1.8580e-01,\n",
      "         4.5931e-02,  7.6061e-02,  1.0496e-01, -1.2660e-01, -4.2139e-02,\n",
      "        -8.2277e-02, -1.4728e-01, -6.5631e-02, -8.0154e-02, -3.5748e-02,\n",
      "         5.3958e-02,  9.5627e-02, -2.0282e-02,  4.9823e-02,  5.4665e-02,\n",
      "        -4.5130e-01,  1.0649e-01, -3.7907e-02, -4.4066e-02,  2.2208e-02,\n",
      "        -5.1547e-03,  2.8113e-01, -6.6522e-02,  9.2171e-02,  1.7494e-02,\n",
      "        -1.4836e-01, -3.4533e-03,  1.4114e-01, -1.4549e-01,  2.6293e-02,\n",
      "        -1.0252e-01, -9.1711e-02, -4.4933e-02, -1.8692e-02, -2.1993e-02,\n",
      "         3.2264e-03,  1.3967e-02, -2.8481e-02, -1.4913e-01, -9.1051e-02,\n",
      "         3.9888e-02, -4.8912e-02, -9.4883e-02,  1.3721e-02,  3.3163e-02,\n",
      "        -1.6499e-01, -3.9790e-02, -4.5493e-03,  1.1105e-01, -2.9148e-02,\n",
      "        -4.1659e-02, -2.0134e-01, -4.9231e-02,  1.1906e-01, -1.3337e-01,\n",
      "         1.0928e-02,  1.5222e-01, -7.5770e-02,  5.1390e-02, -4.5789e-02,\n",
      "         4.1191e-01,  5.1656e-02, -1.7002e-01,  9.6573e-02, -6.4475e-02,\n",
      "        -1.6274e-01, -7.3911e-02,  3.2534e-01, -1.3640e-01, -9.1051e-02,\n",
      "        -3.9113e-02, -5.1964e-02, -1.4728e-02,  9.5567e-02, -3.0453e-01,\n",
      "         4.9397e-02,  7.7281e-02, -2.0980e-01,  1.3360e-01, -5.4493e-02,\n",
      "        -9.2164e-02,  6.1992e-02,  3.9270e-03, -1.0176e-01, -7.1568e-02,\n",
      "        -1.9041e-01,  1.4594e-01, -4.9409e-02, -1.0839e-01,  1.1297e-02,\n",
      "        -9.1030e-02, -6.3892e-02, -1.3601e-01, -4.7126e-02,  1.0431e-01,\n",
      "         1.2021e-01, -7.2285e-03, -4.1923e-02, -3.0529e-02, -2.1581e-02,\n",
      "        -2.4533e-01,  4.8957e-03, -1.7248e-04,  3.1634e-02,  2.7645e-02,\n",
      "        -2.6671e-02,  5.8838e-02, -1.2700e-01,  4.0932e-02, -4.4434e-02,\n",
      "        -8.9510e-02, -8.4061e-05, -1.8936e-03, -7.6442e-02, -8.5867e-02,\n",
      "         3.6410e-03,  1.0914e-01, -3.5020e-02, -1.0355e-01, -2.8205e-01,\n",
      "        -6.2289e-02,  1.5333e-01, -5.0480e-02, -3.2438e-02, -1.1360e-01,\n",
      "        -6.3252e-02, -4.3177e-03, -1.8378e-01, -2.8954e-02,  1.8174e-01,\n",
      "         3.2420e-02, -7.2692e-02,  8.3667e-03,  1.1529e-01, -6.7861e-02,\n",
      "        -1.1008e-01,  1.2941e-01, -1.4846e-02,  7.2553e-02,  8.9079e-02,\n",
      "        -4.6507e-02, -1.0127e-01, -1.3501e-01, -1.0327e+00, -3.1622e-03,\n",
      "        -1.5172e-01, -1.0368e-01,  1.2802e-01,  7.5823e-02, -1.3481e-01,\n",
      "         8.8376e-02, -4.8903e-02, -6.7820e-02, -5.3022e-02, -8.6679e-02,\n",
      "        -2.7180e-02,  1.6691e-01,  6.1938e-02, -2.7889e-01, -7.9135e-02,\n",
      "        -1.1899e-01, -1.4626e-02,  1.8332e-01, -2.5914e-02, -8.6856e-02,\n",
      "         6.8072e-02,  7.8954e-02,  5.3224e-02,  1.7055e-01, -5.1558e-02,\n",
      "        -1.0577e-01, -1.4009e-01, -2.7469e-02,  4.7304e-03, -1.3622e-01,\n",
      "         1.2525e-02,  1.0520e-01,  6.8147e-02,  5.0730e-02, -4.0849e-02,\n",
      "        -1.7330e-01, -1.3342e-02, -2.2979e-01, -9.0664e-02, -2.6308e-01,\n",
      "        -6.1537e-02, -1.0180e-01,  2.2991e-01, -7.1487e-02, -1.7504e-02,\n",
      "        -6.6763e-02, -1.0061e-01,  1.2692e-01,  2.4475e-02,  6.2756e-03,\n",
      "        -1.8025e-02, -2.1105e-01,  1.1078e-01,  1.7901e-02,  5.7091e-02,\n",
      "         1.5526e-02, -1.0141e-01,  5.2689e-03, -1.6347e-01, -1.0707e-01,\n",
      "        -2.7559e-01, -5.2087e-02, -2.8723e-02, -2.0108e-01, -2.6397e-01,\n",
      "        -2.1626e-02,  1.1753e-01, -1.6254e-01, -2.1556e-01,  1.0447e-02,\n",
      "        -6.2737e-02, -1.4833e+00, -1.1046e-01, -1.7192e-01,  2.4766e-01,\n",
      "        -5.0067e-02, -9.2092e-02,  7.8533e-02, -5.4513e-02, -1.7551e-01,\n",
      "        -1.3987e-01, -8.4708e-03,  3.1370e-03, -1.7431e-01, -1.0219e-01,\n",
      "        -6.1529e-02, -1.8624e-01, -1.5332e-01,  1.7929e-01, -7.5322e-02,\n",
      "        -3.3790e-02, -1.2696e-01,  9.9250e-02, -9.6178e-02, -1.3553e-01,\n",
      "        -5.8228e-02,  2.9613e-02, -1.9149e-01,  8.2919e-02, -6.0430e-02,\n",
      "        -5.1517e-02,  7.5239e-03, -1.8235e-02, -8.7784e-02, -4.8428e-02,\n",
      "         5.1432e-02,  2.6579e-02, -1.2016e-01, -9.3306e-03, -2.8217e-02,\n",
      "        -2.0247e-02, -7.8200e-02,  2.2541e-02, -1.4143e-01, -1.0894e-02,\n",
      "         1.6573e-02, -1.0229e-01, -1.9638e-01, -1.4054e-01,  8.7213e-02,\n",
      "        -1.3702e-01, -1.5208e-01, -7.6166e-02,  9.1434e-02, -8.1207e-02,\n",
      "         3.4009e-02,  4.7558e-02, -1.8234e-01, -3.5899e-02, -5.7347e-02,\n",
      "         5.4107e-02,  1.4793e-02,  1.0430e-02, -1.7031e-01, -1.0464e-01,\n",
      "        -3.5042e-01, -1.9158e-01,  9.2418e-02,  3.7559e-02, -9.7405e-02,\n",
      "         6.1124e-02, -1.1003e-01,  1.3475e-01, -1.1100e-01,  1.8244e-01,\n",
      "         1.1706e-01, -4.4486e-02,  5.5285e-02, -3.4761e-01, -2.7539e-01,\n",
      "        -1.7336e-02, -1.7760e-01,  1.8534e-02,  1.2154e-01,  2.1128e-01,\n",
      "        -8.5735e-02,  1.3856e-01, -3.2358e-01, -3.3461e-03, -5.2334e-02,\n",
      "         1.5147e-02, -6.4845e-02, -9.5687e-02,  1.0531e-01, -1.1930e-01,\n",
      "        -1.6826e-01, -6.2170e-02,  3.8616e-02, -1.6144e-01, -7.9383e-02,\n",
      "        -4.7872e-02, -6.3124e-02, -1.1797e-01,  9.5356e-02, -1.5895e-01,\n",
      "        -1.1327e-01, -4.9932e-02,  1.3097e-01, -2.3225e-02, -4.4876e-02,\n",
      "        -1.6668e-01, -1.0864e-01,  2.1689e-03,  1.7966e-02, -4.6334e-02,\n",
      "        -1.0440e-01, -1.7442e-01, -2.9528e-02,  1.8975e-02,  1.1198e-01,\n",
      "         4.1941e-02,  1.1200e-01, -8.8794e-02, -1.4606e-01, -4.5993e-02,\n",
      "        -3.4797e-03, -1.5252e-01,  4.3101e-02, -2.1567e-01, -7.5942e-02,\n",
      "        -1.0493e-01, -6.9546e-02, -3.4403e-02, -9.2701e-02, -3.4303e-02,\n",
      "         1.7153e-01, -2.0395e-02, -1.1249e-01, -1.9277e-01,  1.5488e-02,\n",
      "        -5.1233e-02,  6.1843e-02,  3.2551e-02, -8.7959e-02, -1.8635e-02,\n",
      "        -1.0520e-01, -1.0839e-01, -5.9517e-02,  1.5896e-02,  3.5751e-02,\n",
      "         1.3413e-02, -3.1133e-02, -1.4934e-01, -9.0302e-03,  4.5739e-02,\n",
      "        -1.4792e-01, -2.0007e-02, -1.3493e-01,  1.0787e-02, -3.3816e-01,\n",
      "         3.1175e-02, -4.6899e-02,  4.3312e-02, -6.6313e-02, -1.5312e-01,\n",
      "        -1.0540e-01, -9.8326e-02, -4.7153e-03,  1.3070e-01,  8.9454e-02,\n",
      "        -9.8902e-02, -9.2686e-02,  1.0269e-01, -2.8549e-02,  6.6661e-03,\n",
      "        -5.3918e-02, -7.3896e-02,  1.2809e-01, -1.2009e-02, -8.5162e-02,\n",
      "        -1.2883e-01, -8.3677e-02,  1.4764e-01, -1.0634e-01, -1.3061e-01,\n",
      "         1.0886e-02, -1.9140e-02,  5.7075e-02, -5.2589e-02,  2.6397e-02,\n",
      "        -1.2699e-01, -4.0904e-03,  1.5376e-01, -2.3331e-02, -1.1140e-01,\n",
      "         5.7679e-02,  3.1887e-02, -2.9850e-01,  2.0528e-02, -2.6509e-01,\n",
      "        -7.3115e-02,  5.6351e-02,  6.2625e-02,  2.8202e-02, -1.5173e-01,\n",
      "        -1.3854e-01,  1.9960e-01, -7.8035e-02, -1.3321e-01, -6.6651e-02,\n",
      "        -6.6570e-02, -1.0854e-01,  2.9537e-02, -6.8222e-02, -4.1412e-02,\n",
      "         5.0937e-02, -1.3486e-01, -3.0162e-02,  2.1738e-02,  6.1227e-02,\n",
      "        -1.3544e-04, -5.1528e-02, -1.2112e-01,  1.0918e-01,  5.4059e-02,\n",
      "         1.4152e-01, -2.2390e-02,  1.9856e-02, -2.0093e-01,  7.6524e-03,\n",
      "         5.3075e-02, -1.1348e-01, -5.5725e-02,  2.0508e-02, -3.6909e-02,\n",
      "        -1.3771e-01, -1.8553e-01, -4.8380e-02, -1.0147e-01, -1.7881e-01,\n",
      "         1.9155e-01, -1.5777e-02,  4.4207e-02, -2.8531e-02,  3.4955e-03,\n",
      "         6.8470e-02,  1.5250e-01, -6.3887e-02,  1.6214e-02,  1.7061e-02,\n",
      "         1.2607e-01, -5.5609e-03,  1.2198e-01,  2.5293e-01, -3.7000e-02,\n",
      "         1.1397e-02,  1.0501e-01,  3.7040e-02,  1.0137e-01, -1.6220e-02,\n",
      "        -1.1710e-01,  2.0133e-01, -1.5687e-01, -2.4239e-01, -4.3420e-02,\n",
      "        -3.1539e-01,  8.1076e-03,  2.2814e-01,  7.4322e-02,  1.9125e-01,\n",
      "         1.0501e-01,  7.0834e-02,  1.4475e-01,  8.2203e-02, -6.9037e-02,\n",
      "        -1.0078e-01,  8.2504e-02,  1.0292e-01, -9.9707e-03, -1.0356e-01,\n",
      "         2.0269e-02, -8.9988e-02,  9.3030e-02, -7.7161e-02, -9.7605e-03,\n",
      "         1.4462e-02, -3.4608e-02,  2.0389e-01,  1.3810e-01,  1.7598e-01,\n",
      "         1.2853e-01, -5.7037e-02, -1.6307e-01, -6.9622e-02,  1.8726e-01,\n",
      "        -9.3597e-02,  7.7707e-02,  2.6425e-02, -3.1967e-02, -6.2175e-02,\n",
      "         2.8677e-02, -8.3913e-02,  7.2417e-02, -6.8797e-02, -3.4197e-02,\n",
      "        -4.2399e-02,  3.0505e-02,  1.4990e-01, -1.6313e-01, -3.1918e-02,\n",
      "        -8.8834e-02, -9.3050e-02, -5.1629e-02,  9.4387e-05,  4.2220e-02,\n",
      "        -2.3819e-02, -2.0130e-01, -6.7952e-02, -6.0981e-02,  6.2493e-02,\n",
      "         1.7185e-01, -8.5870e-03, -1.0499e-01, -7.3440e-03,  1.7131e-01,\n",
      "         3.7926e-02,  2.7036e-01,  7.5689e-02, -1.9814e-02,  3.0447e-02,\n",
      "        -9.0346e-02,  7.0207e-02,  5.3655e-02,  4.0548e-02, -9.5520e-02,\n",
      "        -1.1084e-02, -2.1027e-01,  2.4161e-02,  4.7274e-02, -4.2653e-02,\n",
      "        -6.7483e-01, -1.2850e-02, -9.2719e-02,  5.6161e-02,  9.8158e-02,\n",
      "        -1.5418e-01, -8.3470e-02, -1.3492e-01, -8.9307e-02, -6.8725e-02,\n",
      "         4.8085e-02,  1.1932e-02, -8.5937e-02,  7.0960e-02,  2.3639e-01,\n",
      "        -6.3271e-02, -1.0931e-01,  2.1265e-01, -1.1311e-01,  5.1169e-02,\n",
      "        -3.5789e-02, -2.7773e-02, -2.3312e-01, -1.4190e-01,  4.1386e-02,\n",
      "         7.7668e-02,  1.8668e-02, -2.0945e-01,  6.8989e-02,  7.8100e-02,\n",
      "        -8.4330e-02, -8.3940e-02, -7.9275e-02,  1.5135e-01, -1.6493e-01,\n",
      "        -4.6172e-02, -2.9566e-01,  6.8822e-02, -8.8592e-02, -1.5561e-01,\n",
      "         6.2503e-02, -1.3773e-01,  5.5092e-02, -1.6108e-01, -1.6780e-01,\n",
      "        -5.0354e-02, -7.1745e-02, -6.7072e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8510, 0.8257, 0.7928, 0.7660, 0.8913, 0.8281, 0.8445, 0.8589, 0.8335,\n",
      "        0.8318, 0.7619, 0.7584, 0.7800, 0.7981, 0.8150, 0.8331, 0.8130, 0.7681,\n",
      "        0.8971, 0.8065, 0.8500, 0.7828, 0.8105, 0.8420, 0.7675, 0.8296, 0.7695,\n",
      "        0.8437, 0.7941, 0.7906, 0.8720, 0.7707, 0.8293, 0.8442, 0.8368, 0.8053,\n",
      "        0.8500, 0.8071, 0.8720, 0.8020, 0.8177, 0.8587, 0.8598, 0.8547, 0.7876,\n",
      "        0.7641, 0.8262, 0.8296, 0.8399, 0.8436, 0.8437, 0.8228, 0.8427, 0.7761,\n",
      "        0.7994, 0.8011, 0.7898, 0.7825, 0.7900, 0.7748, 0.8243, 0.7830, 0.7415,\n",
      "        0.8101, 0.8179, 0.8152, 0.8171, 0.8370, 0.8315, 0.8333, 0.8294, 0.8526,\n",
      "        0.8107, 0.7847, 0.8414, 0.7678, 0.8014, 0.7884, 0.7709, 0.7842, 0.7995,\n",
      "        0.8541, 0.8626, 0.7935, 0.7421, 0.7844, 0.8210, 0.8014, 0.8193, 0.8523,\n",
      "        0.8044, 0.7955, 0.7774, 0.7874, 0.8264, 0.7873, 0.7921, 0.7779, 0.8087,\n",
      "        0.8809, 0.8706, 0.8710, 0.8334, 0.8159, 0.7753, 0.7403, 0.7870, 0.8629,\n",
      "        0.8031, 0.8052, 0.8132, 0.7908, 0.7825, 0.8076, 0.7931, 0.7665, 0.7964,\n",
      "        0.8168, 0.8174, 0.7986, 0.7972, 0.7374, 0.7908, 1.5736, 0.7438, 0.8114,\n",
      "        0.8175, 0.7729, 0.8241, 0.7774, 0.8176, 0.7696, 0.7618, 0.7936, 0.7794,\n",
      "        0.8420, 0.8064, 0.7897, 0.8098, 0.7232, 0.8446, 0.8544, 0.8349, 0.9250,\n",
      "        0.7950, 0.7515, 0.8202, 0.8211, 0.8191, 0.8058, 0.7866, 0.8170, 0.7736,\n",
      "        0.8333, 0.8139, 0.7869, 0.7910, 0.7895, 0.8322, 0.7710, 0.8215, 0.8464,\n",
      "        0.7851, 0.8645, 0.8320, 0.8428, 0.8414, 0.8195, 0.7995, 0.8295, 0.7840,\n",
      "        0.8225, 0.7822, 0.7974, 0.8703, 0.7734, 0.7586, 0.7917, 0.7643, 0.7986,\n",
      "        1.1452, 0.8621, 0.7994, 0.8423, 0.8737, 0.8176, 0.8991, 0.8092, 0.8371,\n",
      "        0.8004, 0.8279, 0.8419, 0.8627, 0.7956, 0.8486, 0.8536, 0.8331, 0.7776,\n",
      "        0.8259, 0.7628, 0.8196, 0.8029, 0.7470, 0.7574, 0.7778, 0.8815, 0.7860,\n",
      "        0.7648, 0.8167, 0.7715, 0.7660, 0.8203, 0.7959, 0.8472, 0.7968, 0.7602,\n",
      "        0.7818, 0.8744, 0.7962, 0.7917, 0.8197, 0.8365, 0.8365, 0.7572, 0.7966,\n",
      "        1.2110, 0.8171, 0.7959, 0.8326, 0.8835, 0.8224, 0.8375, 0.8334, 0.8147,\n",
      "        0.8302, 0.7990, 0.8172, 0.7980, 0.8131, 0.7872, 0.7525, 0.8322, 0.8247,\n",
      "        0.8071, 0.7800, 0.7475, 0.7993, 0.8181, 0.7804, 0.7601, 0.8367, 0.8880,\n",
      "        0.7911, 0.7993, 0.8491, 0.7950, 0.7475, 0.8268, 0.7655, 0.7931, 0.8672,\n",
      "        0.7960, 0.8240, 0.7914, 0.8019, 0.8155, 0.8145, 0.7911, 0.7961, 0.8460,\n",
      "        0.8038, 0.8091, 0.7963, 0.7739, 0.8235, 0.8404, 0.7353, 0.7868, 0.7713,\n",
      "        0.8257, 0.7760, 0.8650, 0.8396, 0.7547, 0.8330, 0.7613, 0.8619, 0.8795,\n",
      "        0.7843, 0.8174, 0.7860, 0.8342, 0.8418, 0.8442, 0.8352, 0.7675, 0.7637,\n",
      "        0.7934, 0.8304, 0.8057, 0.8740, 0.8356, 0.8312, 0.7938, 0.9066, 0.8161,\n",
      "        0.7882, 0.8436, 0.6058, 0.7900, 0.8079, 0.7783, 0.8306, 0.7776, 0.8318,\n",
      "        0.7522, 0.8328, 0.7851, 0.8060, 0.7278, 0.7904, 0.8447, 0.7845, 0.8577,\n",
      "        0.8094, 0.8080, 0.7544, 0.8361, 0.7872, 0.7987, 0.7649, 0.7356, 0.7803,\n",
      "        0.8844, 0.8541, 0.7947, 0.8072, 0.8383, 0.8543, 0.7873, 0.7751, 0.9416,\n",
      "        0.7676, 0.7911, 0.7594, 0.7603, 0.8573, 0.8636, 0.8009, 0.9210, 0.8014,\n",
      "        0.8343, 0.8710, 0.7355, 0.8167, 0.8286, 0.8040, 0.7774, 0.7924, 0.7905,\n",
      "        0.8173, 0.7930, 0.7894, 0.7766, 0.8055, 0.8762, 0.7797, 0.7965, 0.8487,\n",
      "        0.7506, 0.8332, 0.7838, 0.8682, 0.8132, 0.9272, 0.8566, 0.8289, 0.8118,\n",
      "        0.8008, 0.8701, 0.8313, 2.5250, 0.8217, 0.8654, 0.8540, 0.8199, 0.8196,\n",
      "        0.8356, 0.8108, 0.8126, 0.7997, 0.7916, 0.8042, 0.7916, 0.8182, 0.8213,\n",
      "        0.8095, 0.7978, 0.8538, 0.8172, 0.7845, 0.7694, 0.7891, 0.7885, 0.7536,\n",
      "        0.7715, 0.8031, 0.8020, 0.8340, 0.8361, 0.8003, 0.8129, 0.8456, 0.8083,\n",
      "        0.7897, 0.7797, 0.7836, 0.8082, 0.7998, 0.8000, 0.8122, 0.7390, 0.8369,\n",
      "        0.8250, 0.7879, 0.7934, 0.7731, 0.7736, 0.7948, 0.7981, 0.8248, 0.8291,\n",
      "        0.7916, 0.7704, 0.8154, 0.8876, 0.7966, 0.7762, 0.7982, 0.8266, 0.8194,\n",
      "        0.8384, 0.8074, 0.7418, 0.8020, 0.9121, 0.8985, 0.8177, 0.8144, 0.7862,\n",
      "        0.8073, 0.8366, 0.8987, 0.7970, 0.8768, 0.8282, 0.7598, 0.8120, 0.8513,\n",
      "        0.8381, 0.8151, 0.7820, 0.8012, 0.8504, 0.8743, 0.7752, 0.8129, 0.8677,\n",
      "        0.7503, 0.7722, 0.9080, 0.7513, 0.8448, 0.8314, 0.7648, 0.8037, 0.7693,\n",
      "        0.8952, 0.8161, 0.8136, 0.8226, 0.7862, 0.7451, 0.8143, 0.8198, 0.8029,\n",
      "        0.8108, 0.8793, 0.7652, 0.7910, 0.8355, 0.8227, 0.8080, 0.9047, 0.8200,\n",
      "        0.7819, 0.7874, 0.7532, 0.9005, 0.8115, 0.8480, 0.8377, 0.7895, 0.8139,\n",
      "        0.7743, 0.8517, 0.7912, 0.8046, 0.8307, 0.8291, 0.7865, 0.7627, 0.7951,\n",
      "        0.8157, 0.7955, 0.8474, 0.8280, 0.7328, 0.8517, 0.7856, 0.8500, 0.7728,\n",
      "        0.8499, 0.7929, 0.7896, 0.8106, 1.0271, 0.7854, 0.8490, 0.7472, 0.7939,\n",
      "        0.7746, 0.8279, 0.7994, 0.8574, 0.8042, 0.8021, 0.7965, 0.7551, 0.9254,\n",
      "        0.8000, 0.7857, 0.8344, 0.8255, 0.8428, 0.8358, 0.7970, 0.7681, 0.8200,\n",
      "        0.7992, 0.8324, 0.7910, 0.8328, 0.8405, 0.8329, 0.8147, 0.8015, 0.8139,\n",
      "        0.8428, 0.7922, 0.8250, 0.7718, 0.7867, 0.8736, 0.8075, 0.8417, 0.7727,\n",
      "        0.8572, 0.7758, 0.8489, 0.7720, 0.8072, 0.8162, 0.7464, 0.8102, 0.7856,\n",
      "        0.7961, 0.7762, 0.8932, 0.8287, 0.7932, 0.8443, 0.8429, 0.7838, 0.8267,\n",
      "        0.8288, 0.9089, 0.8545, 0.8156, 0.8476, 0.8470, 0.8044, 0.8675, 0.7878,\n",
      "        0.7617, 0.8100, 0.8019, 0.8035, 0.8220, 0.8211, 0.7597, 0.8634, 0.8445,\n",
      "        0.8229, 0.8500, 0.7857, 0.8870, 0.7757, 0.8299, 0.7883, 0.7862, 0.7882,\n",
      "        0.8250, 0.7542, 0.7470, 0.7905, 0.7932, 0.8666, 0.8015, 0.8408, 0.8210,\n",
      "        0.8140, 0.8361, 0.7913, 0.8215, 0.8108, 0.8295, 0.8067, 0.7671, 0.8479,\n",
      "        0.8202, 0.7591, 0.8208, 0.8396, 0.7866, 0.7910, 0.8478, 0.8291, 0.7592,\n",
      "        0.8228, 0.8095, 0.8147, 0.7912, 0.7899, 0.8279, 0.8742, 0.7988, 0.8531,\n",
      "        0.7806, 0.8327, 0.8789, 0.8030, 0.8163, 0.7886, 0.7544, 0.7862, 0.8216,\n",
      "        0.8141, 0.8043, 0.8063, 0.8437, 0.8568, 0.8603, 0.8814, 0.8053, 0.7855,\n",
      "        0.8483, 0.8329, 0.8150, 0.7926, 0.8178, 0.8195, 0.8148, 0.8055, 0.8319,\n",
      "        0.8338, 0.8059, 0.7993, 0.8197, 0.8854, 0.8378, 0.8840, 0.8034, 0.8227,\n",
      "        0.8283, 0.8310, 0.7845, 0.8204, 0.8798, 0.8211, 0.7838, 0.7893, 0.7812,\n",
      "        0.8488, 0.8126, 0.8101, 0.8248, 0.7681, 0.7743, 0.7819, 0.9110, 0.8200,\n",
      "        0.7569, 0.7792, 0.8366, 0.7883, 0.8131, 0.8047, 0.8415, 0.8265, 0.7610,\n",
      "        0.7898, 0.8392, 0.7941, 0.7533, 0.8616, 0.8420, 0.7869, 0.8216, 0.8024,\n",
      "        1.2443, 0.8168, 0.8529, 0.7907, 0.8274, 0.8539, 0.7878, 0.8237, 0.8074,\n",
      "        0.8214, 0.7938, 0.8045, 0.8205, 0.9494, 0.8140, 0.7813, 0.8519, 0.8056,\n",
      "        0.8402, 0.7720, 0.7907, 0.8242, 0.8478, 0.8229, 0.7814, 0.8257, 0.7962,\n",
      "        0.8264, 0.8280, 0.8258, 0.8627, 0.8229, 0.7925, 0.8172, 0.8139, 0.7995,\n",
      "        0.8773, 0.8333, 0.8293, 0.7715, 0.8145, 0.7740, 0.7940, 0.8213, 0.8117,\n",
      "        0.7642, 0.7931, 0.8569], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0802, -0.1383, -0.0637,  ..., -0.1672,  0.6182, -0.1027],\n",
      "         [-0.1601, -0.0891, -0.0590,  ..., -0.1160, -0.1692, -0.0282],\n",
      "         [-0.0927, -0.1256, -0.1669,  ..., -0.1695, -0.0600, -0.1626],\n",
      "         ...,\n",
      "         [-0.1095, -0.1662, -0.0788,  ..., -0.1700, -0.1698, -0.1208],\n",
      "         [-0.0651, -0.1659, -0.0986,  ..., -0.1658, -0.1618, -0.1149],\n",
      "         [-0.0397, -0.1585, -0.0930,  ..., -0.1695, -0.1685, -0.1012]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-1.0854,  0.3101, -0.7881,  ..., -0.3551,  0.9623,  0.3051],\n",
      "         [-0.9066, -0.9401, -0.2240,  ..., -0.0775,  0.2050, -0.7810],\n",
      "         [ 0.1134, -1.2062, -0.4319,  ...,  0.2880,  0.4676,  0.1824],\n",
      "         ...,\n",
      "         [-1.0911, -0.5657,  0.0996,  ..., -0.0929,  0.1313,  0.1772],\n",
      "         [-1.0164, -0.3810,  0.4895,  ...,  0.2671,  0.1041, -0.1886],\n",
      "         [-0.9339, -0.7275,  0.6382,  ...,  0.5994, -0.0535, -0.3221]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[ 0.0938, -0.1255,  0.0862,  ..., -0.3652, -0.0874,  0.0918],\n",
      "         [ 0.0238,  0.0669,  0.0700,  ...,  0.1914,  0.0049, -0.0857],\n",
      "         [ 0.1251,  0.0065,  0.1371,  ...,  0.0419,  0.0935,  0.0212],\n",
      "         ...,\n",
      "         [-0.1133,  0.2451, -0.0829,  ...,  0.0281, -0.0359, -0.0161],\n",
      "         [-0.1519,  0.2113, -0.0349,  ...,  0.0289, -0.0314, -0.0597],\n",
      "         [-0.2201,  0.1934, -0.0205,  ...,  0.0698,  0.0271, -0.0738]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-1.0854,  0.3101, -0.7881,  ..., -0.3551,  0.9623,  0.3051],\n",
      "         [-0.9066, -0.9401, -0.2240,  ..., -0.0775,  0.2050, -0.7810],\n",
      "         [ 0.1134, -1.2062, -0.4319,  ...,  0.2880,  0.4676,  0.1824],\n",
      "         ...,\n",
      "         [-1.0911, -0.5657,  0.0996,  ..., -0.0929,  0.1313,  0.1772],\n",
      "         [-1.0164, -0.3810,  0.4895,  ...,  0.2671,  0.1041, -0.1886],\n",
      "         [-0.9339, -0.7275,  0.6382,  ...,  0.5994, -0.0535, -0.3221]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-1.3388,  0.2030, -0.8217,  ..., -0.9192,  1.0294,  0.4662],\n",
      "         [-1.0980, -0.9771, -0.1200,  ...,  0.0703,  0.2108, -1.0136],\n",
      "         [ 0.1887, -1.3265, -0.2784,  ...,  0.3001,  0.5879,  0.2017],\n",
      "         ...,\n",
      "         [-1.2365, -0.3077,  0.0729,  ..., -0.1101,  0.0707,  0.1302],\n",
      "         [-1.1121, -0.1561,  0.4533,  ...,  0.1974,  0.0452, -0.2400],\n",
      "         [-1.1028, -0.4656,  0.5983,  ...,  0.5131, -0.0377, -0.3699]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-1.0854,  0.3101, -0.7881,  ..., -0.3551,  0.9623,  0.3051],\n",
      "         [-0.9066, -0.9401, -0.2240,  ..., -0.0775,  0.2050, -0.7810],\n",
      "         [ 0.1134, -1.2062, -0.4319,  ...,  0.2880,  0.4676,  0.1824],\n",
      "         ...,\n",
      "         [-1.0911, -0.5657,  0.0996,  ..., -0.0929,  0.1313,  0.1772],\n",
      "         [-1.0164, -0.3810,  0.4895,  ...,  0.2671,  0.1041, -0.1886],\n",
      "         [-0.9339, -0.7275,  0.6382,  ...,  0.5994, -0.0535, -0.3221]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-1.0986e-01, -4.3323e-02,  2.6039e-02, -6.2369e-02,  1.4469e-01,\n",
      "         2.3894e-02,  8.5148e-03,  1.4266e-01, -1.0833e-01, -2.4890e-02,\n",
      "         9.4963e-03,  4.1846e-02, -1.2760e-01,  6.9555e-02, -7.1048e-02,\n",
      "         1.3593e-01,  5.3569e-02, -6.0933e-03, -1.6118e-02, -3.3533e-02,\n",
      "         7.2184e-02, -2.3410e-02, -2.8510e-03,  5.5217e-02, -8.6453e-02,\n",
      "         6.9857e-02,  3.2686e-02, -6.1245e-02, -7.7870e-02,  3.0835e-02,\n",
      "         1.2870e-01,  3.8405e-02, -4.6480e-02, -6.3926e-02, -7.3934e-02,\n",
      "        -8.2546e-02, -1.8265e-02,  5.9811e-02, -9.6765e-02, -1.3181e-01,\n",
      "        -1.0285e-01, -1.4961e-01,  1.4476e-02,  1.3553e-01,  5.3121e-02,\n",
      "        -8.5382e-02, -3.9924e-02, -9.4157e-02, -1.5067e-02, -7.8801e-02,\n",
      "        -3.4228e-01,  3.4178e-02, -7.2961e-02, -6.9763e-02,  1.4378e-01,\n",
      "         6.5267e-02, -1.1072e-01,  1.9447e-02, -4.5457e-02, -1.2777e-01,\n",
      "        -2.0564e-03,  3.3590e-02, -4.5370e-02, -1.2036e-01,  4.5831e-02,\n",
      "         8.6247e-02,  3.3824e-02,  9.5974e-02, -2.5458e-01, -4.9390e-02,\n",
      "        -1.4536e-01,  4.9588e-02,  2.2120e-02, -7.0260e-02,  1.3164e-02,\n",
      "        -1.6465e-01, -8.2084e-02,  6.4286e-02,  1.6903e-02,  3.3468e-02,\n",
      "        -9.5631e-03,  4.3854e-02, -6.4052e-02,  1.0266e-01, -3.3576e-02,\n",
      "        -1.0900e-01, -8.6243e-03,  1.3397e-02,  3.6529e-02,  2.3307e-01,\n",
      "        -1.4428e-01, -4.8490e-02, -3.1271e-02, -5.4141e-02, -2.4304e-02,\n",
      "        -1.0987e-01,  1.4673e-01, -9.2204e-02, -1.8399e-02, -9.1738e-02,\n",
      "         1.5773e-02, -9.6264e-02,  5.0307e-03, -1.6426e-02, -8.9977e-02,\n",
      "         3.2157e-02, -3.2686e-02, -7.7668e-02, -1.3247e-02, -4.3428e-02,\n",
      "        -5.8969e-02,  7.2364e-02,  1.8438e-02, -9.3603e-02, -4.2916e-02,\n",
      "        -4.3911e-02,  1.1861e-02, -7.4816e-02, -1.1364e-01, -4.7971e-03,\n",
      "         6.2905e-03, -5.6361e-02,  7.5789e-02,  7.6594e-01, -1.1893e-01,\n",
      "        -7.8478e-02, -1.1712e-01,  3.9348e-02, -7.5822e-02,  5.4312e-03,\n",
      "         5.8176e-02,  6.4310e-02,  3.0069e-02, -6.3156e-02, -8.0370e-02,\n",
      "        -6.2562e-02, -5.1549e-02, -7.0859e-03, -7.0210e-02, -1.5485e-02,\n",
      "        -7.1645e-02,  5.2737e-03,  1.7610e-03,  1.3995e-01, -2.1642e-02,\n",
      "        -6.3398e-04,  7.1803e-03, -2.5995e-01, -8.1443e-02, -5.0951e-02,\n",
      "        -4.2187e-03,  3.2522e-02, -1.4275e-01, -1.5201e-01, -8.1764e-02,\n",
      "         1.5360e-02,  6.0379e-02,  1.0348e-01,  5.9798e-02, -4.7835e-02,\n",
      "         2.3904e-02, -1.0125e-01, -6.1234e-02, -4.0206e-02,  1.8066e-01,\n",
      "        -1.6602e-02, -5.2931e-02,  2.6196e-02, -1.2571e-01,  6.1788e-03,\n",
      "        -1.0078e-02, -4.8021e-02,  1.8180e-01,  2.6755e-03,  6.9794e-02,\n",
      "        -6.1887e-02,  2.9740e-02,  1.5290e-02,  5.7968e-02,  1.7624e-01,\n",
      "        -4.6810e-01,  1.0850e-01, -1.7701e-02,  1.3952e-02, -2.3574e-02,\n",
      "        -1.1344e-02,  1.7689e-01, -1.9464e-02,  3.5528e-02,  5.0650e-02,\n",
      "        -1.0216e-01,  5.9138e-02,  6.1453e-02, -4.9731e-02, -1.7462e-02,\n",
      "         5.3093e-02, -1.1973e-01, -4.3167e-02,  7.7277e-03,  6.1520e-02,\n",
      "        -6.4361e-03,  3.3494e-02,  6.8797e-02, -6.8644e-02, -1.2263e-01,\n",
      "         3.6359e-01, -2.7784e-03,  4.2856e-02, -1.4364e-02,  3.3354e-02,\n",
      "        -7.1967e-02, -1.3416e-02,  3.3289e-02,  1.3586e-01, -8.1726e-04,\n",
      "        -1.1282e-01, -6.6198e-02, -2.0349e-02,  7.1594e-02, -3.8219e-02,\n",
      "        -9.9592e-04,  4.3075e-02, -4.7170e-03,  9.2223e-02,  7.9907e-03,\n",
      "         1.8319e-01,  6.7595e-03, -1.6716e-01,  3.7392e-02, -1.1114e-01,\n",
      "        -3.5641e-02, -6.7132e-02,  1.0227e-01, -7.8563e-02, -3.4340e-02,\n",
      "        -6.4997e-02, -5.4021e-02,  3.2029e-03, -2.4696e-02, -1.2331e-01,\n",
      "         9.3232e-02, -3.5118e-02,  1.2557e-02,  1.6174e-01, -7.3716e-02,\n",
      "        -4.3041e-02, -2.1121e-02, -9.8188e-03, -8.1827e-02, -1.1507e-01,\n",
      "        -1.4251e-01, -1.3007e-02, -1.1562e-01, -1.3869e-02, -5.5616e-02,\n",
      "        -1.3748e-01,  2.0038e-02, -1.0893e-01,  3.7848e-02,  2.6370e-02,\n",
      "         1.5256e-01, -1.7048e-02, -5.4340e-03,  6.5350e-02, -4.5541e-02,\n",
      "        -1.3889e-02, -1.3546e-02, -1.9981e-02,  1.8412e-02,  2.5642e-02,\n",
      "         3.9688e-02,  1.5539e-02, -5.1998e-02,  1.5934e-01, -1.1188e-01,\n",
      "        -8.8874e-02,  1.6955e-02, -4.8052e-02, -8.5343e-02, -1.3444e-01,\n",
      "         1.3075e-01,  1.8793e-01, -6.2813e-02, -1.0423e-01, -2.3544e-01,\n",
      "        -9.2175e-02,  5.2775e-02,  2.1384e-02,  2.0662e-02, -4.3228e-02,\n",
      "         1.0775e-02,  9.1036e-02, -3.2879e-02, -5.3863e-02,  1.2356e-01,\n",
      "         8.3869e-03, -8.6142e-02, -8.5249e-02, -8.5097e-03,  1.7478e-02,\n",
      "        -1.6477e-01, -1.0826e-02, -3.2626e-02,  4.0981e-02, -6.2419e-02,\n",
      "         2.2502e-02,  3.1876e-02, -5.2900e-02, -1.3520e+00,  4.5146e-02,\n",
      "        -1.3262e-01, -9.9322e-02,  1.7022e-01, -1.0428e-02, -5.8837e-02,\n",
      "        -5.3764e-02, -5.0277e-02,  1.4743e-03, -1.4262e-02, -9.3190e-02,\n",
      "         7.9996e-03,  1.8367e-01, -1.3173e-04, -1.9939e-01,  3.4430e-02,\n",
      "        -1.8154e-01, -2.7990e-02,  3.4381e-01, -5.9272e-02, -8.2675e-02,\n",
      "        -3.7347e-02,  1.6080e-02,  1.0628e-03,  1.3274e-01,  3.6783e-02,\n",
      "        -2.6304e-02, -1.4164e-01, -1.7573e-02, -2.2060e-02, -1.0065e-02,\n",
      "         2.0352e-02,  1.8773e-01, -7.8374e-02,  3.5247e-02, -4.4983e-02,\n",
      "        -1.5688e-01, -2.4710e-02, -9.4099e-02, -4.4270e-03, -2.4106e-01,\n",
      "         3.7364e-03,  1.2799e-02,  2.1714e-01,  2.4250e-02, -5.8620e-02,\n",
      "        -3.4071e-02, -9.4078e-02,  5.6670e-02,  2.0208e-02, -5.5237e-03,\n",
      "         1.1586e-02, -1.6815e-01,  1.3505e-01, -6.3363e-03,  7.6929e-02,\n",
      "         9.7497e-02, -1.7614e-01, -4.1746e-02, -1.1368e-01, -1.1160e-01,\n",
      "        -2.6994e-01,  2.3484e-02,  3.9368e-02, -1.1853e-01, -3.3546e-01,\n",
      "        -6.0605e-03, -5.0075e-02, -1.1067e-01, -5.9053e-02,  8.0018e-02,\n",
      "        -1.3309e-02, -1.8882e+00, -3.5066e-02, -9.8846e-02,  1.2757e-01,\n",
      "         6.2637e-03, -3.9878e-02,  3.0363e-04, -7.2995e-02, -1.4242e-01,\n",
      "         7.0475e-03, -2.7715e-02,  1.8113e-02, -1.0867e-01, -6.6526e-02,\n",
      "        -2.5656e-03, -1.0198e-01, -1.5856e-01,  1.2822e-02, -1.9299e-02,\n",
      "         5.2201e-02, -3.6497e-02,  4.5979e-02, -2.7962e-02, -4.6199e-02,\n",
      "         4.2883e-02, -1.8909e-02, -4.6832e-02,  3.7635e-02, -3.5515e-02,\n",
      "         2.1452e-02, -4.2567e-02,  4.3887e-02, -1.3776e-02, -6.0612e-02,\n",
      "         1.8401e-02,  1.0900e-01,  4.9528e-02,  7.3153e-02, -2.6886e-03,\n",
      "        -3.9657e-03,  2.6168e-02, -9.5414e-04, -1.2313e-01,  8.4072e-02,\n",
      "         4.1441e-02,  2.9816e-02, -1.4693e-01, -4.8402e-02, -5.3208e-02,\n",
      "        -7.6187e-02, -1.3760e-01,  1.3487e-02,  1.0734e-02, -1.3575e-01,\n",
      "        -1.1805e-02, -7.8494e-03,  8.7178e-02, -6.3253e-02, -1.3882e-01,\n",
      "         1.1763e-01,  1.0118e-01,  3.5979e-02, -4.7568e-02, -3.9294e-02,\n",
      "        -4.7182e-01, -8.1394e-02,  9.1848e-03, -3.3915e-03, -8.3868e-02,\n",
      "         4.2592e-02, -3.1215e-03,  1.6061e-01, -3.4119e-03,  8.8006e-02,\n",
      "         3.6886e-02, -3.5935e-02,  4.2436e-02, -1.8397e-01, -1.6714e-01,\n",
      "         2.4234e-02, -6.4019e-02,  4.0601e-02,  1.4015e-01,  2.4564e-01,\n",
      "        -2.8037e-02,  7.1655e-02, -1.4700e-01,  8.2924e-02, -1.0860e-01,\n",
      "        -5.6395e-02, -9.3245e-02, -5.7061e-02,  5.4943e-02, -6.0407e-02,\n",
      "        -1.3406e-01,  1.4752e-02,  3.0653e-02, -1.9031e-01,  2.8196e-02,\n",
      "        -1.6686e-02, -4.3753e-02, -7.7339e-02,  5.0255e-02, -6.3097e-02,\n",
      "        -9.4029e-02, -1.1533e-01,  1.7190e-01, -6.8709e-02, -9.8590e-02,\n",
      "        -5.1946e-02,  2.4300e-02,  7.0611e-02, -3.7195e-02, -2.9533e-02,\n",
      "        -1.0373e-01, -4.3854e-02, -4.1990e-02, -1.5301e-02,  6.3708e-02,\n",
      "         1.4197e-02, -6.9791e-02, -9.4691e-02, -1.6623e-01,  1.6627e-02,\n",
      "        -4.0652e-02, -1.4417e-01,  6.5713e-02, -4.7441e-02, -6.9841e-02,\n",
      "        -1.5034e-02,  2.7414e-02,  2.7576e-02, -1.4552e-01, -4.4078e-03,\n",
      "         1.2355e-01, -1.7122e-01, -6.5077e-02, -1.5522e-01, -8.7137e-02,\n",
      "        -1.2388e-01,  6.8649e-02, -7.5485e-02, -4.4840e-03, -7.6560e-02,\n",
      "        -9.5748e-02,  2.5241e-01, -8.6654e-02, -1.9855e-02, -7.0882e-02,\n",
      "         5.3095e-02, -1.8605e-02, -9.2690e-02,  1.7085e-02,  8.7068e-02,\n",
      "        -4.7949e-02, -1.3915e-02,  4.0671e-03, -3.1902e-02, -3.1709e-01,\n",
      "         2.4549e-02,  1.7502e-02, -3.6478e-02, -8.7535e-02, -1.0812e-01,\n",
      "        -1.9368e-01,  5.6600e-02,  6.2011e-02,  8.3298e-02,  2.9690e-02,\n",
      "        -5.7632e-02, -1.0018e-01,  6.3107e-02,  1.1186e-01,  4.4521e-02,\n",
      "        -5.9027e-02, -2.8306e-02,  1.0422e-01, -9.7934e-04, -1.3316e-01,\n",
      "        -4.8509e-02, -6.3272e-02,  1.2613e-01, -6.9826e-02, -1.3674e-01,\n",
      "         2.8832e-02, -7.8703e-02,  8.5840e-02, -3.6097e-02,  5.7578e-02,\n",
      "        -1.5922e-01,  8.2077e-02,  1.3649e-01, -7.2033e-02, -1.8363e-02,\n",
      "        -3.8284e-02, -3.4231e-02, -1.8843e-01,  7.0095e-02, -7.9218e-02,\n",
      "        -5.9366e-02, -5.6867e-05, -7.8419e-02,  7.7719e-02, -4.0908e-02,\n",
      "        -4.4340e-02,  7.3448e-02, -1.0578e-01, -2.0239e-01,  1.1986e-02,\n",
      "        -5.2369e-02, -4.3708e-02,  1.3372e-02, -2.5477e-01, -5.7132e-02,\n",
      "         3.7881e-02, -7.2967e-02, -2.6268e-02,  1.2307e-01,  4.9715e-02,\n",
      "         1.4006e-02, -1.2879e-02, -6.9322e-02,  9.7442e-02,  1.2173e-01,\n",
      "         5.7379e-02,  1.5167e-03, -4.3250e-02, -1.1526e-01,  5.1005e-03,\n",
      "         9.7860e-02, -8.3183e-02,  2.7053e-02,  2.8275e-02,  4.4384e-02,\n",
      "        -1.7175e-01, -1.0611e-01, -4.4474e-02,  2.4240e-02, -1.7871e-01,\n",
      "         2.4465e-01,  1.0488e-01, -4.2515e-02, -2.7527e-02,  5.0208e-02,\n",
      "        -3.9354e-02,  6.3300e-02, -9.6138e-02, -6.3638e-03,  7.2867e-02,\n",
      "         8.9386e-02,  9.6692e-03,  4.8196e-02,  1.4490e-01, -5.4485e-02,\n",
      "         3.7006e-02,  6.7020e-02,  2.1314e-01,  5.7172e-02,  3.8746e-02,\n",
      "        -5.2525e-02,  4.3298e-02, -1.0086e-01, -1.3973e-01,  1.1766e-01,\n",
      "        -1.5524e-01,  3.9553e-02,  1.8500e-01,  4.3291e-02,  6.8795e-03,\n",
      "         1.1762e-01,  2.7638e-02,  7.9839e-02,  2.3957e-02,  5.0306e-02,\n",
      "        -3.6551e-02,  1.1517e-01,  1.6246e-01, -7.3600e-03, -4.1794e-02,\n",
      "         7.8460e-02,  2.4231e-02,  1.3117e-01, -3.5845e-02,  4.5641e-02,\n",
      "        -2.2538e-02,  4.5256e-03,  1.8366e-01,  5.2601e-02,  2.3233e-02,\n",
      "         1.4683e-01, -4.2960e-02, -3.5630e-02, -6.2859e-02,  8.5867e-02,\n",
      "        -8.0193e-02,  9.7035e-02, -9.7163e-02,  8.4262e-02, -5.3432e-02,\n",
      "         7.3728e-02, -4.5167e-02,  5.1862e-02, -9.0663e-02,  9.1148e-03,\n",
      "        -5.4825e-02,  1.9205e-02,  5.7003e-02, -4.8414e-02, -3.9301e-02,\n",
      "        -1.5805e-01, -1.3792e-01, -1.9177e-02,  1.3517e-02,  4.7648e-02,\n",
      "         1.6847e-03, -5.9760e-02, -1.3280e-01, -1.3314e-02,  5.7509e-03,\n",
      "         9.9600e-02, -1.9460e-02, -5.7305e-02, -9.6649e-03,  8.1935e-02,\n",
      "         1.6404e-02,  1.2222e-01,  4.2565e-02,  3.5585e-03,  8.3056e-02,\n",
      "        -1.6466e-01,  5.9435e-02, -4.2335e-03,  3.8446e-02, -2.8132e-02,\n",
      "        -2.5972e-02, -2.7214e-01, -3.8381e-02,  2.9798e-02,  4.6312e-02,\n",
      "        -9.1643e-01,  2.4492e-03,  4.6300e-03,  4.1784e-02,  3.2197e-02,\n",
      "        -1.5715e-01, -1.7571e-01, -1.7412e-01, -1.1094e-01, -9.1469e-02,\n",
      "        -5.7015e-02, -3.4931e-02, -9.5130e-02,  2.2514e-02,  9.2526e-02,\n",
      "        -5.1622e-02, -8.4395e-02, -3.9269e-02, -1.2635e-01,  7.3185e-02,\n",
      "        -6.1763e-02,  1.1407e-02, -1.3769e-01, -1.4406e-01, -5.7378e-02,\n",
      "         6.8709e-02, -5.2696e-02, -1.0309e-01,  5.8251e-02,  8.8800e-02,\n",
      "        -5.8064e-02, -5.7135e-02,  9.2848e-02,  8.7045e-02, -3.6935e-02,\n",
      "        -1.4810e-01, -2.4457e-01,  6.8419e-02, -1.1099e-01, -2.3767e-02,\n",
      "         1.1027e-01, -6.5348e-02,  1.4737e-03, -1.1822e-01, -2.5063e-02,\n",
      "        -8.1075e-02, -4.6832e-02, -5.4376e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8516, 0.8139, 0.8367, 0.7804, 0.8809, 0.8266, 0.8254, 0.8704, 0.8311,\n",
      "        0.8326, 0.8139, 0.8068, 0.7919, 0.8126, 0.8157, 0.8379, 0.8228, 0.8021,\n",
      "        0.8737, 0.8193, 0.8451, 0.8182, 0.8029, 0.8129, 0.7986, 0.8149, 0.8223,\n",
      "        0.8579, 0.8327, 0.8090, 0.8529, 0.8110, 0.8050, 0.8320, 0.8345, 0.7915,\n",
      "        0.8518, 0.8148, 0.8619, 0.8186, 0.8246, 0.8251, 0.8375, 0.8504, 0.8012,\n",
      "        0.8117, 1.1885, 0.8286, 0.8332, 0.8408, 0.8715, 0.8231, 0.8083, 0.8043,\n",
      "        0.8448, 0.8442, 0.8043, 0.8106, 0.7972, 0.8112, 0.8506, 0.8161, 0.7982,\n",
      "        0.8390, 0.8310, 0.8446, 0.8066, 0.8471, 0.8720, 0.8347, 0.8236, 0.8442,\n",
      "        0.8164, 0.8162, 0.8356, 0.8548, 0.8220, 0.8554, 0.8124, 0.8086, 0.8203,\n",
      "        0.8478, 0.8427, 0.8122, 0.7985, 0.7930, 0.8720, 0.7804, 0.8037, 0.8763,\n",
      "        0.8309, 0.8152, 0.8255, 0.8356, 0.8150, 0.8157, 0.8542, 0.8154, 0.8086,\n",
      "        0.8437, 0.8366, 0.8480, 0.8255, 0.8181, 0.8310, 0.7936, 0.7778, 0.8487,\n",
      "        0.8150, 0.8810, 0.8325, 0.8273, 0.8007, 0.8074, 0.7856, 0.7914, 0.8108,\n",
      "        0.8064, 0.8252, 0.7947, 0.8152, 0.8136, 0.8196, 2.1988, 0.7953, 0.8158,\n",
      "        0.8449, 0.8231, 0.8200, 0.8187, 0.8287, 0.8035, 0.7919, 0.8154, 0.8176,\n",
      "        0.8215, 0.8216, 0.8111, 0.8236, 0.8094, 0.8398, 0.8436, 0.8508, 0.9312,\n",
      "        0.8243, 0.7902, 0.8260, 0.8784, 0.8314, 0.8245, 0.8200, 0.8517, 0.8160,\n",
      "        0.8384, 0.8111, 0.8152, 0.8347, 0.8506, 0.8516, 0.8480, 0.8463, 0.8275,\n",
      "        0.8108, 0.8466, 0.8684, 0.8373, 0.8337, 0.8474, 0.8418, 0.8237, 0.8073,\n",
      "        0.8386, 0.8017, 0.8128, 0.8366, 0.7916, 0.8101, 0.8203, 0.8014, 0.8276,\n",
      "        2.2593, 0.8438, 0.7983, 0.8251, 0.8337, 0.8015, 0.8900, 0.8267, 0.8513,\n",
      "        0.7960, 0.8199, 0.8095, 0.8462, 0.8228, 0.8278, 0.8558, 0.8596, 0.7976,\n",
      "        0.8492, 0.7893, 0.8185, 0.7881, 0.7980, 0.8282, 0.8181, 1.3081, 0.7967,\n",
      "        0.7993, 0.8166, 0.8408, 0.8113, 0.8258, 0.8352, 0.8712, 0.8399, 0.7994,\n",
      "        0.8216, 0.8576, 0.8240, 0.8173, 0.8217, 0.8454, 0.8357, 0.8063, 0.8231,\n",
      "        1.5342, 0.8137, 0.8222, 0.8210, 0.9655, 0.8404, 0.8097, 0.8240, 0.8170,\n",
      "        0.8348, 0.8265, 0.8302, 0.8378, 0.8204, 0.8553, 0.7878, 0.8218, 0.8350,\n",
      "        0.8566, 0.8354, 0.8035, 0.8040, 0.8193, 0.8325, 0.7953, 0.8337, 0.8443,\n",
      "        0.8021, 0.8238, 0.8603, 0.8051, 0.7853, 0.8646, 0.7991, 0.8277, 0.8433,\n",
      "        0.8164, 0.7853, 0.8163, 0.8234, 0.8473, 0.8588, 0.8047, 0.8205, 0.8385,\n",
      "        0.8216, 0.8512, 0.8250, 0.8227, 0.8381, 0.8374, 0.7989, 0.8114, 0.8323,\n",
      "        0.8334, 0.8393, 0.8753, 0.8273, 0.8021, 0.8497, 0.7771, 0.8626, 0.8884,\n",
      "        0.8147, 0.8230, 0.8180, 0.8737, 0.8015, 0.8688, 0.8634, 0.8222, 0.8147,\n",
      "        0.7858, 0.8370, 0.7966, 0.8638, 0.8275, 0.8445, 0.7997, 0.8477, 0.8180,\n",
      "        0.8131, 0.8466, 0.5300, 0.7921, 0.8668, 0.8326, 0.8596, 0.8008, 0.8337,\n",
      "        0.7762, 0.8375, 0.8009, 0.8420, 0.7761, 0.8216, 0.8668, 0.8108, 0.8748,\n",
      "        0.8364, 0.8154, 0.7936, 0.9133, 0.8168, 0.8020, 0.8219, 0.7865, 0.8064,\n",
      "        0.8714, 0.8580, 0.8162, 0.8419, 0.8091, 0.8386, 0.8272, 0.8200, 0.8527,\n",
      "        0.8080, 0.8012, 0.7897, 0.8253, 0.8327, 0.8740, 0.8075, 0.8821, 0.8000,\n",
      "        0.8234, 0.8885, 0.7922, 0.8337, 0.8499, 0.8347, 0.8196, 0.8143, 0.8178,\n",
      "        0.8547, 0.8037, 0.8078, 0.8129, 0.8160, 0.8777, 0.8296, 0.8319, 0.8401,\n",
      "        0.7785, 0.8852, 0.8079, 0.8408, 0.8573, 0.9176, 0.8365, 0.8049, 0.8349,\n",
      "        0.8222, 0.8514, 0.8362, 3.1994, 0.8310, 0.8773, 0.8864, 0.8246, 0.8234,\n",
      "        0.8371, 0.8331, 0.8531, 0.8445, 0.8116, 0.8156, 0.8169, 0.8032, 0.8072,\n",
      "        0.8374, 0.8342, 0.8453, 0.8442, 0.8173, 0.8155, 0.8378, 0.8092, 0.8058,\n",
      "        0.8317, 0.7968, 0.8162, 0.8266, 0.8346, 0.8269, 0.8537, 0.8465, 0.7964,\n",
      "        0.8174, 0.7943, 0.7900, 0.8370, 0.7915, 0.8274, 0.8434, 0.8009, 0.8588,\n",
      "        0.8175, 0.8153, 0.8234, 0.8027, 0.7989, 0.8172, 0.8170, 0.8407, 0.8841,\n",
      "        0.8255, 0.8080, 0.8183, 0.8504, 0.8317, 0.8106, 0.8048, 0.8322, 0.8469,\n",
      "        0.8296, 0.8161, 0.8067, 0.8128, 0.9662, 0.8350, 0.8515, 0.8332, 0.8418,\n",
      "        0.8191, 0.8680, 0.8824, 0.8296, 0.8565, 0.8265, 0.7991, 0.8029, 0.8814,\n",
      "        0.8646, 0.8364, 0.8145, 0.8410, 0.8604, 0.8705, 0.7852, 0.8511, 0.8877,\n",
      "        0.8133, 0.8167, 0.9911, 0.8181, 0.8459, 0.8313, 0.7976, 0.8256, 0.7844,\n",
      "        0.8758, 0.8412, 0.8192, 0.8440, 0.7821, 0.7770, 0.8157, 0.8390, 0.8312,\n",
      "        0.8428, 0.8897, 0.8142, 0.8153, 0.8233, 0.8222, 0.8055, 0.8962, 0.8279,\n",
      "        0.8096, 0.7981, 0.8129, 0.8920, 0.8296, 0.8829, 0.8344, 0.8296, 0.8655,\n",
      "        0.7928, 0.8444, 0.8144, 0.8047, 0.8177, 0.8316, 0.8234, 0.8078, 0.8300,\n",
      "        0.8294, 0.8389, 0.8289, 0.8332, 0.8000, 0.8540, 0.8194, 0.8425, 0.8334,\n",
      "        0.8388, 0.8237, 0.8154, 0.8075, 1.5364, 0.8095, 0.8266, 0.7928, 0.8374,\n",
      "        0.7862, 0.8404, 0.8142, 0.8454, 0.8615, 0.8225, 0.8244, 0.8117, 0.8681,\n",
      "        0.8166, 0.8321, 0.8287, 0.8350, 0.8878, 0.8556, 0.8362, 0.8222, 0.8547,\n",
      "        0.8017, 0.8317, 0.8326, 0.8374, 0.8742, 0.8102, 0.8213, 0.8039, 0.8718,\n",
      "        0.8637, 0.8106, 0.8033, 0.7914, 0.8150, 0.8669, 0.8319, 0.8549, 0.8010,\n",
      "        0.8615, 0.8012, 0.8388, 0.8147, 0.8004, 0.8225, 0.8036, 0.8079, 0.8040,\n",
      "        0.8066, 0.8645, 0.8635, 0.8290, 0.8484, 0.8457, 0.8401, 0.8158, 0.8141,\n",
      "        0.8270, 0.8721, 0.8536, 0.8339, 0.8179, 0.8317, 0.8162, 0.8355, 0.8383,\n",
      "        0.7929, 0.8359, 0.8111, 0.8062, 0.8061, 0.8139, 0.7818, 0.8455, 0.8310,\n",
      "        0.8546, 0.8446, 0.8277, 0.8750, 0.7991, 0.8450, 0.7979, 0.8168, 0.8426,\n",
      "        0.8186, 0.7913, 0.8123, 0.8163, 0.8479, 0.8612, 0.8139, 0.8641, 0.8820,\n",
      "        0.8241, 0.8044, 0.8284, 0.8351, 0.8440, 0.8401, 0.8465, 0.7994, 0.8624,\n",
      "        0.8297, 0.7926, 0.8416, 0.8594, 0.8046, 0.8307, 0.8443, 0.8361, 0.7991,\n",
      "        0.8373, 0.8148, 0.8511, 0.8440, 0.8393, 0.8328, 0.8824, 0.8366, 0.8598,\n",
      "        0.8273, 0.8226, 0.8240, 0.8333, 0.8207, 0.7816, 0.8238, 0.8272, 0.8575,\n",
      "        0.8498, 0.8221, 0.8422, 0.8259, 0.8493, 0.8334, 0.8514, 0.8092, 0.8383,\n",
      "        0.8381, 0.8798, 0.8294, 0.8112, 0.8698, 0.8403, 0.8144, 0.8294, 0.8296,\n",
      "        0.8507, 0.8324, 0.8246, 0.8201, 0.8782, 0.8429, 0.8603, 0.8141, 0.8261,\n",
      "        0.8285, 0.8405, 0.8011, 0.8276, 0.8736, 0.8315, 0.8129, 0.8327, 0.8386,\n",
      "        0.8210, 0.8268, 0.8239, 0.8434, 0.8195, 0.8146, 0.8368, 0.8611, 0.8269,\n",
      "        0.8236, 0.8026, 0.8456, 0.8388, 0.8736, 0.9052, 0.8264, 0.8228, 0.8041,\n",
      "        0.8032, 0.8736, 0.7961, 0.8114, 0.8553, 0.9220, 0.8062, 0.8310, 0.8195,\n",
      "        2.2278, 0.8226, 0.8401, 0.8217, 0.8251, 0.8460, 0.8280, 0.8745, 0.8515,\n",
      "        0.8174, 0.7941, 0.8202, 0.8214, 0.8997, 0.8288, 0.8202, 0.8498, 0.8283,\n",
      "        0.8155, 0.8155, 0.8343, 0.8463, 0.8305, 0.8457, 0.8121, 0.8282, 0.8091,\n",
      "        0.8278, 0.8238, 0.8573, 0.8414, 0.8250, 0.8370, 0.8395, 0.8216, 0.8565,\n",
      "        1.0024, 0.8248, 0.8308, 0.8249, 0.8481, 0.7920, 0.8152, 0.8485, 0.8238,\n",
      "        0.8056, 0.8108, 0.8425], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0266, -0.1681, -0.0005,  ..., -0.1699, -0.1636, -0.0010],\n",
      "         [-0.0013, -0.0069, -0.0114,  ..., -0.0966, -0.0182, -0.0027],\n",
      "         [-0.0008, -0.1235, -0.0261,  ..., -0.1654, -0.1321, -0.0158],\n",
      "         ...,\n",
      "         [-0.0319, -0.0207, -0.0126,  ..., -0.0323, -0.1144, -0.0851],\n",
      "         [-0.0342, -0.0168, -0.0247,  ..., -0.0369, -0.1023, -0.0615],\n",
      "         [-0.0378, -0.0192, -0.0200,  ..., -0.0543, -0.0903, -0.0627]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.9111,  0.3089, -0.5762,  ..., -0.7933,  0.4733,  0.6922],\n",
      "         [-0.9542, -0.8566, -0.1645,  ...,  0.0354,  0.0209, -0.7024],\n",
      "         [ 0.0846, -1.1509, -0.0358,  ..., -0.1470,  0.2674,  0.1312],\n",
      "         ...,\n",
      "         [-1.1545, -0.4926,  0.1616,  ..., -0.0402,  0.1679, -0.2654],\n",
      "         [-1.1772, -0.3303,  0.5822,  ...,  0.1597,  0.1207, -0.6171],\n",
      "         [-1.2012, -0.6398,  0.6587,  ...,  0.5276,  0.0558, -0.6454]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output  tensor([[[-0.4036,  0.7457, -1.2203,  ..., -1.6130,  0.8994,  0.7739],\n",
      "         [-0.3694,  0.2822, -0.1068,  ..., -0.1615,  0.1140,  0.1601],\n",
      "         [-0.2456,  0.3947, -0.2943,  ..., -0.2507,  0.1965,  0.1243],\n",
      "         ...,\n",
      "         [-0.0428,  0.0606, -0.1520,  ...,  0.2574,  0.1638,  0.1960],\n",
      "         [-0.1622,  0.0256, -0.2443,  ...,  0.2236,  0.1295,  0.1598],\n",
      "         [-0.1949,  0.0082, -0.2739,  ...,  0.1974,  0.1257,  0.1667]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.9111,  0.3089, -0.5762,  ..., -0.7933,  0.4733,  0.6922],\n",
      "         [-0.9542, -0.8566, -0.1645,  ...,  0.0354,  0.0209, -0.7024],\n",
      "         [ 0.0846, -1.1509, -0.0358,  ..., -0.1470,  0.2674,  0.1312],\n",
      "         ...,\n",
      "         [-1.1545, -0.4926,  0.1616,  ..., -0.0402,  0.1679, -0.2654],\n",
      "         [-1.1772, -0.3303,  0.5822,  ...,  0.1597,  0.1207, -0.6171],\n",
      "         [-1.2012, -0.6398,  0.6587,  ...,  0.5276,  0.0558, -0.6454]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "attention output 2  tensor([[[-0.9039,  0.6967, -1.2490,  ..., -1.6199,  0.9787,  1.0190],\n",
      "         [-1.5349, -0.5814, -0.3248,  ..., -0.1493,  0.2462, -0.5611],\n",
      "         [-0.1850, -0.7876, -0.3990,  ..., -0.4647,  0.6204,  0.3549],\n",
      "         ...,\n",
      "         [-1.1251, -0.3382, -0.0020,  ...,  0.1916,  0.3953, -0.0047],\n",
      "         [-1.1316, -0.1982,  0.2760,  ...,  0.3090,  0.2961, -0.3235],\n",
      "         [-1.1844, -0.4610,  0.3171,  ...,  0.5949,  0.2394, -0.3431]]],\n",
      "       grad_fn=<AddBackward0>) tensor([[[-0.9111,  0.3089, -0.5762,  ..., -0.7933,  0.4733,  0.6922],\n",
      "         [-0.9542, -0.8566, -0.1645,  ...,  0.0354,  0.0209, -0.7024],\n",
      "         [ 0.0846, -1.1509, -0.0358,  ..., -0.1470,  0.2674,  0.1312],\n",
      "         ...,\n",
      "         [-1.1545, -0.4926,  0.1616,  ..., -0.0402,  0.1679, -0.2654],\n",
      "         [-1.1772, -0.3303,  0.5822,  ...,  0.1597,  0.1207, -0.6171],\n",
      "         [-1.2012, -0.6398,  0.6587,  ...,  0.5276,  0.0558, -0.6454]]],\n",
      "       grad_fn=<AddBackward0>) 1e-12 Parameter containing:\n",
      "tensor([-3.1344e-02,  1.2080e-02, -4.6364e-02, -3.0130e-02,  7.9443e-02,\n",
      "         5.0981e-02,  1.1908e-02, -2.0652e-02, -1.0936e-01, -1.1688e-01,\n",
      "        -8.6071e-02,  2.6358e-02, -1.1645e-01,  1.5676e-02, -3.1420e-02,\n",
      "         3.6622e-02,  5.6841e-02,  4.6632e-02, -8.3829e-03, -7.8806e-02,\n",
      "        -2.4978e-02, -3.3134e-02,  6.8759e-02,  3.2692e-02,  3.3648e-02,\n",
      "        -2.8723e-02,  8.5249e-03, -5.7247e-02, -1.0357e-01, -1.4365e-02,\n",
      "         6.1772e-02, -7.3953e-02, -3.5921e-04, -3.0949e-02, -1.3592e-01,\n",
      "        -7.2458e-02, -5.0496e-02, -1.5356e-02, -5.2733e-02, -1.0026e-01,\n",
      "        -1.4037e-01, -6.2274e-02, -7.1671e-02, -1.5139e-02, -8.2626e-02,\n",
      "        -4.7771e-02, -1.4454e-01,  3.6936e-02, -7.9435e-02, -3.5952e-02,\n",
      "        -1.0434e-01,  7.1274e-02,  3.2480e-02,  6.3918e-02,  2.8611e-02,\n",
      "        -1.6015e-02, -1.3967e-01, -1.6648e-02, -6.6715e-02, -7.7961e-02,\n",
      "        -8.4401e-03,  4.0578e-02,  1.0819e-01, -4.4659e-02,  1.1845e-02,\n",
      "         5.9508e-02,  1.6610e-02,  1.8677e-02, -1.1001e-01, -1.2207e-01,\n",
      "        -1.0786e-01, -9.6549e-02, -6.1122e-02, -3.2128e-02, -7.1845e-02,\n",
      "        -1.2359e-01, -2.4683e-02, -1.9532e-02,  1.5631e-02,  9.3122e-02,\n",
      "        -7.1852e-02, -2.3720e-02, -3.4551e-02, -2.2903e-02, -4.2727e-02,\n",
      "        -8.6009e-02, -9.0888e-02,  9.3918e-02, -5.9314e-02,  7.0650e-02,\n",
      "        -1.0035e-01,  1.4553e-02,  1.3380e-02,  1.0654e-01,  7.3451e-03,\n",
      "        -1.3439e-01,  1.1810e-01, -7.5076e-02, -3.9675e-02, -1.2730e-02,\n",
      "        -9.3441e-02, -7.8124e-02,  5.0937e-03, -4.6052e-02, -1.3498e-01,\n",
      "        -9.7142e-02, -7.0380e-02, -9.0184e-02, -5.4622e-02, -1.1805e-02,\n",
      "        -9.8805e-02,  9.0518e-03, -6.6125e-02, -7.6516e-02, -1.6316e-01,\n",
      "         9.1859e-02,  6.8594e-02, -1.1347e-01, -9.3453e-02, -1.6409e-02,\n",
      "         4.9831e-02,  5.9697e-02, -2.5936e-02,  3.3220e-01, -1.5791e-02,\n",
      "        -4.6808e-02,  1.6120e-03,  5.4064e-02, -1.0811e-01, -1.0923e-01,\n",
      "        -2.3789e-02,  1.1774e-01, -2.7513e-03, -1.0053e-01, -6.7293e-02,\n",
      "        -1.4769e-01, -5.7897e-02, -1.0835e-01, -3.9113e-03, -3.5699e-02,\n",
      "        -5.1963e-02, -9.8398e-02, -8.2750e-02, -6.5031e-02, -5.8932e-02,\n",
      "        -3.8372e-02, -3.2102e-02, -1.0399e-01, -2.4155e-02, -6.1841e-02,\n",
      "        -5.7919e-02,  1.0234e-03, -1.5286e-01, -1.6116e-01, -6.3931e-02,\n",
      "         5.4084e-02,  6.7514e-02, -4.2360e-02, -9.1926e-02, -3.0753e-02,\n",
      "         3.0540e-02, -6.8048e-02, -6.3836e-02, -1.0519e-01,  3.6709e-02,\n",
      "         2.3915e-02,  8.1677e-02,  6.4061e-02, -1.0920e-01,  6.5083e-02,\n",
      "        -7.1140e-02, -9.0670e-02,  5.8689e-02, -7.2455e-02, -8.5068e-02,\n",
      "        -6.9670e-02,  6.7066e-02,  2.5771e-02,  6.9231e-02, -1.5279e-02,\n",
      "        -6.2065e-01,  4.2855e-02, -1.0108e-01,  1.4958e-02, -5.1842e-02,\n",
      "        -2.9548e-02,  1.2663e-01, -1.9485e-02, -1.4611e-02, -7.0454e-02,\n",
      "        -1.5010e-01, -1.1479e-01, -2.0274e-02, -4.6186e-02,  3.7312e-02,\n",
      "        -4.6845e-03, -1.1243e-01,  5.7662e-03, -4.8168e-02,  4.1642e-02,\n",
      "        -1.2994e-01, -1.0522e-01,  5.8554e-02, -7.5463e-02, -1.3078e-01,\n",
      "         3.4147e-02,  3.6588e-02, -4.6127e-02, -3.3408e-02,  3.4817e-03,\n",
      "        -4.2363e-02, -4.2850e-02,  6.9876e-02, -4.7035e-02,  3.6683e-02,\n",
      "         2.3717e-02, -5.6663e-02, -2.3482e-02,  2.2695e-03, -7.1891e-02,\n",
      "         3.4086e-02, -3.9050e-02, -5.4687e-02, -1.0103e-02, -4.6291e-02,\n",
      "        -1.3215e-01, -4.2017e-03, -6.4696e-02,  7.4318e-02, -3.7038e-02,\n",
      "        -7.4783e-02,  3.7466e-03, -4.4902e-02, -5.4526e-02, -3.2154e-02,\n",
      "        -6.9844e-02, -1.7045e-02, -9.5056e-02,  5.8156e-02, -1.0002e-01,\n",
      "        -2.9361e-03,  5.3961e-02, -1.5724e-02,  1.2291e-01, -7.9840e-02,\n",
      "        -6.2188e-02, -3.2282e-02,  8.2793e-03, -1.1508e-01, -1.4068e-01,\n",
      "        -5.5942e-02, -1.2841e-01, -4.0451e-02, -4.0718e-02, -6.1471e-02,\n",
      "        -8.3071e-02, -1.5063e-02, -2.3455e-02,  1.1974e-02,  7.6468e-02,\n",
      "        -6.5692e-02, -1.8200e-02, -1.4435e-02, -6.2212e-02, -1.2290e-01,\n",
      "        -1.7033e-01, -2.5049e-02, -9.7031e-02,  7.9129e-03, -3.5679e-02,\n",
      "        -6.2448e-03,  5.5563e-02, -9.0274e-02,  3.9126e-02, -1.1295e-01,\n",
      "        -5.1131e-02,  5.9279e-03, -9.5505e-03, -6.1520e-02, -2.4447e-02,\n",
      "        -1.8512e-02,  8.4790e-02, -9.8691e-02, -8.7056e-02, -1.1217e-01,\n",
      "        -1.5344e-01,  5.8255e-02, -1.4153e-01, -7.1881e-03, -1.1657e-03,\n",
      "        -7.4974e-02,  4.0591e-02, -8.0208e-02,  3.8199e-02, -5.8486e-04,\n",
      "         5.0867e-02,  3.5725e-03, -1.4645e-02, -2.7721e-04, -5.1357e-02,\n",
      "        -5.7399e-02, -8.0470e-03, -6.7928e-02,  2.5977e-02, -1.1493e-01,\n",
      "        -2.4930e-02, -1.7211e-02, -1.4713e-01, -2.2575e+00,  3.7316e-02,\n",
      "        -1.8979e-02, -1.0595e-01, -1.0567e-02, -7.6957e-02,  2.2319e-02,\n",
      "        -1.2596e-01, -5.9056e-02,  3.7224e-02,  1.6287e-02, -6.8733e-02,\n",
      "        -4.9052e-02,  4.2215e-02, -6.1539e-03, -9.2575e-02, -2.2745e-02,\n",
      "        -4.3827e-02,  1.5006e-02,  2.1218e-02, -5.9763e-02, -1.6321e-01,\n",
      "        -1.3407e-01,  7.0248e-02, -2.2305e-02,  4.9774e-02, -7.5658e-02,\n",
      "        -5.4044e-02, -1.0192e-01, -7.0567e-02, -4.1365e-02, -1.0061e-01,\n",
      "        -1.5344e-02, -2.0617e-02, -7.9850e-02, -9.0090e-02, -1.2089e-03,\n",
      "        -1.7109e-02, -1.2406e-01, -1.1144e-01, -1.0369e-01, -7.0175e-02,\n",
      "        -5.0133e-02, -1.3557e-02,  1.6262e-01, -9.9455e-02, -7.2051e-02,\n",
      "        -1.1553e-01,  3.9777e-03,  5.5598e-02, -1.8694e-02, -9.6043e-04,\n",
      "         6.9841e-02, -7.7826e-02,  5.5478e-02, -4.6886e-02, -7.8571e-03,\n",
      "        -1.7885e-02, -1.2144e-01,  3.0650e-02,  5.2436e-03, -8.6549e-02,\n",
      "        -1.2067e-01, -1.2578e-01, -3.0886e-02, -1.4648e-01, -1.3343e-01,\n",
      "        -9.9811e-02,  1.0904e-01,  5.2390e-02, -6.2593e-02,  7.3330e-02,\n",
      "        -8.2041e-02, -2.8029e-01, -1.3285e-02, -1.2791e-01,  2.9784e-02,\n",
      "        -4.9950e-02,  1.3312e-02, -2.3558e-04, -1.0833e-01, -9.1062e-02,\n",
      "        -3.6797e-02,  4.6885e-02, -8.6031e-03, -7.7853e-02, -4.7832e-02,\n",
      "         2.6897e-02, -4.3142e-02, -8.8117e-02,  1.3069e-02, -7.0432e-02,\n",
      "        -3.8987e-02, -9.9468e-02,  4.2137e-02,  2.7479e-02, -6.7665e-02,\n",
      "        -4.4414e-02,  7.5395e-03, -2.0309e-03,  4.0419e-02, -1.4872e-01,\n",
      "        -8.2230e-02, -6.8386e-02, -6.8401e-02, -4.1047e-02, -8.8755e-02,\n",
      "         5.6166e-02,  5.6017e-02, -5.5956e-02,  2.1983e-02, -2.2961e-02,\n",
      "        -7.1032e-03, -7.5360e-02,  6.5075e-02, -9.5735e-02,  4.0330e-02,\n",
      "         7.4802e-02,  8.9258e-03,  2.9245e-02, -1.5354e-02,  6.8174e-02,\n",
      "        -1.0322e-01, -2.5460e-03, -1.0041e-01, -1.2218e-02, -8.0847e-02,\n",
      "        -6.2028e-02,  6.3530e-02,  8.3978e-03, -5.7599e-02, -1.0613e-01,\n",
      "        -2.3226e-02,  5.4509e-02,  1.6102e-02, -4.9709e-02, -1.4171e-01,\n",
      "        -1.9604e-01, -9.9754e-02, -9.7179e-02, -8.0188e-02,  3.6264e-02,\n",
      "         3.3763e-02, -1.4644e-02,  1.3093e-01,  8.6540e-03,  2.5184e-02,\n",
      "         1.3682e-02, -9.1370e-02, -6.9037e-04, -2.6415e-02, -6.0097e-02,\n",
      "        -3.5762e-03, -4.9125e-02, -8.7560e-02,  5.8042e-02,  8.4096e-02,\n",
      "         3.6079e-03, -5.9528e-02, -7.0748e-02,  3.0864e-02, -1.0618e-01,\n",
      "        -8.6109e-02, -4.3467e-02, -7.9412e-03,  1.1864e-01, -8.4393e-02,\n",
      "        -6.7752e-02, -1.3682e-02,  1.3190e-01, -4.1765e-02, -9.5315e-02,\n",
      "        -8.5517e-02, -1.0381e-02,  7.4308e-03,  2.5057e-02, -1.2178e-01,\n",
      "        -1.0872e-01, -3.0953e-02, -6.2515e-03, -6.9138e-02, -1.0413e-01,\n",
      "        -9.2249e-02, -8.9106e-03,  1.4600e-01, -1.2124e-01, -8.7356e-02,\n",
      "        -1.2041e-01, -1.1749e-01, -3.5789e-02,  3.8827e-02, -8.9954e-03,\n",
      "        -2.3699e-02, -7.3208e-02, -8.5094e-02, -1.4362e-01, -5.3073e-02,\n",
      "         1.5437e-02, -8.0135e-02,  1.2968e-02, -2.8527e-02, -1.8904e-02,\n",
      "        -1.3154e-01,  3.9962e-02,  7.3063e-02, -2.0348e-01,  3.3176e-02,\n",
      "        -4.0622e-02, -7.5475e-02,  6.5075e-02, -1.1102e-01, -6.7218e-02,\n",
      "        -7.5741e-02,  2.4920e-02, -1.2435e-02, -1.9510e-02, -1.4103e-02,\n",
      "        -6.3889e-02,  1.4682e-01, -1.2999e-01, -4.1460e-02, -1.1495e-01,\n",
      "        -1.9698e-02, -1.1444e-01, -3.0427e-02,  4.8089e-02, -6.2266e-02,\n",
      "        -7.7924e-02,  2.4362e-04, -1.4317e-01, -3.6246e-02, -1.2393e-01,\n",
      "         4.1629e-02,  8.1272e-02, -8.8061e-02, -7.7921e-02, -8.0148e-02,\n",
      "        -9.8167e-02, -8.5414e-02, -1.5176e-02,  1.9671e-02,  5.1377e-02,\n",
      "        -1.5865e-02, -1.1811e-01, -7.8402e-02,  2.3705e-02,  2.9540e-02,\n",
      "        -8.7734e-02, -8.4668e-02,  9.9336e-02, -4.6445e-02, -1.6185e-01,\n",
      "        -1.3077e-01, -4.2604e-02,  5.8688e-02,  1.1358e-02, -1.2765e-01,\n",
      "        -6.1923e-02, -8.4753e-02, -3.0332e-02, -6.0014e-04, -7.5688e-02,\n",
      "        -1.0452e-01, -8.2666e-03,  4.5868e-02, -3.2654e-02, -4.4266e-02,\n",
      "        -5.2327e-02,  8.4792e-02, -7.2354e-02,  2.3774e-02, -3.6025e-02,\n",
      "        -1.1545e-01, -2.8853e-02, -2.0315e-02,  2.3356e-02, -5.5108e-02,\n",
      "        -3.2399e-02,  1.0912e-01, -1.3768e-01, -9.9473e-02, -1.2217e-02,\n",
      "        -9.2016e-02, -3.4299e-02,  1.1813e-02, -2.0268e-01, -1.2499e-02,\n",
      "         3.3174e-02, -1.0902e-01, -2.4698e-02, -1.8718e-02, -1.3656e-02,\n",
      "        -1.8760e-02, -5.9220e-02, -2.1743e-02, -5.2343e-02,  6.8899e-02,\n",
      "         3.8413e-02, -1.1812e-01,  4.0992e-02, -8.7708e-02,  6.0118e-02,\n",
      "         1.1129e-01, -2.3863e-03, -2.4479e-02,  5.5275e-02, -4.8966e-02,\n",
      "        -1.3996e-01, -1.2337e-01,  5.8795e-03,  1.5241e-02, -1.1743e-01,\n",
      "         2.0023e-02, -1.0781e-02, -8.2880e-02, -6.0243e-02,  9.4822e-03,\n",
      "        -1.2696e-01,  4.8240e-03, -6.5649e-02,  6.8928e-03, -5.0116e-02,\n",
      "         8.7673e-02,  3.1290e-02, -2.8279e-02,  1.1816e-02, -7.9418e-03,\n",
      "         3.5138e-02, -3.7828e-02,  1.0395e-01,  5.2070e-02,  1.1879e-02,\n",
      "        -3.6229e-02,  5.4177e-02, -1.7798e-01, -9.3546e-02,  3.3682e-02,\n",
      "        -1.0662e-01, -3.3063e-02,  2.0191e-02,  5.7537e-02, -1.8093e-02,\n",
      "        -9.0532e-02,  9.5468e-02, -7.4289e-02, -2.5821e-02, -2.6561e-02,\n",
      "        -4.5162e-02,  2.9504e-02, -3.1473e-02,  2.8452e-02, -2.9851e-02,\n",
      "        -2.8579e-02, -9.0557e-02,  5.2943e-03, -5.8344e-02,  4.7541e-02,\n",
      "        -2.5308e-02, -1.0288e-01, -4.9240e-02, -5.8421e-02,  1.6592e-02,\n",
      "         1.2111e-01, -5.4313e-02, -8.1466e-02, -4.9400e-02,  7.0303e-03,\n",
      "        -4.2782e-02, -4.3525e-02, -8.5807e-03,  6.2815e-02,  5.9304e-02,\n",
      "        -1.0129e-01, -6.6303e-02, -7.5956e-03, -6.3686e-02, -2.5517e-02,\n",
      "        -2.0086e-02, -6.0147e-02, -2.3865e-02, -3.3763e-02, -7.4924e-02,\n",
      "        -1.3382e-01, -1.6790e-01, -1.3245e-01, -3.7097e-02,  2.2390e-02,\n",
      "        -1.0047e-01, -8.1847e-02, -7.7880e-02,  5.0404e-02, -1.4665e-01,\n",
      "         5.9582e-02, -1.1912e-02, -1.1096e-01, -1.0069e-01,  6.5853e-02,\n",
      "         1.5271e-02,  4.4003e-02,  1.9579e-02,  4.8464e-02,  6.3348e-02,\n",
      "        -6.8227e-02,  3.1810e-02, -3.2205e-02,  8.8901e-02, -5.0658e-02,\n",
      "        -1.0127e-01, -1.6246e-01,  1.6157e-02, -4.4182e-02, -6.6988e-02,\n",
      "        -5.1723e-01,  1.4864e-02, -7.6332e-02, -7.5355e-03,  2.3505e-02,\n",
      "        -4.6339e-02, -1.4055e-02, -1.6824e-01,  7.3489e-03, -3.3287e-02,\n",
      "        -6.3824e-04,  5.9921e-03, -8.3247e-02,  9.5896e-02,  3.1140e-02,\n",
      "        -5.0645e-02, -1.2309e-01,  2.1778e-02, -8.4379e-02, -7.0738e-02,\n",
      "        -8.9048e-02, -4.2593e-02, -1.2673e-01, -1.4987e-01,  8.0047e-02,\n",
      "         7.0084e-03, -8.5599e-02, -6.0880e-02, -5.7156e-02, -8.4430e-03,\n",
      "        -6.1077e-02, -9.3670e-02, -6.0107e-02, -5.3923e-02, -5.4385e-02,\n",
      "        -1.1415e-01, -1.8597e-01, -2.0845e-02, -4.5953e-02,  1.9309e-02,\n",
      "         9.4666e-02, -1.1881e-01, -4.2700e-03, -1.2863e-01, -4.4709e-02,\n",
      "        -4.3273e-02,  5.3781e-02,  2.5261e-02], requires_grad=True) Parameter containing:\n",
      "tensor([0.8520, 0.8020, 0.8554, 0.8150, 0.8442, 0.8227, 0.8563, 0.8517, 0.8476,\n",
      "        0.8635, 0.8254, 0.8606, 0.8426, 0.8118, 0.8281, 0.9073, 0.8642, 0.8385,\n",
      "        0.8235, 0.8600, 0.8387, 0.8405, 0.8271, 0.8326, 0.8258, 0.8467, 0.8510,\n",
      "        0.8035, 0.8450, 0.8336, 0.8817, 0.8028, 0.8418, 0.8576, 0.8510, 0.8368,\n",
      "        0.8673, 0.8282, 0.8233, 0.8328, 0.8379, 0.8346, 0.8188, 0.8750, 0.8391,\n",
      "        0.8413, 1.1254, 0.8492, 0.8118, 0.8285, 0.8939, 0.8430, 0.8605, 0.8560,\n",
      "        0.8493, 0.8606, 0.8180, 0.8470, 0.8082, 0.8299, 0.8415, 0.8650, 0.8327,\n",
      "        0.8603, 0.8658, 0.8812, 0.8387, 0.8172, 0.8931, 0.8885, 0.8196, 0.8302,\n",
      "        0.8447, 0.8454, 0.8261, 0.8812, 0.8591, 0.8650, 0.8281, 0.8417, 0.8359,\n",
      "        0.8303, 0.8623, 0.8417, 0.8185, 0.8217, 0.8374, 0.8196, 0.8117, 0.9004,\n",
      "        0.8762, 0.8366, 0.8584, 0.8453, 0.8473, 0.8528, 0.8545, 0.8678, 0.8649,\n",
      "        0.8712, 0.8332, 0.8691, 0.8300, 0.8504, 0.8626, 0.8236, 0.8181, 0.8370,\n",
      "        0.8406, 0.9652, 0.8724, 0.8404, 0.8218, 0.8162, 0.8251, 0.8224, 0.8512,\n",
      "        0.8259, 0.8450, 0.8153, 0.8587, 0.8993, 0.8241, 1.2920, 0.8287, 0.8297,\n",
      "        0.8681, 0.8488, 0.8393, 0.8451, 0.8472, 0.8094, 0.8018, 0.8529, 0.8470,\n",
      "        0.8798, 0.8535, 0.8492, 0.8686, 0.8382, 0.8385, 0.8307, 1.0296, 0.9308,\n",
      "        0.8808, 0.8298, 0.8514, 0.8985, 0.8438, 0.8417, 0.8342, 0.8715, 0.8730,\n",
      "        0.8460, 0.8054, 0.8398, 0.8586, 0.8332, 0.8217, 0.7981, 0.8342, 0.8277,\n",
      "        0.8168, 0.8385, 0.8649, 0.8550, 0.8443, 0.8911, 0.8568, 0.8623, 0.8185,\n",
      "        0.8600, 0.8310, 0.8460, 0.8509, 0.8397, 0.8454, 0.8457, 0.8253, 0.8643,\n",
      "        1.1840, 0.8535, 0.8279, 0.8479, 0.8503, 0.8324, 0.8675, 0.8430, 0.8236,\n",
      "        0.8451, 0.8621, 0.8554, 0.8516, 0.8620, 0.8416, 0.8643, 0.8535, 0.8616,\n",
      "        0.8649, 0.8345, 0.8596, 0.8026, 0.8355, 0.8560, 0.8635, 1.2936, 0.8091,\n",
      "        0.8206, 0.8587, 0.8788, 0.8294, 0.8213, 0.8474, 0.8747, 0.8431, 0.8346,\n",
      "        0.8436, 0.8378, 0.8255, 0.8402, 0.8609, 0.8418, 0.8252, 0.8469, 0.8228,\n",
      "        1.0993, 0.8294, 0.8562, 0.8261, 1.0967, 0.8632, 0.8498, 0.8616, 0.8442,\n",
      "        0.8458, 0.8609, 0.8296, 0.8142, 0.8627, 0.8550, 0.8378, 0.8412, 0.8500,\n",
      "        0.9119, 0.8637, 0.8188, 0.8246, 0.8469, 0.8447, 0.8039, 0.8527, 0.8527,\n",
      "        0.8055, 0.8358, 0.8419, 0.8348, 0.8158, 0.8385, 0.8372, 0.8460, 0.8587,\n",
      "        0.8313, 0.8400, 0.8633, 0.8494, 0.8659, 0.8852, 0.8484, 0.8725, 0.8815,\n",
      "        0.8591, 0.8487, 0.8529, 0.9536, 0.8555, 0.8516, 0.8515, 0.8438, 0.8383,\n",
      "        0.8323, 0.8229, 0.8680, 0.8332, 0.8290, 0.8668, 0.8555, 0.8582, 0.9376,\n",
      "        0.8376, 0.8351, 0.8133, 0.8465, 0.8208, 0.8636, 0.8641, 0.8529, 0.8250,\n",
      "        0.8188, 0.8456, 0.8390, 0.8307, 0.8390, 0.8566, 0.8196, 0.7716, 0.8295,\n",
      "        0.8440, 0.8294, 1.6117, 0.8321, 0.8725, 0.8724, 0.8583, 0.8386, 0.8363,\n",
      "        0.7997, 0.7855, 0.8234, 0.8731, 0.8391, 0.8603, 0.8488, 0.8502, 0.8635,\n",
      "        0.8604, 0.8325, 0.8267, 0.9433, 0.8411, 0.8297, 0.8507, 0.8619, 0.8198,\n",
      "        0.8596, 0.9341, 0.8629, 0.8587, 0.8290, 0.8592, 0.8635, 0.8309, 0.7260,\n",
      "        0.8483, 0.8215, 0.8450, 0.8619, 0.8555, 0.8625, 0.8018, 0.8728, 0.8412,\n",
      "        0.8330, 0.9033, 0.8347, 0.8568, 0.8650, 0.8478, 0.8590, 0.8300, 0.8117,\n",
      "        0.9446, 0.8524, 0.8504, 0.8431, 0.8505, 0.8232, 0.8646, 0.8924, 0.8613,\n",
      "        0.8237, 0.8907, 0.8542, 0.8381, 0.8493, 0.9394, 0.8559, 0.8457, 0.8347,\n",
      "        0.8384, 0.8523, 0.8497, 0.7008, 0.8630, 0.8686, 0.8991, 0.8517, 0.8630,\n",
      "        0.8588, 0.8568, 0.8810, 0.8367, 0.8362, 0.8216, 0.8617, 0.8304, 0.7871,\n",
      "        0.8425, 0.8594, 0.8298, 0.8511, 0.8365, 0.8434, 0.8607, 0.8294, 0.8358,\n",
      "        0.8798, 0.8147, 0.8358, 0.8634, 0.8583, 0.8488, 0.8582, 0.8690, 0.8204,\n",
      "        0.8500, 0.8367, 0.8347, 0.8550, 0.8554, 0.8579, 0.8344, 0.8463, 0.8962,\n",
      "        0.8290, 0.8439, 0.8604, 0.8367, 0.7990, 0.8059, 0.8492, 0.8747, 0.8827,\n",
      "        0.8564, 0.8559, 0.8367, 0.8087, 0.8446, 0.8243, 0.8188, 0.8481, 0.8595,\n",
      "        0.8728, 0.8444, 0.8342, 0.8165, 0.9047, 0.8274, 0.8475, 0.8678, 0.8303,\n",
      "        0.8312, 0.8790, 0.8942, 0.8525, 0.8330, 0.8378, 0.8249, 0.8349, 0.8707,\n",
      "        0.8478, 0.8532, 0.8067, 0.8345, 0.8687, 0.8831, 0.8205, 0.8745, 0.8849,\n",
      "        0.8116, 0.8607, 1.0646, 0.8105, 0.8714, 0.8553, 0.8233, 0.8396, 0.8215,\n",
      "        0.8131, 0.8671, 0.8644, 0.8547, 0.8483, 0.8177, 0.8354, 0.8391, 0.8526,\n",
      "        0.8758, 0.8899, 0.8336, 0.8404, 0.8413, 0.8585, 0.8640, 0.8350, 0.8547,\n",
      "        0.8350, 0.8335, 0.8430, 0.8507, 0.8385, 0.8965, 0.8858, 0.8368, 0.8680,\n",
      "        0.8462, 0.8884, 0.8296, 0.8275, 0.8526, 0.8689, 0.8584, 0.8190, 0.8756,\n",
      "        0.8311, 0.9045, 0.8605, 0.8543, 0.8210, 0.8476, 0.8063, 0.8726, 0.8491,\n",
      "        0.8436, 0.8547, 0.8466, 0.8363, 1.0828, 0.8279, 0.8633, 0.8279, 0.8396,\n",
      "        0.8248, 0.8555, 0.8601, 0.8649, 0.9414, 0.8409, 0.8382, 0.8422, 0.7070,\n",
      "        0.8349, 0.8393, 0.8890, 0.8678, 0.8809, 0.8638, 0.8489, 0.8288, 0.8630,\n",
      "        0.8506, 0.8118, 0.8649, 0.8647, 0.8499, 0.8278, 0.8581, 0.7798, 0.8732,\n",
      "        0.8623, 0.8570, 0.8415, 0.8307, 0.8156, 0.8518, 0.8377, 0.8572, 0.8434,\n",
      "        0.8699, 0.8062, 0.8603, 0.8478, 0.8088, 0.8536, 0.8418, 0.8255, 0.8336,\n",
      "        0.8214, 0.8632, 0.8536, 0.8572, 0.8295, 0.8693, 0.8435, 0.8618, 0.8350,\n",
      "        0.8450, 0.8762, 0.8235, 0.8998, 0.8017, 0.8178, 0.8160, 0.8456, 0.8700,\n",
      "        0.8464, 0.8341, 0.8547, 0.8416, 0.8600, 0.8253, 0.8222, 0.8422, 0.8318,\n",
      "        0.8716, 0.8259, 0.8703, 0.8493, 0.8493, 0.8867, 0.8211, 0.8444, 0.8451,\n",
      "        0.8555, 0.8471, 0.8320, 0.8377, 0.8427, 0.8544, 0.8529, 0.8680, 0.8911,\n",
      "        0.8428, 0.8413, 0.8471, 0.8487, 0.8346, 0.8576, 0.8398, 0.8257, 0.8380,\n",
      "        0.8574, 0.8316, 0.8696, 0.8554, 0.8419, 0.8407, 0.8487, 0.8443, 0.8418,\n",
      "        0.8603, 0.8382, 0.8771, 0.8511, 0.8505, 0.8360, 0.8882, 0.8473, 0.9015,\n",
      "        0.8756, 0.8869, 0.8460, 0.8616, 0.8442, 0.8155, 0.8514, 0.8648, 0.8657,\n",
      "        0.8804, 0.8519, 0.8621, 0.8479, 0.8607, 0.8232, 0.8773, 0.8574, 0.8427,\n",
      "        0.8494, 0.9454, 0.8615, 0.8602, 0.9206, 0.8314, 0.8427, 0.8468, 0.8528,\n",
      "        0.8728, 0.8320, 0.8267, 0.8138, 0.8464, 0.8511, 0.8437, 0.8251, 0.8903,\n",
      "        0.8346, 0.8762, 0.8286, 0.8463, 0.8644, 0.8704, 0.8401, 0.8421, 0.8382,\n",
      "        0.8371, 0.8343, 0.8397, 0.8621, 0.8402, 0.8429, 0.8619, 0.8596, 0.8597,\n",
      "        0.8786, 0.8325, 0.8564, 0.8275, 0.8584, 0.8863, 0.8499, 0.8511, 0.8212,\n",
      "        0.8188, 0.8686, 0.8701, 0.8482, 0.8704, 0.8937, 0.8291, 0.8391, 0.8516,\n",
      "        1.5371, 0.8554, 0.8568, 0.8563, 0.8404, 0.8409, 0.8398, 0.8875, 0.8691,\n",
      "        0.8495, 0.8180, 0.8389, 0.8536, 0.6582, 0.8556, 0.8524, 0.8522, 0.8569,\n",
      "        0.8311, 0.8471, 0.8558, 0.8501, 0.8438, 0.8706, 0.8647, 0.8470, 0.8286,\n",
      "        0.8387, 0.8455, 0.8571, 0.9022, 0.8090, 1.0631, 0.8578, 0.8398, 0.8367,\n",
      "        0.9711, 0.8156, 0.8653, 0.8613, 0.7998, 0.7831, 0.8282, 0.8466, 0.8523,\n",
      "        0.8346, 0.8365, 0.8424], requires_grad=True)\n",
      "intermediate output  tensor([[[-0.0795, -0.1383,  0.1472,  ..., -0.1659,  0.1462, -0.1471],\n",
      "         [-0.0750, -0.0026, -0.0566,  ..., -0.0976, -0.1695, -0.1506],\n",
      "         [-0.0193, -0.0067, -0.1546,  ..., -0.1030, -0.1008, -0.1242],\n",
      "         ...,\n",
      "         [-0.1177, -0.0864, -0.0799,  ..., -0.1243, -0.0760, -0.1461],\n",
      "         [-0.1072, -0.0839, -0.0761,  ..., -0.1122, -0.1308, -0.1398],\n",
      "         [-0.1095, -0.0965, -0.0903,  ..., -0.1048, -0.1027, -0.1128]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "layer done  tensor([[[-0.6281,  0.1932, -0.7518,  ..., -1.0646,  0.5816,  0.5693],\n",
      "         [-0.7641, -0.3282, -0.3601,  ...,  0.0180,  0.6060, -0.2501],\n",
      "         [ 0.2143, -0.5702, -0.3042,  ..., -0.1024,  0.4167,  0.0218],\n",
      "         ...,\n",
      "         [-0.3970, -0.3921, -0.0527,  ...,  0.1927,  0.5087, -0.2546],\n",
      "         [-0.4303, -0.2716,  0.0897,  ...,  0.2107,  0.4422, -0.2477],\n",
      "         [-0.4749, -0.3964,  0.1256,  ...,  0.3534,  0.3822, -0.2507]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "layer 0 0\n",
      "layer 1 1\n",
      "layer 2 2\n",
      "layer 3 3\n",
      "layer 4 4\n",
      "layer 5 5\n",
      "layer 6 6\n",
      "layer 7 7\n",
      "layer 8 8\n",
      "layer 9 9\n",
      "layer 10 10\n",
      "layer 11 11\n"
     ]
    }
   ],
   "source": [
    "layer_indexes = list(range(12))\n",
    "\n",
    "pytorch_all_out = []\n",
    "for input_ids, input_mask, input_type_ids, example_indices in eval_dataloader:\n",
    "    print(input_ids)\n",
    "    print(input_mask)\n",
    "    print(example_indices)\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "\n",
    "    all_encoder_layers, _ = model(input_ids, token_type_ids=input_type_ids, attention_mask=input_mask)\n",
    "\n",
    "    for b, example_index in enumerate(example_indices):\n",
    "        feature = features[example_index.item()]\n",
    "        unique_id = int(feature.unique_id)\n",
    "        # feature = unique_id_to_feature[unique_id]\n",
    "        output_json = collections.OrderedDict()\n",
    "        output_json[\"linex_index\"] = unique_id\n",
    "        all_out_features = []\n",
    "        # for (i, token) in enumerate(feature.tokens):\n",
    "        all_layers = []\n",
    "        for (j, layer_index) in enumerate(layer_indexes):\n",
    "            print(\"layer\", j, layer_index)\n",
    "            layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()\n",
    "            layer_output = layer_output[b]\n",
    "            layers = collections.OrderedDict()\n",
    "            layers[\"index\"] = layer_index\n",
    "            layer_output = layer_output\n",
    "            layers[\"values\"] = layer_output if not isinstance(layer_output, (int, float)) else [layer_output]\n",
    "            all_layers.append(layers)\n",
    "\n",
    "            out_features = collections.OrderedDict()\n",
    "            out_features[\"layers\"] = all_layers\n",
    "            all_out_features.append(out_features)\n",
    "        output_json[\"features\"] = all_out_features\n",
    "        pytorch_all_out.append(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:35.703615Z",
     "start_time": "2018-11-15T15:21:35.666150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "odict_keys(['linex_index', 'features'])\n",
      "number of tokens 1\n",
      "number of layers 12\n",
      "hidden_size 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pytorch_all_out))\n",
    "print(len(pytorch_all_out[0]))\n",
    "print(pytorch_all_out[0].keys())\n",
    "print(\"number of tokens\", len(pytorch_all_out))\n",
    "print(\"number of layers\", len(pytorch_all_out[0]['features'][0]['layers']))\n",
    "print(\"hidden_size\", len(pytorch_all_out[0]['features'][0]['layers'][0]['values']))\n",
    "pytorch_all_out[0]['features'][0]['layers'][0]['values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:36.999073Z",
     "start_time": "2018-11-15T15:21:36.966762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 768)\n",
      "(128, 768)\n"
     ]
    }
   ],
   "source": [
    "pytorch_outputs = list(pytorch_all_out[0]['features'][0]['layers'][t]['values'] for t in layer_indexes)\n",
    "print(pytorch_outputs[0].shape)\n",
    "print(pytorch_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:37.936522Z",
     "start_time": "2018-11-15T15:21:37.905269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 768)\n",
      "(128, 768)\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0].shape)\n",
    "print(tensorflow_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10810544  0.00736203 -0.14134324 ...  0.08043151  0.07175563\n",
      "   0.0031992 ]\n",
      " [-0.00526232  0.6327945  -0.2985075  ...  0.10594425  0.09061253\n",
      "  -0.76824725]\n",
      " [-0.3182612  -0.8120704   0.15033704 ... -0.1900597   0.15686822\n",
      "   0.12246863]\n",
      " ...\n",
      " [ 0.09414048 -0.33054894  0.61384857 ...  0.43929374 -0.3086228\n",
      "   0.06017733]\n",
      " [ 0.01996002 -0.37984183  0.49045902 ...  0.45061845 -0.21570973\n",
      "  -0.05887301]\n",
      " [ 0.15295641 -0.2668718   0.49672574 ...  0.7504021  -0.5253611\n",
      "  -0.10960616]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[0])\n",
    "# for i in range(tensorflow_outputs[0].shape[0]):\n",
    "#     print(tensorflow_outputs[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10828765  0.0067381  -0.14161144 ...  0.08049869  0.07187192\n",
      "   0.00301047]\n",
      " [-0.00542086  0.63183767 -0.29846492 ...  0.10570657  0.09073348\n",
      "  -0.7689243 ]\n",
      " [-0.31781077 -0.8129925   0.15038896 ... -0.18991752  0.15697755\n",
      "   0.12239677]\n",
      " ...\n",
      " [ 0.09426479 -0.33084315  0.61350524 ...  0.4392033  -0.30860138\n",
      "   0.05975728]\n",
      " [ 0.02007423 -0.3801041   0.49014768 ...  0.4505022  -0.21570988\n",
      "  -0.05932539]\n",
      " [ 0.15316041 -0.26719052  0.49640608 ...  0.75035083 -0.52535987\n",
      "  -0.1100089 ]]\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.10810544  0.00736203 -0.14134324 ...  0.08043151  0.07175563\n",
      "   0.0031992 ]\n",
      " [-0.00526232  0.6327945  -0.2985075  ...  0.10594425  0.09061253\n",
      "  -0.76824725]\n",
      " [-0.3182612  -0.8120704   0.15033704 ... -0.1900597   0.15686822\n",
      "   0.12246863]\n",
      " ...\n",
      " [ 0.09414048 -0.33054894  0.61384857 ...  0.43929374 -0.3086228\n",
      "   0.06017733]\n",
      " [ 0.01996002 -0.37984183  0.49045902 ...  0.45061845 -0.21570973\n",
      "  -0.05887301]\n",
      " [ 0.15295641 -0.2668718   0.49672574 ...  0.7504021  -0.5253611\n",
      "  -0.10960616]]\n",
      "0 [[ 0.10828765  0.0067381  -0.14161144 ...  0.08049869  0.07187192\n",
      "   0.00301047]\n",
      " [-0.00542086  0.63183767 -0.29846492 ...  0.10570657  0.09073348\n",
      "  -0.7689243 ]\n",
      " [-0.31781077 -0.8129925   0.15038896 ... -0.18991752  0.15697755\n",
      "   0.12239677]\n",
      " ...\n",
      " [ 0.09426479 -0.33084315  0.61350524 ...  0.4392033  -0.30860138\n",
      "   0.05975728]\n",
      " [ 0.02007423 -0.3801041   0.49014768 ...  0.4505022  -0.21570988\n",
      "  -0.05932539]\n",
      " [ 0.15316041 -0.26719052  0.49640608 ...  0.75035083 -0.52535987\n",
      "  -0.1100089 ]]\n",
      "0 [[-1.82211399e-04  6.23929314e-04  2.68206000e-04 ... -6.71818852e-05\n",
      "  -1.16288662e-04  1.88734382e-04]\n",
      " [ 1.58533454e-04  9.56833363e-04 -4.25875187e-05 ...  2.37680972e-04\n",
      "  -1.20952725e-04  6.77049160e-04]\n",
      " [-4.50432301e-04  9.22083855e-04 -5.19156456e-05 ... -1.42186880e-04\n",
      "  -1.09329820e-04  7.18608499e-05]\n",
      " ...\n",
      " [-1.24305487e-04  2.94208527e-04  3.43322754e-04 ...  9.04500484e-05\n",
      "  -2.14278698e-05  4.20052558e-04]\n",
      " [-1.14209950e-04  2.62260437e-04  3.11344862e-04 ...  1.16258860e-04\n",
      "   1.49011612e-07  4.52380627e-04]\n",
      " [-2.03996897e-04  3.18706036e-04  3.19659710e-04 ...  5.12599945e-05\n",
      "  -1.25169754e-06  4.02741134e-04]]\n",
      "1 [[-0.11897288 -0.20198373 -0.2884692  ...  0.14798686  0.09036567\n",
      "   0.03352347]\n",
      " [-0.42948273  0.5473927  -0.23947474 ...  0.45206162 -0.12681447\n",
      "  -1.3169142 ]\n",
      " [-0.3233264  -0.23859246  0.1108414  ... -0.2591034   0.24496184\n",
      "  -0.517779  ]\n",
      " ...\n",
      " [-0.1382316  -0.3729512   0.650605   ...  0.8346486  -0.50753295\n",
      "  -0.07884623]\n",
      " [-0.27554965 -0.24870181  0.58153665 ...  0.8746397  -0.42844853\n",
      "  -0.16796018]\n",
      " [-0.05959707 -0.36331248  0.5175965  ...  1.0593148  -0.69793767\n",
      "  -0.27391958]]\n",
      "1 [[-0.11858806 -0.20275524 -0.28900433 ...  0.14802465  0.08992688\n",
      "   0.03356105]\n",
      " [-0.43029398  0.5465253  -0.23932356 ...  0.4524782  -0.1272927\n",
      "  -1.3178267 ]\n",
      " [-0.32292652 -0.23925197  0.11096834 ... -0.259128    0.24450377\n",
      "  -0.5181512 ]\n",
      " ...\n",
      " [-0.13780129 -0.3733201   0.6494114  ...  0.83477426 -0.5074718\n",
      "  -0.07907225]\n",
      " [-0.2751303  -0.24906793  0.5805389  ...  0.8745948  -0.42865598\n",
      "  -0.16832529]\n",
      " [-0.05909459 -0.3636416   0.51646966 ...  1.059266   -0.6978907\n",
      "  -0.27393666]]\n",
      "1 [[-3.8482249e-04  7.7150762e-04  5.3513050e-04 ... -3.7789345e-05\n",
      "   4.3879449e-04 -3.7588179e-05]\n",
      " [ 8.1124902e-04  8.6742640e-04 -1.5118718e-04 ... -4.1657686e-04\n",
      "   4.7822297e-04  9.1254711e-04]\n",
      " [-3.9988756e-04  6.5951049e-04 -1.2693554e-04 ...  2.4616718e-05\n",
      "   4.5807660e-04  3.7223101e-04]\n",
      " ...\n",
      " [-4.3031573e-04  3.6889315e-04  1.1936426e-03 ... -1.2564659e-04\n",
      "  -6.1154366e-05  2.2602081e-04]\n",
      " [-4.1934848e-04  3.6612153e-04  9.9772215e-04 ...  4.4882298e-05\n",
      "   2.0745397e-04  3.6510825e-04]\n",
      " [-5.0248206e-04  3.2910705e-04  1.1268258e-03 ...  4.8875809e-05\n",
      "  -4.6968460e-05  1.7076731e-05]]\n",
      "2 [[-0.10065358 -0.22039229 -0.09111832 ...  0.1500864   0.08662349\n",
      "   0.1169717 ]\n",
      " [-0.7875237   0.4987949   0.47532898 ...  0.3109045  -0.3090303\n",
      "  -1.0728124 ]\n",
      " [-0.48414615 -0.17314494  0.34687328 ... -0.337559    0.28318426\n",
      "  -0.5799342 ]\n",
      " ...\n",
      " [-0.36874118 -0.23636177  0.78799486 ...  0.8991036  -0.23088121\n",
      "  -0.26291594]\n",
      " [-0.46867168 -0.18532905  0.6907864  ...  0.9044033  -0.15563615\n",
      "  -0.38067216]\n",
      " [-0.23992004 -0.31867903  0.6687768  ...  1.0202096  -0.36089402\n",
      "  -0.44989982]]\n",
      "2 [[-0.10061163 -0.22088018 -0.09148235 ...  0.1505027   0.0859358\n",
      "   0.1165626 ]\n",
      " [-0.7887718   0.4992637   0.47503558 ...  0.31165406 -0.309697\n",
      "  -1.0737016 ]\n",
      " [-0.4838563  -0.1733253   0.34664014 ... -0.33861744  0.28320512\n",
      "  -0.5806207 ]\n",
      " ...\n",
      " [-0.36850855 -0.23603436  0.7872675  ...  0.8992588  -0.2316817\n",
      "  -0.2630628 ]\n",
      " [-0.46812162 -0.18516764  0.6906037  ...  0.90450734 -0.15605962\n",
      "  -0.38073644]\n",
      " [-0.23968017 -0.31835684  0.66848457 ...  1.0203274  -0.36133373\n",
      "  -0.4498411 ]]\n",
      "2 [[-4.19542193e-05  4.87893820e-04  3.64027917e-04 ... -4.16293740e-04\n",
      "   6.87688589e-04  4.09096479e-04]\n",
      " [ 1.24812126e-03 -4.68790531e-04  2.93403864e-04 ... -7.49558210e-04\n",
      "   6.66707754e-04  8.89182091e-04]\n",
      " [-2.89857388e-04  1.80363655e-04  2.33143568e-04 ...  1.05842948e-03\n",
      "  -2.08616257e-05  6.86526299e-04]\n",
      " ...\n",
      " [-2.32636929e-04 -3.27408314e-04  7.27355480e-04 ... -1.55210495e-04\n",
      "   8.00490379e-04  1.46865845e-04]\n",
      " [-5.50061464e-04 -1.61409378e-04  1.82747841e-04 ... -1.04010105e-04\n",
      "   4.23476100e-04  6.42836094e-05]\n",
      " [-2.39863992e-04 -3.22192907e-04  2.92241573e-04 ... -1.17897987e-04\n",
      "   4.39703465e-04 -5.87105751e-05]]\n",
      "3 [[-0.16956973 -0.5298302  -0.6238723  ... -0.12010702  0.160947\n",
      "   0.47763437]\n",
      " [-0.79321176 -0.11029729  0.71102816 ...  0.45669833 -0.5436332\n",
      "  -1.3896017 ]\n",
      " [-0.46882278 -0.15283072  0.34094727 ... -0.46597004  0.19818446\n",
      "  -0.18980475]\n",
      " ...\n",
      " [-0.6629108  -0.84915125  0.7535677  ...  0.52906305 -0.7499684\n",
      "  -0.15863109]\n",
      " [-0.6896764  -0.7735596   0.71426755 ...  0.6853264  -0.63519114\n",
      "  -0.30325097]\n",
      " [-0.41451818 -0.8914872   0.8254208  ...  0.8305949  -0.7876378\n",
      "  -0.43717545]]\n",
      "3 [[-0.16978028 -0.5306821  -0.62416893 ... -0.12001255  0.15971118\n",
      "   0.47731337]\n",
      " [-0.7937946  -0.10897183  0.7107842  ...  0.45760936 -0.5453817\n",
      "  -1.3903819 ]\n",
      " [-0.46787012 -0.15311995  0.34108117 ... -0.46696624  0.19769493\n",
      "  -0.1903292 ]\n",
      " ...\n",
      " [-0.66149354 -0.84910786  0.7529021  ...  0.5291563  -0.75036556\n",
      "  -0.15842873]\n",
      " [-0.6877791  -0.7739458   0.71423614 ...  0.68604606 -0.6350458\n",
      "  -0.30297834]\n",
      " [-0.413033   -0.8914739   0.82464564 ...  0.8306885  -0.7878569\n",
      "  -0.43656287]]\n",
      "3 [[ 2.1055341e-04  8.5186958e-04  2.9665232e-04 ... -9.4473362e-05\n",
      "   1.2358129e-03  3.2100081e-04]\n",
      " [ 5.8281422e-04 -1.3254583e-03  2.4396181e-04 ... -9.1102719e-04\n",
      "   1.7485023e-03  7.8022480e-04]\n",
      " [-9.5266104e-04  2.8923154e-04 -1.3390183e-04 ...  9.9620223e-04\n",
      "   4.8953295e-04  5.2444637e-04]\n",
      " ...\n",
      " [-1.4172792e-03 -4.3392181e-05  6.6560507e-04 ... -9.3281269e-05\n",
      "   3.9714575e-04 -2.0235777e-04]\n",
      " [-1.8972754e-03  3.8623810e-04  3.1411648e-05 ... -7.1966648e-04\n",
      "  -1.4531612e-04 -2.7263165e-04]\n",
      " [-1.4851689e-03 -1.3291836e-05  7.7515841e-04 ... -9.3579292e-05\n",
      "   2.1904707e-04 -6.1258674e-04]]\n",
      "4 [[-0.54007185  0.05002726 -0.89557606 ... -0.26914373  0.43056202\n",
      "   0.3108974 ]\n",
      " [-0.4531035  -0.5789233   0.08489337 ...  0.6713315  -0.4799617\n",
      "  -1.5545336 ]\n",
      " [-0.74064267 -0.7101188   0.13860293 ... -0.6605276   0.799326\n",
      "  -0.01612026]\n",
      " ...\n",
      " [-0.6225507  -0.8654652   0.6163627  ... -0.07588618 -0.57280046\n",
      "  -0.0834857 ]\n",
      " [-0.67378694 -0.7609527   0.81480783 ...  0.2166681  -0.59974736\n",
      "  -0.39639023]\n",
      " [-0.36915848 -0.8968721   0.9678703  ...  0.39923286 -0.6669936\n",
      "  -0.4843124 ]]\n",
      "4 [[-0.5410976   0.04835665 -0.896015   ... -0.26799214  0.42967474\n",
      "   0.3098511 ]\n",
      " [-0.4545353  -0.57881457  0.08439159 ...  0.67245036 -0.48261362\n",
      "  -1.5543356 ]\n",
      " [-0.74104905 -0.7120805   0.1387337  ... -0.66255575  0.79913247\n",
      "  -0.01594212]\n",
      " ...\n",
      " [-0.6218218  -0.8655435   0.61396474 ... -0.07474855 -0.57485425\n",
      "  -0.08350234]\n",
      " [-0.6721684  -0.7621208   0.81328666 ...  0.21863632 -0.6010748\n",
      "  -0.3967684 ]\n",
      " [-0.3682408  -0.89730483  0.96589166 ...  0.40005133 -0.668887\n",
      "  -0.48446015]]\n",
      "4 [[ 1.0257363e-03  1.6706064e-03  4.3892860e-04 ... -1.1515915e-03\n",
      "   8.8727474e-04  1.0462999e-03]\n",
      " [ 1.4317930e-03 -1.0871887e-04  5.0178170e-04 ... -1.1188388e-03\n",
      "   2.6519299e-03 -1.9800663e-04]\n",
      " [ 4.0638447e-04  1.9617081e-03 -1.3077259e-04 ...  2.0281672e-03\n",
      "   1.9353628e-04 -1.7814897e-04]\n",
      " ...\n",
      " [-7.2890520e-04  7.8260899e-05  2.3979545e-03 ... -1.1376217e-03\n",
      "   2.0537972e-03  1.6644597e-05]\n",
      " [-1.6185641e-03  1.1680722e-03  1.5211701e-03 ... -1.9682199e-03\n",
      "   1.3274550e-03  3.7816167e-04]\n",
      " [-9.1767311e-04  4.3272972e-04  1.9786358e-03 ... -8.1846118e-04\n",
      "   1.8934011e-03  1.4775991e-04]]\n",
      "5 [[-0.51608485  0.54645395 -1.4612309  ... -0.17483857  0.11642317\n",
      "   0.2523068 ]\n",
      " [-0.51161134 -0.560762    0.33194834 ...  0.69935995 -0.4279396\n",
      "  -1.6298282 ]\n",
      " [-0.4174652  -0.55307245 -0.2042126  ... -0.6533516   0.61767024\n",
      "  -0.05461235]\n",
      " ...\n",
      " [-1.0404124  -0.97312915  0.40119526 ...  0.07897779 -0.6028683\n",
      "   0.06049548]\n",
      " [-1.0183474  -0.8874264   0.83832115 ...  0.42651695 -0.64047295\n",
      "  -0.27007008]\n",
      " [-0.68482256 -1.0121242   0.94070363 ...  0.46813852 -0.71701115\n",
      "  -0.42986414]]\n",
      "5 [[-0.517328    0.5457123  -1.4602914  ... -0.17388207  0.1155955\n",
      "   0.25212738]\n",
      " [-0.5127104  -0.5614474   0.33180517 ...  0.70161855 -0.42981663\n",
      "  -1.6293632 ]\n",
      " [-0.41638786 -0.5546271  -0.2030706  ... -0.65490866  0.61806434\n",
      "  -0.05416132]\n",
      " ...\n",
      " [-1.0400126  -0.97417605  0.3994509  ...  0.07850619 -0.60362417\n",
      "   0.05914283]\n",
      " [-1.0173074  -0.88925534  0.8362456  ...  0.42605376 -0.64014715\n",
      "  -0.27216998]\n",
      " [-0.6841079  -1.0132074   0.938176   ...  0.46694553 -0.71663105\n",
      "  -0.43157572]]\n",
      "5 [[ 0.00124317  0.00074166 -0.00093949 ... -0.00095651  0.00082766\n",
      "   0.00017941]\n",
      " [ 0.00109905  0.00068539  0.00014317 ... -0.0022586   0.00187704\n",
      "  -0.00046504]\n",
      " [-0.00107735  0.00155467 -0.00114201 ...  0.00155705 -0.00039411\n",
      "  -0.00045103]\n",
      " ...\n",
      " [-0.00039983  0.0010469   0.00174436 ...  0.0004716   0.00075585\n",
      "   0.00135265]\n",
      " [-0.00103998  0.00182897  0.00207555 ...  0.00046319 -0.0003258\n",
      "   0.0020999 ]\n",
      " [-0.00071466  0.00108325  0.00252765 ...  0.00119299 -0.0003801\n",
      "   0.00171158]]\n",
      "6 [[-0.08303837  0.7216723  -1.3987335  ... -0.5160107   0.03211639\n",
      "   0.92033565]\n",
      " [-0.97245276 -0.62165457 -0.08244003 ...  0.32639566 -0.08087059\n",
      "  -1.5462978 ]\n",
      " [-0.15577263 -0.7962857  -0.20059162 ... -0.54348236  0.34646857\n",
      "  -0.34430838]\n",
      " ...\n",
      " [-0.8507656  -0.9036512  -0.34004077 ... -0.03875089  0.05193239\n",
      "   0.56164706]\n",
      " [-1.001335   -0.83296597  0.667963   ...  0.40393442 -0.210712\n",
      "   0.01032217]\n",
      " [-0.6984395  -0.9781923   0.8252411  ...  0.5512464  -0.42670408\n",
      "  -0.24808002]]\n",
      "6 [[-0.08286027  0.71905357 -1.3989586  ... -0.51574     0.03086875\n",
      "   0.91850114]\n",
      " [-0.97266585 -0.6233136  -0.08204151 ...  0.32718772 -0.08097479\n",
      "  -1.5465227 ]\n",
      " [-0.155747   -0.79889673 -0.19844595 ... -0.54454774  0.34640443\n",
      "  -0.34264567]\n",
      " ...\n",
      " [-0.8497905  -0.9031901  -0.34192994 ... -0.04038344  0.04974868\n",
      "   0.56046075]\n",
      " [-1.0008291  -0.83264345  0.66427916 ...  0.40197718 -0.211118\n",
      "   0.00952795]\n",
      " [-0.69880056 -0.9785056   0.8207159  ...  0.54923975 -0.42646712\n",
      "  -0.2492372 ]]\n",
      "6 [[-1.7809868e-04  2.6187301e-03  2.2506714e-04 ... -2.7072430e-04\n",
      "   1.2476426e-03  1.8345118e-03]\n",
      " [ 2.1308661e-04  1.6590357e-03 -3.9852411e-04 ... -7.9205632e-04\n",
      "   1.0420382e-04  2.2494793e-04]\n",
      " [-2.5629997e-05  2.6110411e-03 -2.1456778e-03 ...  1.0653734e-03\n",
      "   6.4134598e-05 -1.6627014e-03]\n",
      " ...\n",
      " [-9.7507238e-04 -4.6110153e-04  1.8891692e-03 ...  1.6325563e-03\n",
      "   2.1837056e-03  1.1863112e-03]\n",
      " [-5.0592422e-04 -3.2252073e-04  3.6838651e-03 ...  1.9572377e-03\n",
      "   4.0599704e-04  7.9422258e-04]\n",
      " [ 3.6108494e-04  3.1328201e-04  4.5251846e-03 ...  2.0066500e-03\n",
      "  -2.3695827e-04  1.1571795e-03]]\n",
      "7 [[-0.40050486  0.5999686  -1.2778066  ... -0.60073864  0.2281749\n",
      "   0.8154308 ]\n",
      " [-0.7125106  -1.0438733  -0.23263782 ...  0.3133481  -0.1894485\n",
      "  -0.8807531 ]\n",
      " [ 0.05551867 -1.343536   -0.1836879  ... -0.07562688  0.56710243\n",
      "   0.09780508]\n",
      " ...\n",
      " [-1.0074893  -0.8497404  -0.27465972 ... -0.37996307 -0.30851987\n",
      "   0.37444335]\n",
      " [-1.087804   -0.6782817   0.68030345 ...  0.08400168 -0.32138684\n",
      "  -0.18527   ]\n",
      " [-0.82669014 -0.7939924   0.7693476  ...  0.31335193 -0.51849306\n",
      "  -0.39608154]]\n",
      "7 [[-0.4003608   0.5992122  -1.2786411  ... -0.6001551   0.22661345\n",
      "   0.81289977]\n",
      " [-0.712978   -1.0455863  -0.23108439 ...  0.31533015 -0.18959443\n",
      "  -0.8812064 ]\n",
      " [ 0.05646604 -1.3469024  -0.18286654 ... -0.07522531  0.5674362\n",
      "   0.0989693 ]\n",
      " ...\n",
      " [-1.0065101  -0.84839576 -0.27489442 ... -0.3799436  -0.31074122\n",
      "   0.37131217]\n",
      " [-1.0855752  -0.6757822   0.6756789  ...  0.08451647 -0.32262635\n",
      "  -0.18746603]\n",
      " [-0.82654214 -0.7933802   0.7649707  ...  0.312972   -0.52023995\n",
      "  -0.39891034]]\n",
      "7 [[-1.4406443e-04  7.5638294e-04  8.3446503e-04 ... -5.8352947e-04\n",
      "   1.5614480e-03  2.5310516e-03]\n",
      " [ 4.6741962e-04  1.7130375e-03 -1.5534312e-03 ... -1.9820333e-03\n",
      "   1.4592707e-04  4.5329332e-04]\n",
      " [-9.4736740e-04  3.3663511e-03 -8.2135201e-04 ... -4.0157139e-04\n",
      "  -3.3378601e-04 -1.1642128e-03]\n",
      " ...\n",
      " [-9.7918510e-04 -1.3446212e-03  2.3469329e-04 ... -1.9460917e-05\n",
      "   2.2213459e-03  3.1311810e-03]\n",
      " [-2.2287369e-03 -2.4995208e-03  4.6245456e-03 ... -5.1479042e-04\n",
      "   1.2395084e-03  2.1960288e-03]\n",
      " [-1.4799833e-04 -6.1219931e-04  4.3768883e-03 ...  3.7992001e-04\n",
      "   1.7468929e-03  2.8288066e-03]]\n",
      "8 [[-0.81920385  0.5194984  -1.0329167  ... -0.29164362  0.40805474\n",
      "   0.57922035]\n",
      " [-0.7607314  -1.0892713  -0.15426005 ...  0.09640472 -0.20681804\n",
      "  -0.73512346]\n",
      " [ 0.09441271 -1.2478522   0.07891774 ...  0.3944794   0.17395586\n",
      "  -0.09134556]\n",
      " ...\n",
      " [-0.8648111  -0.5189958  -0.22545734 ... -0.49712792 -0.25385293\n",
      "   0.3268536 ]\n",
      " [-0.86020255 -0.32150412  0.3791648  ... -0.07292668 -0.16282043\n",
      "  -0.07786675]\n",
      " [-0.6939051  -0.68028224  0.45106432 ...  0.3616936  -0.22200257\n",
      "  -0.493348  ]]\n",
      "8 [[-0.82031846  0.51893723 -1.0327524  ... -0.2932682   0.40748692\n",
      "   0.57653105]\n",
      " [-0.76245034 -1.0912446  -0.15337162 ...  0.09754249 -0.20561373\n",
      "  -0.733878  ]\n",
      " [ 0.0949636  -1.2515374   0.08037816 ...  0.39380512  0.17430982\n",
      "  -0.08888911]\n",
      " ...\n",
      " [-0.86411184 -0.52016693 -0.22596638 ... -0.49618527 -0.25557572\n",
      "   0.32509342]\n",
      " [-0.85829675 -0.32092112  0.37570012 ... -0.07169562 -0.16414095\n",
      "  -0.08002545]\n",
      " [-0.69291437 -0.6796272   0.44713047 ...  0.36117154 -0.22456561\n",
      "  -0.49553302]]\n",
      "8 [[ 0.00111461  0.00056118 -0.00016427 ...  0.00162458  0.00056782\n",
      "   0.0026893 ]\n",
      " [ 0.00171894  0.00197327 -0.00088844 ... -0.00113777 -0.00120431\n",
      "  -0.00124544]\n",
      " [-0.00055089  0.00368524 -0.00146042 ...  0.00067428 -0.00035396\n",
      "  -0.00245645]\n",
      " ...\n",
      " [-0.00069928  0.00117111  0.00050904 ... -0.00094265  0.00172278\n",
      "   0.00176018]\n",
      " [-0.0019058  -0.00058299  0.00346467 ... -0.00123106  0.00132053\n",
      "   0.0021587 ]\n",
      " [-0.00099075 -0.00065506  0.00393385 ...  0.00052205  0.00256304\n",
      "   0.00218502]]\n",
      "9 [[-1.0850376   0.31066683 -0.7876851  ... -0.35431206  0.96151143\n",
      "   0.30817342]\n",
      " [-0.90402114 -0.9382762  -0.22637647 ... -0.07830018  0.20473197\n",
      "  -0.78290313]\n",
      " [ 0.11387892 -1.2016054  -0.43271708 ...  0.287103    0.46687883\n",
      "   0.18098845]\n",
      " ...\n",
      " [-1.0934032  -0.56386703  0.09995283 ... -0.09225254  0.13286163\n",
      "   0.17988802]\n",
      " [-1.0195221  -0.38205498  0.49210873 ...  0.2681017   0.10748599\n",
      "  -0.18636194]\n",
      " [-0.9361528  -0.72997874  0.6413366  ...  0.6019962  -0.05007812\n",
      "  -0.31917208]]\n",
      "9 [[-1.0853858   0.31009406 -0.78809035 ... -0.35509133  0.9623143\n",
      "   0.3050905 ]\n",
      " [-0.9066044  -0.9400682  -0.22396824 ... -0.07748205  0.20500211\n",
      "  -0.78101015]\n",
      " [ 0.11340802 -1.2061765  -0.43187124 ...  0.28799254  0.46758902\n",
      "   0.182351  ]\n",
      " ...\n",
      " [-1.0911167  -0.56570166  0.09961782 ... -0.09294108  0.13129057\n",
      "   0.17722365]\n",
      " [-1.0164206  -0.3810068   0.48948708 ...  0.26706922  0.10407874\n",
      "  -0.18862456]\n",
      " [-0.93389827 -0.72752804  0.63822323 ...  0.59939367 -0.05349415\n",
      "  -0.32212457]]\n",
      "9 [[ 0.00034821  0.00057277  0.00040525 ...  0.00077927 -0.00080287\n",
      "   0.00308293]\n",
      " [ 0.00258327  0.00179201 -0.00240824 ... -0.00081813 -0.00027014\n",
      "  -0.00189298]\n",
      " [ 0.0004709   0.00457108 -0.00084585 ... -0.00088954 -0.00071019\n",
      "  -0.00136255]\n",
      " ...\n",
      " [-0.00228655  0.00183463  0.00033502 ...  0.00068855  0.00157106\n",
      "   0.00266437]\n",
      " [-0.00310147 -0.00104818  0.00262165 ...  0.00103247  0.00340725\n",
      "   0.00226262]\n",
      " [-0.00225455 -0.0024507   0.00311339 ...  0.00260252  0.00341603\n",
      "   0.00295249]]\n",
      "10 [[-0.9091689   0.30783722 -0.5778292  ... -0.7939384   0.471214\n",
      "   0.6953346 ]\n",
      " [-0.9509741  -0.8545809  -0.16839032 ...  0.03392965  0.01996958\n",
      "  -0.7037993 ]\n",
      " [ 0.08593966 -1.1473032  -0.03704683 ... -0.14936905  0.2671036\n",
      "   0.13064301]\n",
      " ...\n",
      " [-1.1558388  -0.49350983  0.16129619 ... -0.03864192  0.17063609\n",
      "  -0.26275846]\n",
      " [-1.1789799  -0.3336795   0.58346784 ...  0.16087276  0.1254016\n",
      "  -0.6140558 ]\n",
      " [-1.2023162  -0.6441384   0.6601857  ...  0.5305264   0.06072028\n",
      "  -0.6409829 ]]\n",
      "10 [[-0.9110892   0.30892447 -0.5761568  ... -0.79329395  0.4732527\n",
      "   0.69220257]\n",
      " [-0.954209   -0.85657644 -0.16449001 ...  0.03537128  0.02089566\n",
      "  -0.7024077 ]\n",
      " [ 0.08460355 -1.1508987  -0.03576094 ... -0.14698021  0.26741034\n",
      "   0.13115723]\n",
      " ...\n",
      " [-1.1544858  -0.49256352  0.1615835  ... -0.04023896  0.16786261\n",
      "  -0.26538923]\n",
      " [-1.1771811  -0.3302661   0.5821581  ...  0.15965074  0.12072124\n",
      "  -0.6171172 ]\n",
      " [-1.201164   -0.63976824  0.65867954 ...  0.52763647  0.05579618\n",
      "  -0.6454346 ]]\n",
      "10 [[ 0.00192028 -0.00108725 -0.00167239 ... -0.00064445 -0.00203872\n",
      "   0.00313205]\n",
      " [ 0.00323492  0.00199556 -0.0039003  ... -0.00144162 -0.00092608\n",
      "  -0.00139159]\n",
      " [ 0.00133611  0.00359547 -0.00128589 ... -0.00238883 -0.00030673\n",
      "  -0.00051422]\n",
      " ...\n",
      " [-0.00135303 -0.00094631 -0.00028731 ...  0.00159704  0.00277348\n",
      "   0.00263077]\n",
      " [-0.00179875 -0.00341341  0.00130975 ...  0.00122201  0.00468037\n",
      "   0.00306141]\n",
      " [-0.00115216 -0.00437015  0.00150615 ...  0.00288993  0.0049241\n",
      "   0.00445169]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    print(i, tensorflow_outputs[i])\n",
    "    print(i, pytorch_outputs[i])\n",
    "    print(i, tensorflow_outputs[i] - pytorch_outputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.19542193e-05  4.87893820e-04  3.64027917e-04 ... -4.16293740e-04\n",
      "   6.87688589e-04  4.09096479e-04]\n",
      " [ 1.24812126e-03 -4.68790531e-04  2.93403864e-04 ... -7.49558210e-04\n",
      "   6.66707754e-04  8.89182091e-04]\n",
      " [-2.89857388e-04  1.80363655e-04  2.33143568e-04 ...  1.05842948e-03\n",
      "  -2.08616257e-05  6.86526299e-04]\n",
      " ...\n",
      " [-2.32636929e-04 -3.27408314e-04  7.27355480e-04 ... -1.55210495e-04\n",
      "   8.00490379e-04  1.46865845e-04]\n",
      " [-5.50061464e-04 -1.61409378e-04  1.82747841e-04 ... -1.04010105e-04\n",
      "   4.23476100e-04  6.42836094e-05]\n",
      " [-2.39863992e-04 -3.22192907e-04  2.92241573e-04 ... -1.17897987e-04\n",
      "   4.39703465e-04 -5.87105751e-05]]\n"
     ]
    }
   ],
   "source": [
    "# print(tensorflow_outputs[0] - pytorch_outputs[0])\n",
    "# print(tensorflow_outputs[1] - pytorch_outputs[1])\n",
    "print(tensorflow_outputs[2] - pytorch_outputs[2])\n",
    "# print(tensorflow_outputs[11] - pytorch_outputs[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10065358 -0.22039229 -0.09111832 ...  0.1500864   0.08662349\n",
      "   0.1169717 ]\n",
      " [-0.7875237   0.4987949   0.47532898 ...  0.3109045  -0.3090303\n",
      "  -1.0728124 ]\n",
      " [-0.48414615 -0.17314494  0.34687328 ... -0.337559    0.28318426\n",
      "  -0.5799342 ]\n",
      " ...\n",
      " [-0.36874118 -0.23636177  0.78799486 ...  0.8991036  -0.23088121\n",
      "  -0.26291594]\n",
      " [-0.46867168 -0.18532905  0.6907864  ...  0.9044033  -0.15563615\n",
      "  -0.38067216]\n",
      " [-0.23992004 -0.31867903  0.6687768  ...  1.0202096  -0.36089402\n",
      "  -0.44989982]]\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow_outputs[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/ Comparing the standard deviation on the last layer of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:39.437137Z",
     "start_time": "2018-11-15T15:21:39.406150Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T15:21:40.181870Z",
     "start_time": "2018-11-15T15:21:40.137023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape tensorflow layer, shape pytorch layer, standard deviation\n",
      "((128, 768), (128, 768), 0.00021029184)\n",
      "((128, 768), (128, 768), 0.00055583025)\n",
      "((128, 768), (128, 768), 0.00068541203)\n",
      "((128, 768), (128, 768), 0.0008927335)\n",
      "((128, 768), (128, 768), 0.001315971)\n",
      "((128, 768), (128, 768), 0.0016274694)\n",
      "((128, 768), (128, 768), 0.0021441837)\n",
      "((128, 768), (128, 768), 0.0024197593)\n",
      "((128, 768), (128, 768), 0.0026458544)\n",
      "((128, 768), (128, 768), 0.0028913843)\n",
      "((128, 768), (128, 768), 0.0030688304)\n",
      "((128, 768), (128, 768), 0.001419331)\n"
     ]
    }
   ],
   "source": [
    "print('shape tensorflow layer, shape pytorch layer, standard deviation')\n",
    "print('\\n'.join(list(str((np.array(tensorflow_outputs[i]).shape,\n",
    "                          np.array(pytorch_outputs[i]).shape, \n",
    "                          np.sqrt(np.mean((np.array(tensorflow_outputs[i]) - np.array(pytorch_outputs[i]))**2.0)))) for i in range(12))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "bert-pretraining",
   "language": "python",
   "name": "bert-pretraining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
